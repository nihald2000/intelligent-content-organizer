{
  "0": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_0",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "--- Page --- Provided proper attribution is provided. Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani Google Brain avaswani google.comNoam Shazeer Google Brain noam google.comNiki Parmar Google Research nikip google.comJakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.comAidan N. Gomez University of Toronto",
    "chunk_index": 0,
    "start_pos": 0,
    "end_pos": 482,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 482,
      "word_count": 63,
      "optimized_for_embedding": true,
      "original_content_length": 482,
      "optimized_content_length": 472
    }
  },
  "1": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_1",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "google.comAidan N. Gomez University of Toronto aidan cs.toronto.edu\u0141ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose new simple network architecture. the Transformer.",
    "chunk_index": 1,
    "start_pos": 483,
    "end_pos": 914,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 482,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 482,
      "optimized_content_length": 470
    }
  },
  "2": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_2",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "new simple network architecture. the Transformer. based solely on attention mechanisms. dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task. improving over the existing best results. including ensembles. by over BLEU. On the WMT 2014 English-to-French translation task.",
    "chunk_index": 2,
    "start_pos": 915,
    "end_pos": 1388,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 521
    }
  },
  "3": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_3",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "the WMT 2014 English-to-French translation task. our model establishes new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs. small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started",
    "chunk_index": 3,
    "start_pos": 1389,
    "end_pos": 1859,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 514
    }
  },
  "4": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_4",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "sed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish. with Illia. designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention. multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed. implemented. tuned and evaluated countless model variants in our original codebase and",
    "chunk_index": 4,
    "start_pos": 1860,
    "end_pos": 2304,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 495
    }
  },
  "5": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_5",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ntless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants. was responsible for our initial codebase. and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor. replacing our earlier codebase. greatly improving results and massively accelerating our research. Work performed while at Google Brain. Work performed while at Google Research.",
    "chunk_index": 5,
    "start_pos": 2305,
    "end_pos": 2738,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 484,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 484,
      "optimized_content_length": 482
    }
  },
  "6": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_6",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "Brain. Work performed while at Google Research. 31st Conference on Neural Information Processing Systems NIPS 2017 Long Beach. CA. USA.arXiv.1706.03762v7 cs.CL Aug 2023",
    "chunk_index": 6,
    "start_pos": 2739,
    "end_pos": 2868,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 180,
      "word_count": 25,
      "optimized_for_embedding": true,
      "original_content_length": 180,
      "optimized_content_length": 168
    }
  },
  "7": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_7",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "h. CA. USA.arXiv.1706.03762v7 cs.CL Aug 2023 --- Page --- Introduction Recurrent neural networks. long short-term memory 13 and gated recurrent neural networks in particular. have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 35.2.5 Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures 38. 24. 15",
    "chunk_index": 7,
    "start_pos": 2870,
    "end_pos": 3317,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 473
    }
  },
  "8": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_8",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ls and encoder-decoder architectures 38. 24. 15 Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time. they generate sequence of hidden states ht. as function of the previous hidden state ht 1and the input for position t. This inherently sequential nature precludes parallelization within training examples. which becomes critical at longer sequence lengths. as memory constraints limit batching across examples. Recent work has achieved",
    "chunk_index": 8,
    "start_pos": 3318,
    "end_pos": 3817,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 543
    }
  },
  "9": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_9",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks 21 and conditional computation 32 while also improving model performance in case of the latter. The fundamental constraint of sequential computation. however. remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks. allowing modeling of dependencies without regard to their distance in",
    "chunk_index": 9,
    "start_pos": 3818,
    "end_pos": 4270,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 503,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 503,
      "optimized_content_length": 496
    }
  },
  "10": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_10",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "dependencies without regard to their distance in the input or output sequences 2.19 In all but few cases 27 however. such attention mechanisms are used in conjunction with recurrent network. In this work we propose the Transformer. model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach new state of the art in",
    "chunk_index": 10,
    "start_pos": 4271,
    "end_pos": 4718,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 480
    }
  },
  "11": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_11",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "lelization and can reach new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 16 ByteNet 18 and ConvS2S all of which use convolutional neural networks as basic building block. computing hidden representations in parallel for all input and output positions. In these models.",
    "chunk_index": 11,
    "start_pos": 4719,
    "end_pos": 5126,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 458,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 458,
      "optimized_content_length": 442
    }
  },
  "12": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_12",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "all input and output positions. In these models. the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions. linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions 12 In the Transformer this is reduced to constant number of operations. albeit at the cost of reduced effective resolution due to averaging attention-weighted positions. an effect we counteract with Multi-Head Attention as",
    "chunk_index": 12,
    "start_pos": 5127,
    "end_pos": 5624,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 540
    }
  },
  "13": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_13",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention. sometimes called intra-attention is an attention mechanism relating different positions of single sequence in order to compute representation of the sequence. Self-attention has been used successfully in variety of tasks including reading comprehension. abstractive summarization. textual entailment and learning task-independent sentence representations 4. 27. 28. 22",
    "chunk_index": 13,
    "start_pos": 5625,
    "end_pos": 6044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 470,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 469,
      "optimized_content_length": 460
    }
  },
  "14": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_14",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ependent sentence representations 4. 27. 28. 22 End-to-end memory networks are based on recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks 34 To the best of our knowledge. however. the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-",
    "chunk_index": 14,
    "start_pos": 6045,
    "end_pos": 6461,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 467,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 467,
      "optimized_content_length": 459
    }
  },
  "15": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_15",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ns of its input and output without using sequence- aligned RNNs or convolution. In the following sections. we will describe the Transformer. motivate self-attention and discuss its advantages over models such as 17. 18 and Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure 5.2.35 Here. the encoder maps an input sequence of symbol representations x1. to sequence of continuous representations z1. Given z. the decoder then generates an output",
    "chunk_index": 15,
    "start_pos": 6462,
    "end_pos": 6949,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 498
    }
  },
  "16": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_16",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "Given z. the decoder then generates an output sequence y1. of symbols one element at time. At each step the model is auto-regressive 10 consuming the previously generated symbols as additional input when generating the next.",
    "chunk_index": 16,
    "start_pos": 6950,
    "end_pos": 7145,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 246,
      "word_count": 41,
      "optimized_for_embedding": true,
      "original_content_length": 245,
      "optimized_content_length": 224
    }
  },
  "17": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_17",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ls as additional input when generating the next. --- Page --- Figure 1. The Transformer model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise. fully connected layers for both the encoder and decoder. shown in the left and right halves of Figure 1. respectively. 3.1 Encoder and Decoder Stacks Encoder. The encoder is composed of stack of identical layers. Each layer has two sub-layers. The first is multi-head self-attention mechanism. and the second is simple. position-",
    "chunk_index": 17,
    "start_pos": 7147,
    "end_pos": 7645,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 532
    }
  },
  "18": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_18",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "mechanism. and the second is simple. position- wise fully connected feed-forward network. We employ residual connection 11 around each of the two sub-layers. followed by layer normalization That is. the output of each sub-layer is LayerNorm Sublayer where Sublayer is the function implemented by the sub-layer itself. To facilitate these residual connections. all sub-layers in the model. as well as the embedding layers. produce outputs of dimension dmodel 512",
    "chunk_index": 18,
    "start_pos": 7646,
    "end_pos": 8088,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 461
    }
  },
  "19": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_19",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ayers. produce outputs of dimension dmodel 512 Decoder. The decoder is also composed of stack of 6identical layers. In addition to the two sub-layers in each encoder layer. the decoder inserts third sub-layer. which performs multi-head attention over the output of the encoder stack. Similar to the encoder. we employ residual connections around each of the sub-layers. followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This",
    "chunk_index": 19,
    "start_pos": 8089,
    "end_pos": 8582,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 533
    }
  },
  "20": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_20",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "tions from attending to subsequent positions. This masking. combined with fact that the output embeddings are offset by one position. ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping query and set of key-value pairs to an output. where the query. keys. values. and output are all vectors. The output is computed as weighted sum",
    "chunk_index": 20,
    "start_pos": 8583,
    "end_pos": 8988,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 456,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 456,
      "optimized_content_length": 448
    }
  },
  "21": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_21",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ectors. The output is computed as weighted sum --- Page --- Scaled Dot-Product Attention Multi-Head Attention Figure 2. left Scaled Dot-Product Attention. right Multi-Head Attention consists of several attention layers running in parallel. of the values. where the weight assigned to each value is computed by compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention We call our particular attention Scaled Dot-Product Attention Figure The input consists of",
    "chunk_index": 21,
    "start_pos": 8990,
    "end_pos": 9458,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 499
    }
  },
  "22": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_22",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "oduct Attention Figure The input consists of queries and keys of dimension dk. and values of dimension dv. We compute the dot products of the query with all keys. divide each by dk. and apply softmax function to obtain the weights on the values. In practice. we compute the attention function on set of queries simultaneously. packed together into matrix Q. The keys and values are also packed together into matrices KandV. We compute the matrix of outputs as. Attention Q. K. softmax QKT dk",
    "chunk_index": 22,
    "start_pos": 9459,
    "end_pos": 9925,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 491
    }
  },
  "23": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_23",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "as. Attention Q. K. softmax QKT dk The two most commonly used attention functions are additive attention and dot-product multi- plicative attention. Dot-product attention is identical to our algorithm. except for the scaling factor of1 dk. Additive attention computes the compatibility function using feed-forward network with single hidden layer. While the two are similar in theoretical complexity. dot-product attention is",
    "chunk_index": 23,
    "start_pos": 9926,
    "end_pos": 10328,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 453,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 453,
      "optimized_content_length": 425
    }
  },
  "24": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_24",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "theoretical complexity. dot-product attention is much faster and more space-efficient in practice. since it can be implemented using highly optimized matrix multiplication code. While for small values of dkthe two mechanisms perform similarly. additive attention outperforms dot product attention without scaling for larger values of dk We suspect that for large values of dk. the dot products grow large in magnitude. pushing the softmax function into regions where it has",
    "chunk_index": 24,
    "start_pos": 10329,
    "end_pos": 10757,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 479,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 479,
      "optimized_content_length": 473
    }
  },
  "25": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_25",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ing the softmax function into regions where it has extremely small gradients4. To counteract this effect. we scale the dot products by1 dk. 3.2.2 Multi-Head Attention Instead of performing single attention function with dmodel-dimensional keys. values and queries. we found it beneficial to linearly project the queries. keys and values htimes with different. learned linear projections to dk.dkanddvdimensions. respectively. On each of these projected versions of",
    "chunk_index": 25,
    "start_pos": 10758,
    "end_pos": 11173,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 466,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 466,
      "optimized_content_length": 464
    }
  },
  "26": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_26",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "spectively. On each of these projected versions of queries. keys and values we then perform the attention function in parallel. yielding dv-dimensional 4To illustrate why the dot products get large. assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product. Pdk 1qiki. has mean 0and variance dk.",
    "chunk_index": 26,
    "start_pos": 11174,
    "end_pos": 11486,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 363,
      "word_count": 56,
      "optimized_for_embedding": true,
      "original_content_length": 363,
      "optimized_content_length": 355
    }
  },
  "27": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_27",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "uct. Pdk 1qiki. has mean 0and variance dk. --- Page --- output values. These are concatenated and once again projected. resulting in the final values. as depicted in Figure 2. Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With single attention head. averaging inhibits this. MultiHead Q. K. Concat head 1. .head WO where head Attention QWQ i. KWK i. WV Where the projections are parameter matrices WQ Rdmodel dk.WK",
    "chunk_index": 27,
    "start_pos": 11488,
    "end_pos": 11975,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 503
    }
  },
  "28": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_28",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "jections are parameter matrices WQ Rdmodel dk.WK Rdmodel dk.WV Rdmodel dv andWO Rhdv dmodel. In this work we employ parallel attention layers. or heads. For each of these we use dk dv dmodel 64 Due to the reduced dimension of each head. the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways.",
    "chunk_index": 28,
    "start_pos": 11976,
    "end_pos": 12388,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 463,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 463,
      "optimized_content_length": 447
    }
  },
  "29": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_29",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "uses multi-head attention in three different ways. In encoder-decoder attention layers. the queries come from the previous decoder layer. and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as 38. 2. The encoder contains self-attention layers. In self-attention layer all of the keys. values",
    "chunk_index": 29,
    "start_pos": 12389,
    "end_pos": 12847,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 498
    }
  },
  "30": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_30",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "In self-attention layer all of the keys. values and queries come from the same place. in this case. the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly. self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this",
    "chunk_index": 30,
    "start_pos": 12848,
    "end_pos": 13324,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 523
    }
  },
  "31": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_31",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers. each of the layers in our encoder and decoder contains fully connected feed-forward network. which is applied to each position separately and identically. This consists of two linear transformations with ReLU activation in between.",
    "chunk_index": 31,
    "start_pos": 13325,
    "end_pos": 13801,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 518
    }
  },
  "32": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_32",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "transformations with ReLU activation in between. FFN max xW b1 W2 b2 While the linear transformations are the same across different positions. they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel 512 and the inner-layer has dimensionality dff 2048 3.4 Embeddings and Softmax Similarly to other sequence transduction models. we use learned embeddings to convert the input",
    "chunk_index": 32,
    "start_pos": 13802,
    "end_pos": 14267,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 491
    }
  },
  "33": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_33",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ls. we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model. we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation. similar to 30 In the embedding layers. we multiply those weights by dmodel.",
    "chunk_index": 33,
    "start_pos": 14268,
    "end_pos": 14672,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 455,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 455,
      "optimized_content_length": 449
    }
  },
  "34": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_34",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ing layers. we multiply those weights by dmodel. --- Page --- Table 1. Maximum path lengths. per-layer complexity and minimum number of sequential operations for different layer types. nis the sequence length. dis the representation dimension. kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention n2 Recurrent d2 Convolutional d2 logk",
    "chunk_index": 34,
    "start_pos": 14674,
    "end_pos": 15145,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 470
    }
  },
  "35": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_35",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "Convolutional d2 logk Self-Attention restricted 3.5 Positional Encoding Since our model contains no recurrence and no convolution. in order for the model to make use of the order of the sequence. we must inject some information about the relative or absolute position of the tokens in the sequence. To this end. we add positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel",
    "chunk_index": 35,
    "start_pos": 15146,
    "end_pos": 15618,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 469
    }
  },
  "36": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_36",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ositional encodings have the same dimension dmodel as the embeddings. so that the two can be summed. There are many choices of positional encodings. learned and fixed In this work. we use sine and cosine functions of different frequencies. PE pos.2i sin pos 100002i model PE pos.2i cos pos 100002i model where posis the position and iis the dimension. That is. each dimension of the positional encoding corresponds to sinusoid. The wavelengths form geometric progression from 2\u03c0to10000 2\u03c0. We",
    "chunk_index": 36,
    "start_pos": 15619,
    "end_pos": 16080,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 492
    }
  },
  "37": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_37",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "orm geometric progression from 2\u03c0to10000 2\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions. since for any fixed offset k.PEpos kcan be represented as linear function of PEpos. We also experimented with using learned positional embeddings instead. and found that the two versions produced nearly identical results see Table row We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered",
    "chunk_index": 37,
    "start_pos": 16081,
    "end_pos": 16580,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 531
    }
  },
  "38": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_38",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "sequence lengths longer than the ones encountered during training. Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations x1. to another sequence of equal length z1. with xi. zi Rd. such as hidden layer in typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.",
    "chunk_index": 38,
    "start_pos": 16581,
    "end_pos": 17038,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 479
    }
  },
  "39": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_39",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "se of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized. as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to",
    "chunk_index": 39,
    "start_pos": 17039,
    "end_pos": 17517,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 527
    }
  },
  "40": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_40",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences. the easier it is to learn long-range dependencies 12 Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1. self-attention layer connects all positions with constant number of sequentially",
    "chunk_index": 40,
    "start_pos": 17518,
    "end_pos": 17941,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 474,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 473,
      "optimized_content_length": 465
    }
  },
  "41": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_41",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "positions with constant number of sequentially executed operations. whereas recurrent layer requires sequential operations. In terms of computational complexity. self-attention layers are faster than recurrent layers when the sequence",
    "chunk_index": 41,
    "start_pos": 17942,
    "end_pos": 18137,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 246,
      "word_count": 33,
      "optimized_for_embedding": true,
      "original_content_length": 246,
      "optimized_content_length": 234
    }
  },
  "42": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_42",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "faster than recurrent layers when the sequence --- Page --- length nis smaller than the representation dimensionality d. which is most often the case with sentence representations used by state-of-the-art models in machine translations. such as word-piece 38 and byte-pair 31 representations. To improve computational performance for tasks involving very long sequences. self-attention could be restricted to considering only neighborhood of size rin",
    "chunk_index": 42,
    "start_pos": 18139,
    "end_pos": 18551,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 463,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 463,
      "optimized_content_length": 450
    }
  },
  "43": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_43",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ted to considering only neighborhood of size rin the input sequence centered around the respective output position. This would increase the maximum path length to We plan to investigate this approach further in future work. single convolutional layer with kernel width does not connect all pairs of input and output positions. Doing so requires stack of convolutional layers in the case of contiguous kernels. orO logk in the case of dilated convolutions 18 increasing the length of the longest paths",
    "chunk_index": 43,
    "start_pos": 18552,
    "end_pos": 19034,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 500
    }
  },
  "44": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_44",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "18 increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers. by factor of k. Separable convolutions however. decrease the complexity considerably. to d2 Even with n. however. the complexity of separable convolution is equal to the combination of self-attention layer and point-wise feed-forward layer. the approach we take in our model.",
    "chunk_index": 44,
    "start_pos": 19035,
    "end_pos": 19453,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 469,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 468,
      "optimized_content_length": 436
    }
  },
  "45": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_45",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "-forward layer. the approach we take in our model. As side benefit. self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks. many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. Training This section describes the training regime for our models. 5.1 Training Data and Batching",
    "chunk_index": 45,
    "start_pos": 19454,
    "end_pos": 19905,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 500
    }
  },
  "46": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_46",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding which has shared source- target vocabulary of about 37000 tokens. For English-French. we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into 32000 word-piece vocabulary 38 Sentence pairs were batched together by approximate sequence length. Each training",
    "chunk_index": 46,
    "start_pos": 19906,
    "end_pos": 20389,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 520
    }
  },
  "47": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_47",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ther by approximate sequence length. Each training batch contained set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper. each training step took about 0.4 seconds. We trained the base models for total of 100.000 steps or 12 hours. For our big models. described on the",
    "chunk_index": 47,
    "start_pos": 20390,
    "end_pos": 20814,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 475,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 475,
      "optimized_content_length": 469
    }
  },
  "48": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_48",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "or 12 hours. For our big models. described on the bottom line of table step time was 1.0 seconds. The big models were trained for 300.000 steps 3.5 days 5.3 Optimizer We used the Adam optimizer 20 with \u03b21 0.9.\u03b22 0.98and\u03b5 10 9. We varied the learning rate over the course of training. according to the formula. lrate 0.5 model min step_num 0.5. step _num warmup _steps 1.5 This corresponds to increasing the learning rate linearly for the first warmup _steps training steps.",
    "chunk_index": 48,
    "start_pos": 20815,
    "end_pos": 21259,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 473
    }
  },
  "49": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_49",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "nearly for the first warmup _steps training steps. and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup _steps 4000 5.4 Regularization We employ three types of regularization during training.",
    "chunk_index": 49,
    "start_pos": 21260,
    "end_pos": 21459,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 250,
      "word_count": 40,
      "optimized_for_embedding": true,
      "original_content_length": 250,
      "optimized_content_length": 244
    }
  },
  "50": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_50",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "three types of regularization during training. --- Page --- Table 2. The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at fraction of the training cost. ModelBLEU Training Cost FLOPs EN-DE EN-FR EN-DE EN-FR ByteNet 18 23.75 Deep-Att PosUnk 39 39.2 1.0 1020 GNMT RL 38 24.6 39.92 2.3 10191.4 1020 ConvS2S 25.16 40.46 9.6 10181.5 1020 MoE 32 26.03 40.56 2.0 10191.2 1020 Deep-Att PosUnk Ensemble 39 40.4 8.0 1020",
    "chunk_index": 50,
    "start_pos": 21461,
    "end_pos": 21948,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 508
    }
  },
  "51": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_51",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "1020 Deep-Att PosUnk Ensemble 39 40.4 8.0 1020 GNMT RL Ensemble 38 26.30 41.16 1.8 10201.1 1021 ConvS2S Ensemble 26.36 41.29 7.7 10191.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.8 2.3 1019 Residual Dropout We apply dropout 33 to the output of each sub-layer. before it is added to the sub-layer input and normalized. In addition. we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model. we use rate of Pdrop 0.1.",
    "chunk_index": 51,
    "start_pos": 21949,
    "end_pos": 22441,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 521
    }
  },
  "52": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_52",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "For the base model. we use rate of Pdrop 0.1. Label Smoothing During training. we employed label smoothing of value \u03b5ls 0.1 36 This hurts perplexity. as the model learns to be more unsure. but improves accuracy and BLEU score. Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task. the big transformer model Transformer big in Table outperforms the best previously reported models including ensembles by more than 2.0",
    "chunk_index": 52,
    "start_pos": 22442,
    "end_pos": 22855,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 464,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 464,
      "optimized_content_length": 446
    }
  },
  "53": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_53",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "rted models including ensembles by more than 2.0 BLEU. establishing new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model surpasses all previously published models and ensembles. at fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task. our big model achieves BLEU score of 41.0.",
    "chunk_index": 53,
    "start_pos": 22856,
    "end_pos": 23270,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 465,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 465,
      "optimized_content_length": 457
    }
  },
  "54": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_54",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "task. our big model achieves BLEU score of 41.0. outperforming all of the previously published single models. at less than 4the training cost of the previous state-of-the-art model. The Transformer big model trained for English-to-French used dropout rate Pdrop 0.1. instead of 0.3. For the base models. we used single model obtained by averaging the last checkpoints. which were written at 10-minute intervals. For the big models. we averaged the last 20 checkpoints. We",
    "chunk_index": 54,
    "start_pos": 23271,
    "end_pos": 23702,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 482,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 482,
      "optimized_content_length": 471
    }
  },
  "55": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_55",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ig models. we averaged the last 20 checkpoints. We used beam search with beam size of 4and length penalty 0.6 38 These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length 50. but terminate early when possible 38 Table summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train",
    "chunk_index": 55,
    "start_pos": 23703,
    "end_pos": 24168,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 500
    }
  },
  "56": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_56",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "umber of floating point operations used to train model by multiplying the training time. the number of GPUs used. and an estimate of the sustained single-precision floating-point capacity of each GPU5. 6.2 Model Variations To evaluate the importance of different components of the Transformer. we varied our base model in different ways. measuring the change in performance on English-to-German translation on the 5We used values of 2.8. 3.7. 6.0 and 9.5 TFLOPS for K80. K40. M40 and P100. respectively.",
    "chunk_index": 56,
    "start_pos": 24169,
    "end_pos": 24625,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 503
    }
  },
  "57": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_57",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "TFLOPS for K80. K40. M40 and P100. respectively. --- Page --- Table 3. Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set. newstest2013. Listed perplexities are per-wordpiece. according to our byte-pair encoding. and should not be compared to per-word perplexities. model dff dvPdrop \u03b5lstrain PPL BLEU params steps dev dev 106 base 512 2048 64 64 0.1 0.1 100K 4.92 25.8 65 512 512 5.29 24.9",
    "chunk_index": 57,
    "start_pos": 24627,
    "end_pos": 25114,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 510
    }
  },
  "58": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_58",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "0.1 0.1 100K 4.92 25.8 65 512 512 5.29 24.9 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 16 5.16 25.1 58 32 5.01 25.4 60 6.11 23.7 36 5.19 25.3 50 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 positional embedding instead of sinusoids 4.92 25.7 big 1024 4096 16 0.3 300K 4.33 26.4 213 development set. newstest2013. We used beam search as described in the previous section. but no",
    "chunk_index": 58,
    "start_pos": 25115,
    "end_pos": 25594,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 112,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 500
    }
  },
  "59": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_59",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "earch as described in the previous section. but no checkpoint averaging. We present these results in Table 3. In Table rows we vary the number of attention heads and the attention key and value dimensions. keeping the amount of computation constant. as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting. quality also drops off with too many heads. In Table rows we observe that reducing the attention key size dkhurts model quality. This",
    "chunk_index": 59,
    "start_pos": 25595,
    "end_pos": 26041,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 483
    }
  },
  "60": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_60",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that more sophisticated compatibility function than dot product may be beneficial. We further observe in rows and that. as expected. bigger models are better. and dropout is very helpful in avoiding over-fitting. In row we replace our sinusoidal positional encoding with learned positional embeddings and observe nearly identical results to the base model. 6.3 English Constituency Parsing",
    "chunk_index": 60,
    "start_pos": 26042,
    "end_pos": 26507,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 496
    }
  },
  "61": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_61",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "the base model. 6.3 English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges. the output is subject to strong structural constraints and is significantly longer than the input. Furthermore. RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes 37 We trained 4-layer transformer with dmodel 1024 on the Wall Street Journal WSJ portion of the",
    "chunk_index": 61,
    "start_pos": 26508,
    "end_pos": 26992,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 524
    }
  },
  "62": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_62",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "24 on the Wall Street Journal WSJ portion of the Penn Treebank 25 about 40K training sentences. We also trained it in semi-supervised setting. using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences 37 We used vocabulary of 16K tokens for the WSJ only setting and vocabulary of 32K tokens for the semi-supervised setting. We performed only small number of experiments to select the dropout. both attention and residual",
    "chunk_index": 62,
    "start_pos": 26993,
    "end_pos": 27418,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 476,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 476,
      "optimized_content_length": 459
    }
  },
  "63": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_63",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "to select the dropout. both attention and residual section 5.4 learning rates and beam size on the Section 22 development set. all other parameters remained unchanged from the English-to-German base translation model. During inference. we",
    "chunk_index": 63,
    "start_pos": 27419,
    "end_pos": 27611,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 243,
      "word_count": 36,
      "optimized_for_embedding": true,
      "original_content_length": 243,
      "optimized_content_length": 238
    }
  },
  "64": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_64",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "man base translation model. During inference. we --- Page 10 --- Table 4. The Transformer generalizes well to English constituency parsing Results are on Section 23 of WSJ Parser Training WSJ 23 F1 Vinyals Kaiser el al. 2014 37 WSJ only. discriminative 88.3 Petrov et al. 2006 29 WSJ only. discriminative 90.4 Zhu et al. 2013 40 WSJ only. discriminative 90.4 Dyer et al. 2016 WSJ only. discriminative 91.7 Transformer layers WSJ only. discriminative 91.3 Zhu et al. 2013 40 semi-supervised 91.3",
    "chunk_index": 64,
    "start_pos": 27613,
    "end_pos": 28088,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 494
    }
  },
  "65": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_65",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "91.3 Zhu et al. 2013 40 semi-supervised 91.3 Huang Harper 2009 14 semi-supervised 91.3 McClosky et al. 2006 26 semi-supervised 92.1 Vinyals Kaiser el al. 2014 37 semi-supervised 92.1 Transformer layers semi-supervised 92.7 Luong et al. 2015 23 multi-task 93.0 Dyer et al. 2016 generative 93.3 increased the maximum output length to input length 300. We used beam size of 21and\u03b1 0.3 for both WSJ only and the semi-supervised setting.",
    "chunk_index": 65,
    "start_pos": 28089,
    "end_pos": 28511,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 473,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 473,
      "optimized_content_length": 432
    }
  },
  "66": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_66",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "for both WSJ only and the semi-supervised setting. Our results in Table show that despite the lack of task-specific tuning our model performs sur- prisingly well. yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar In contrast to RNN sequence-to-sequence models 37 the Transformer outperforms the Berkeley- Parser 29 even when training only on the WSJ training set of 40K sentences. Conclusion",
    "chunk_index": 66,
    "start_pos": 28512,
    "end_pos": 28936,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 475,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 475,
      "optimized_content_length": 460
    }
  },
  "67": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_67",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "he WSJ training set of 40K sentences. Conclusion In this work. we presented the Transformer. the first sequence transduction model based entirely on attention. replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks. the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014",
    "chunk_index": 67,
    "start_pos": 28937,
    "end_pos": 29350,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 464,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 464,
      "optimized_content_length": 462
    }
  },
  "68": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_68",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "s. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks. we achieve new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local. restricted attention mechanisms to efficiently handle large inputs and outputs",
    "chunk_index": 68,
    "start_pos": 29351,
    "end_pos": 29810,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 508
    }
  },
  "69": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_69",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "sms to efficiently handle large inputs and outputs such as images. audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https. github.com tensorflow tensor2tensor Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments. corrections and inspiration. References Jimmy Lei Ba. Jamie Ryan Kiros. and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv.1607.06450 2016.",
    "chunk_index": 69,
    "start_pos": 29811,
    "end_pos": 30284,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 513
    }
  },
  "70": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_70",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "alization. arXiv preprint arXiv.1607.06450 2016. Dzmitry Bahdanau. Kyunghyun Cho. and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR abs 1409.0473. 2014. Denny Britz. Anna Goldie. Minh-Thang Luong. and Quoc Le. Massive exploration of neural machine translation architectures. CoRR abs 1703.03906. 2017. Jianpeng Cheng. Li Dong. and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv.1601.06733 2016. 10",
    "chunk_index": 70,
    "start_pos": 30285,
    "end_pos": 30740,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 485
    }
  },
  "71": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_71",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "eading. arXiv preprint arXiv.1601.06733 2016. 10 --- Page 11 --- Kyunghyun Cho. Bart van Merrienboer. Caglar Gulcehre. Fethi Bougares. Holger Schwenk. and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR abs 1406.1078. 2014. Francois Chollet. Xception. Deep learning with depthwise separable convolutions. arXiv preprint arXiv.1610.02357 2016. Junyoung Chung. aglar Gu lc ehre. Kyunghyun Cho. and Yoshua Bengio. Empirical evaluation",
    "chunk_index": 71,
    "start_pos": 30742,
    "end_pos": 31203,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 498
    }
  },
  "72": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_72",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ghyun Cho. and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR abs 1412.3555. 2014. Chris Dyer. Adhiguna Kuncoro. Miguel Ballesteros. and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL 2016. Jonas Gehring. Michael Auli. David Grangier. Denis Yarats. and Yann N. Dauphin. Convolu- tional sequence to sequence learning. arXiv preprint arXiv.1705.03122v2 2017. 10 Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv.1308.0850 2013.",
    "chunk_index": 72,
    "start_pos": 31204,
    "end_pos": 31703,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 534
    }
  },
  "73": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_73",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "networks. arXiv preprint arXiv.1308.0850 2013. 11 Kaiming He. Xiangyu Zhang. Shaoqing Ren. and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition pages 770 778. 2016. 12 Sepp Hochreiter. Yoshua Bengio. Paolo Frasconi. and Ju rgen Schmidhuber. Gradient flow in recurrent nets. the difficulty of learning long-term dependencies. 2001. 13 Sepp Hochreiter and Ju rgen Schmidhuber. Long short-term memory. Neural computation .1735 1780. 1997.",
    "chunk_index": 73,
    "start_pos": 31704,
    "end_pos": 32194,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 525
    }
  },
  "74": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_74",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "memory. Neural computation .1735 1780. 1997. 14 Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing pages 832 841. ACL. August 2009. 15 Rafal Jozefowicz. Oriol Vinyals. Mike Schuster. Noam Shazeer. and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv.1602.02410 2016. 16 \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention. In Advances in Neural",
    "chunk_index": 74,
    "start_pos": 32195,
    "end_pos": 32689,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 529
    }
  },
  "75": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_75",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ve memory replace attention. In Advances in Neural Information Processing Systems. NIPS 2016. 17 \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations ICLR 2016. 18 Nal Kalchbrenner. Lasse Espeholt. Karen Simonyan. Aaron van den Oord. Alex Graves. and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv.1610.10099v2 2017. 19 Yoon Kim. Carl Denton. Luong Hoang. and Alexander M. Rush. Structured attention networks.",
    "chunk_index": 75,
    "start_pos": 32690,
    "end_pos": 33169,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 514
    }
  },
  "76": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_76",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "Alexander M. Rush. Structured attention networks. InInternational Conference on Learning Representations 2017. 20 Diederik Kingma and Jimmy Ba. Adam. method for stochastic optimization. In ICLR 2015. 21 Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv.1703.10722 2017. 22 Zhouhan Lin. Minwei Feng. Cicero Nogueira dos Santos. Mo Yu. Bing Xiang. Bowen Zhou. and Yoshua Bengio. structured self-attentive sentence embedding. arXiv preprint arXiv.1703.03130 2017.",
    "chunk_index": 76,
    "start_pos": 33170,
    "end_pos": 33646,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 508
    }
  },
  "77": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_77",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "embedding. arXiv preprint arXiv.1703.03130 2017. 23 Minh-Thang Luong. Quoc Le. Ilya Sutskever. Oriol Vinyals. and Lukasz Kaiser. Multi-task sequence to sequence learning. arXiv preprint arXiv.1511.06114 2015. 24 Minh-Thang Luong. Hieu Pham. and Christopher Manning. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv.1508.04025 2015. 11",
    "chunk_index": 77,
    "start_pos": 33647,
    "end_pos": 33987,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 391,
      "word_count": 53,
      "optimized_for_embedding": true,
      "original_content_length": 391,
      "optimized_content_length": 375
    }
  },
  "78": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_78",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "lation. arXiv preprint arXiv.1508.04025 2015. 11 --- Page 12 --- 25 Mitchell Marcus. Mary Ann Marcinkiewicz. and Beatrice Santorini. Building large annotated corpus of english. The penn treebank. Computational linguistics 19 .313 330. 1993. 26 David McClosky. Eugene Charniak. and Mark Johnson. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference of the NAACL. Main Conference pages 152 159. ACL. June 2006.",
    "chunk_index": 78,
    "start_pos": 33989,
    "end_pos": 34405,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 467,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 467,
      "optimized_content_length": 451
    }
  },
  "79": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_79",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "Main Conference pages 152 159. ACL. June 2006. 27 Ankur Parikh. Oscar Ta ckstro m. Dipanjan Das. and Jakob Uszkoreit. decomposable attention model. In Empirical Methods in Natural Language Processing 2016. 28 Romain Paulus. Caiming Xiong. and Richard Socher. deep reinforced model for abstractive summarization. arXiv preprint arXiv.1705.04304 2017. 29 Slav Petrov. Leon Barrett. Romain Thibaux. and Dan Klein. Learning accurate. compact. and interpretable tree annotation. In Proceedings of the 21st International Conference on",
    "chunk_index": 79,
    "start_pos": 34406,
    "end_pos": 34899,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 528
    }
  },
  "80": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_80",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "roceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL pages 433 440. ACL. July 2006. 30 Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv.1608.05859 2016. 31 Rico Sennrich. Barry Haddow. and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv.1508.07909 2015. 32 Noam Shazeer. Azalia Mirhoseini. Krzysztof Maziarz. Andy Davis. Quoc Le. Geoffrey Hinton.",
    "chunk_index": 80,
    "start_pos": 34900,
    "end_pos": 35370,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 509
    }
  },
  "81": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_81",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "tof Maziarz. Andy Davis. Quoc Le. Geoffrey Hinton. and Jeff Dean. Outrageously large neural networks. The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv.1701.06538 2017. 33 Nitish Srivastava. Geoffrey Hinton. Alex Krizhevsky. Ilya Sutskever. and Ruslan Salakhutdi- nov. Dropout. simple way to prevent neural networks from overfitting. Journal of Machine Learning Research 15 .1929 1958. 2014. 34 Sainbayar Sukhbaatar. Arthur Szlam. Jason Weston. and Rob Fergus. End-to-end memory",
    "chunk_index": 81,
    "start_pos": 35371,
    "end_pos": 35828,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 494
    }
  },
  "82": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_82",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "m. Jason Weston. and Rob Fergus. End-to-end memory networks. In C. Cortes. N. D. Lawrence. D. D. Lee. M. Sugiyama. and R. Garnett. editors. Advances in Neural Information Processing Systems 28 pages 2440 2448. Curran Associates. Inc. 2015. 35 Ilya Sutskever. Oriol Vinyals. and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems pages 3104 3112. 2014. 36 Christian Szegedy. Vincent Vanhoucke. Sergey Ioffe. Jonathon Shlens. and Zbigniew Wojna.",
    "chunk_index": 82,
    "start_pos": 35829,
    "end_pos": 36296,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 509
    }
  },
  "83": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_83",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "Sergey Ioffe. Jonathon Shlens. and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR abs 1512.00567. 2015. 37 Vinyals Kaiser. Koo. Petrov. Sutskever. and Hinton. Grammar as foreign language. In Advances in Neural Information Processing Systems 2015. 38 Yonghui Wu. Mike Schuster. Zhifeng Chen. Quoc Le. Mohammad Norouzi. Wolfgang Macherey. Maxim Krikun. Yuan Cao. Qin Gao. Klaus Macherey. et al. Google neural machine",
    "chunk_index": 83,
    "start_pos": 36297,
    "end_pos": 36709,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 463,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 463,
      "optimized_content_length": 447
    }
  },
  "84": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_84",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ao. Klaus Macherey. et al. Google neural machine translation system. Bridging the gap between human and machine translation. arXiv preprint arXiv.1609.08144 2016. 39 Jie Zhou. Ying Cao. Xuguang Wang. Peng Li. and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR abs 1606.04199. 2016. 40 Muhua Zhu. Yue Zhang. Wenliang Chen. Min Zhang. and Jingbo Zhu. Fast and accurate shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL Volume",
    "chunk_index": 84,
    "start_pos": 36710,
    "end_pos": 37180,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 510
    }
  },
  "85": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_85",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ings of the 51st Annual Meeting of the ACL Volume 1. Long Papers pages 434 443. ACL. August 2013. 12",
    "chunk_index": 85,
    "start_pos": 37181,
    "end_pos": 37234,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 104,
      "word_count": 20,
      "optimized_for_embedding": true,
      "original_content_length": 104,
      "optimized_content_length": 100
    }
  },
  "86": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_86",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "Long Papers pages 434 443. ACL. August 2013. 12 --- Page 13 --- Attention Visualizations Input-Input Layer5 It is in this spirit that majority of American governments have passed new laws since 2009 making the registration or voting process more difficult EOS pad pad pad pad pad pad It is in this spirit that majority of American governments have passed new laws since 2009 making the registration or voting process more difficult EOS pad pad pad pad pad pad",
    "chunk_index": 86,
    "start_pos": 37236,
    "end_pos": 37683,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 459
    }
  },
  "87": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_87",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "ficult EOS pad pad pad pad pad pad Figure 3. An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer of 6. Many of the attention heads attend to distant dependency of the verb making completing the phrase making.more difficult Attentions here shown only for the word making Different colors represent different heads. Best viewed in color. 13",
    "chunk_index": 87,
    "start_pos": 37684,
    "end_pos": 38064,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 431,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 431,
      "optimized_content_length": 400
    }
  },
  "88": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_88",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "epresent different heads. Best viewed in color. 13 --- Page 14 --- Input-Input Layer5 The Law will never be perfect but its application should be just this is what we are missing in my opinion EOS pad The Law will never be perfect but its application should be just this is what we are missing in my opinion EOS pad Input-Input Layer5 The Law will never be perfect but its application should be just this is what we are missing in my opinion EOS pad The Law will never be perfect but its application should be",
    "chunk_index": 88,
    "start_pos": 38066,
    "end_pos": 38562,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 109,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 509
    }
  },
  "89": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_89",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "never be perfect but its application should be just this is what we are missing in my opinion EOS pad Figure 4. Two attention heads. also in layer of 6. apparently involved in anaphora resolution. Top. Full attentions for head 5. Bottom. Isolated attentions from just the word its for attention heads and 6. Note that the attentions are very sharp for this word. 14",
    "chunk_index": 89,
    "start_pos": 38563,
    "end_pos": 38896,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 384,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 384,
      "optimized_content_length": 365
    }
  },
  "90": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_90",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "at the attentions are very sharp for this word. 14 --- Page 15 --- Input-Input Layer5 The Law will never be perfect but its application should be just this is what we are missing in my opinion EOS pad The Law will never be perfect but its application should be just this is what we are missing in my opinion EOS pad Input-Input Layer5 The Law will never be perfect but its application should be just this is what we are missing in my opinion EOS pad The Law will never be perfect but its application should be",
    "chunk_index": 90,
    "start_pos": 38898,
    "end_pos": 39394,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 111,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 509
    }
  },
  "91": {
    "chunk_id": "dc28d773-ecf7-4567-9920-bd9ab4879819_chunk_91",
    "document_id": "dc28d773-ecf7-4567-9920-bd9ab4879819",
    "content": "never be perfect but its application should be just this is what we are missing in my opinion EOS pad Figure 5. Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above. from two different heads from the encoder self-attention at layer of 6. The heads clearly learned to perform different tasks. 15",
    "chunk_index": 91,
    "start_pos": 39395,
    "end_pos": 39731,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 387,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 387,
      "optimized_content_length": 372
    }
  },
  "92": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_0",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "--- Page --- Comprehensive Overview of Large Language Models Humza Naveeda. Asad Ullah Khanb. Shi Qiuc. Muhammad Saqibd.e. Saeed Anwarf.g. Muhammad Usmanf.g. Naveed Akhtarh.j. Nick Barnesi. Ajmal Mianj aThe University of Sydney. Sydney. Australia bUniversity of Engineering and Technology UET Lahore. Pakistan cThe Chinese University of Hong Kong CUHK HKSAR. China dUniversity of Technology Sydney UTS Sydney. Australia",
    "chunk_index": 0,
    "start_pos": 0,
    "end_pos": 438,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 438,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 438,
      "optimized_content_length": 419
    }
  },
  "93": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_1",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "sity of Technology Sydney UTS Sydney. Australia eCommonwealth Scientific and Industrial Research Organisation CSIRO Sydney. Australia fKing Fahd University of Petroleum and Minerals KFUPM Dhahran. Saudi Arabia gSDAIA-KFUPM Joint Research Center for Artificial Intelligence JRCAI Dhahran. Saudi Arabia hThe University of Melbourne UoM Melbourne. Australia iAustralian National University ANU Canberra. Australia jThe University of Western Australia UWA Perth. Australia Abstract",
    "chunk_index": 1,
    "start_pos": 439,
    "end_pos": 886,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 477
    }
  },
  "94": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_2",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Western Australia UWA Perth. Australia Abstract Large Language Models LLMs have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations. better training strategies. context length improvements. fine-tuning. multi-modal LLMs.",
    "chunk_index": 2,
    "start_pos": 887,
    "end_pos": 1267,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 431,
      "word_count": 56,
      "optimized_for_embedding": true,
      "original_content_length": 431,
      "optimized_content_length": 424
    }
  },
  "95": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_3",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ength improvements. fine-tuning. multi-modal LLMs. robotics. datasets. benchmarking. fficiency. and more. With the rapid development of techniques and regular breakthroughs in LLM research. it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs. it is imperative that the research community is able to benefit from concise",
    "chunk_index": 3,
    "start_pos": 1268,
    "end_pos": 1657,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 440,
      "word_count": 63,
      "optimized_for_embedding": true,
      "original_content_length": 440,
      "optimized_content_length": 436
    }
  },
  "96": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_4",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "search community is able to benefit from concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only",
    "chunk_index": 4,
    "start_pos": 1658,
    "end_pos": 2040,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 433,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 433,
      "optimized_content_length": 427
    }
  },
  "97": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_5",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "review article is intended to provide not only systematic survey but also quick. comprehensive reference for the researchers and practitioners to draw insights from extensive. informative summaries of the existing works to advance the LLM research. Keywords. Large Language Models. LLMs. chatGPT. Augmented LLMs. Multimodal LLMs. LLM training. LLM Benchmarking 1. Introduction Language plays fundamental role in facilitating commu- nication and self-expression for humans and their interaction",
    "chunk_index": 5,
    "start_pos": 2041,
    "end_pos": 2491,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 493
    }
  },
  "98": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_6",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "self-expression for humans and their interaction with machines. The need for generalized models stems from the growing demand for machines to handle complex language tasks. including translation. summarization. information re- trieval. conversational interactions. etc. Recently. significant breakthroughs have been witnessed in language models. pri- marily attributed to transformers increased computational capabilities. and the availability of large-scale training data. These developments have brought about revolutionary trans-",
    "chunk_index": 6,
    "start_pos": 2492,
    "end_pos": 2982,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 532
    }
  },
  "99": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_7",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "lopments have brought about revolutionary trans- formation by enabling the creation of LLMs that can approxi- mate human-level performance on various tasks 2. Large Equal contribution Email addresses. humza_naveed yahoo.com Humza Naveed aukhanee gmail.com Asad Ullah Khan shiqiu cse.cuhk.edu.hk Shi Qiu muhammad.saqib data61.csiro.au Muhammad Saqib saeed.anwar kfupm.edu.sa Saeed Anwar muhammad.usman kfupm.edu.sa Muhammad Usman naveed.akhtar1 unimelb.edu.au Naveed Akhtar",
    "chunk_index": 7,
    "start_pos": 2983,
    "end_pos": 3433,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 52,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 472
    }
  },
  "100": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_8",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "naveed.akhtar1 unimelb.edu.au Naveed Akhtar nick.barnes anu.edu.au Nick Barnes ajmal.mian uwa.edu.au Ajmal Mian Figure 1. The trend of papers released over the years containing keywords Large Language Model Large Language Model Fine-Tuning and Large Language Model Alignment Preprint submitted to Elsevier October 18. 2024arXiv.2307.06435v10 cs.CL 17 Oct 2024",
    "chunk_index": 8,
    "start_pos": 3434,
    "end_pos": 3769,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 386,
      "word_count": 45,
      "optimized_for_embedding": true,
      "original_content_length": 386,
      "optimized_content_length": 359
    }
  },
  "101": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_9",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "18. 2024arXiv.2307.06435v10 cs.CL 17 Oct 2024 --- Page --- 2019T5 Oct GPT-3 May WebGPT Dec OPT-IML TK-Instruct May mT0 Dec Wizard-LM Vicuna Alpaca Mar HuaTuo Apr Koala May Wizard-Coder Jun Goat PanGu-\u03b1 Apr CPM-2 Jun GPT-NeoX-20B Apr CodeGen Mar Galactica Nov GLM Oct OPT UL2 May LLaMA Feb LLaMA Jul MPT Jun CodeT5 Code Llama Aug StarCoder Xuan Yuan 2.0 May 20202021202220232024mT5 Oct HyperCLOVA Sep ERNIE 3.0 Codex Jul Jurassic-1 Aug Yuan 1.0 Oct Gopher Dec ERNIE 3.0 Titan GLaM LaMDA",
    "chunk_index": 9,
    "start_pos": 3771,
    "end_pos": 4269,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 485
    }
  },
  "102": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_10",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "1.0 Oct Gopher Dec ERNIE 3.0 Titan GLaM LaMDA T0 Oct ChatGPT Nov Sparrow Sep FLAN-U-PaLM Oct Bard Oct MT-NLG Jan AlphaCode Feb Chinchilla Mar PaLM Apr U-PALM Oct BLOOM Nov AlexaTM Aug PaLM2 May GPT-4 PanGu-\u03a3 Mar BloombergGPT Claude Gemini Dec DeepSeek Jan LLaMA Grok-1 Mar Snowflake Arctic Apr DeepSeek-V2 May Mixtral 8x22B Nemotron Feb GPT-4o May OpenAI o1 Sep Gemini-1.5 Feb Grok-1.5 Apr",
    "chunk_index": 10,
    "start_pos": 4270,
    "end_pos": 4663,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 444,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 443,
      "optimized_content_length": 389
    }
  },
  "103": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_11",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "OpenAI o1 Sep Gemini-1.5 Feb Grok-1.5 Apr Figure 2. Chronological display of LLM releases. blue cards represent pre-trained models. while orange cards correspond to instruction-tuned models. Models on the upper half signify open-source availability. whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned and open-source models. highlighting the evolving landscape and trends in natural language processing research. Language Models LLMs have emerged as cutting-edge arti-",
    "chunk_index": 11,
    "start_pos": 4664,
    "end_pos": 5161,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 533
    }
  },
  "104": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_12",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Models LLMs have emerged as cutting-edge arti- ficial intelligence systems that can process and generate text with coherent communication and generalize to multiple tasks 5. The historical progress in natural language processing NLP evolved from statistical to neural language modeling and then from pre-trained language models PLMs to LLMs. While conventional language modeling LM trains task-specific mod- els in supervised settings. PLMs are trained in self-supervised",
    "chunk_index": 12,
    "start_pos": 5162,
    "end_pos": 5603,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 471
    }
  },
  "105": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_13",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ed settings. PLMs are trained in self-supervised setting on large corpus of text 7. 8. with the aim of learning generic representation that is shareable among various NLP tasks. After fine-tuning for downstream tasks. PLMs surpass the performance gains of traditional language modeling LM The larger PLMs bring more performance gains. which has led to the transitioning of PLMs to LLMs by significantly increas- ing model parameters tens to hundreds of billions 10 and training dataset many GBs and TBs 10. 11 Following this",
    "chunk_index": 13,
    "start_pos": 5604,
    "end_pos": 6099,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 524
    }
  },
  "106": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_14",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ataset many GBs and TBs 10. 11 Following this development. numerous LLMs have been proposed in the lit- erature 10. 11. 12. 6. 13. 14. 15 An increasing trend in the number of released LLMs and names of few significant LLMs proposed over the years are shown in Fig and Fig 2. respec- tively. The early work on LLMs. such as T5 10 and mT5 11 em- ployed transfer learning until GPT-3 showed LLMs are zero-shot transferable to downstream tasks without fine-tuning. LLMs accurately respond to task queries when prompted with",
    "chunk_index": 14,
    "start_pos": 6100,
    "end_pos": 6588,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 519
    }
  },
  "107": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_15",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "urately respond to task queries when prompted with task descriptions and examples. However. pre-trained LLMs fail to follow user intent and perform worse in zero-shot set- tings than in few-shot. Fine-tuning them with task instruc- tions data 16. 17. 18. 19 and aligning with human prefer- ences 20. 21 enhances generalization to unseen tasks. im- proving zero-shot performance significantly and reducing mis- aligned behavior. In addition to better generalization and domain adaptation. LLMs appear to have emergent abilities. such as reasoning.",
    "chunk_index": 15,
    "start_pos": 6589,
    "end_pos": 7088,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 546
    }
  },
  "108": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_16",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ear to have emergent abilities. such as reasoning. planning. decision-making. in-context learning. answering in zero-shot settings. etc. These abilities are known to be ac- quired by them due to their gigantic scale even when the pre- trained LLMs are not trained specifically to possess these at- tributes 22. 23. 24 Such abilities have led LLMs to be widely adopted in diverse settings. including multi-modal. robotics.tool manipulation. question answering. autonomous agents. etc. Various improvements have also been suggested in these areas",
    "chunk_index": 16,
    "start_pos": 7089,
    "end_pos": 7585,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 544
    }
  },
  "109": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_17",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "provements have also been suggested in these areas either by task-specific training 25. 26. 27. 28. 29. 30. 31 or better prompting 32 The LLMs abilities to solve diverse tasks with human-level performance come at the cost of slow training and inference. extensive hardware requirements. and higher running costs. Such requirements have limited their adoption and opened up opportunities to devise better architectures 15. 33. 34. 35 and training strategies 36. 37. 21. 38. 39. 40. 41 Param-",
    "chunk_index": 17,
    "start_pos": 7586,
    "end_pos": 8035,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 490
    }
  },
  "110": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_18",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng strategies 36. 37. 21. 38. 39. 40. 41 Param- eter fficient tuning 38. 41. 40 pruning 42. 43 quantiza- tion 44. 45 knowledge distillation. and context length inter- polation 46. 47. 48. 49 among others are some of the methods widely studied for fficient LLM utilization. Due to the success of LLMs on wide variety of tasks. the research literature has recently experienced large influx of LLM-related contributions. Researchers have organized the LLMs literature in surveys 50. 51. 52. 53 and topic-specific",
    "chunk_index": 18,
    "start_pos": 8036,
    "end_pos": 8519,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 509
    }
  },
  "111": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_19",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "re in surveys 50. 51. 52. 53 and topic-specific surveys in 54. 55. 56. 57. 58 In contrast to these surveys. our contribution focuses on providing comprehensive yet concise overview of the general direction of LLM research. This arti- cle summarizes architectural and training details of pre-trained LLMs and delves deeper into the details of concepts like fine- tuning. multi-modal LLMs. augmented LLMs. datasets. eval- uation. applications. challenges. and others to provide self- contained comprehensive overview. Our key contributions are",
    "chunk_index": 19,
    "start_pos": 8520,
    "end_pos": 9020,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 541
    }
  },
  "112": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_20",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "comprehensive overview. Our key contributions are summarized as follows. We present survey on the developments in LLM research. providing concise. comprehensive overview of the direc- tion. We present extensive summaries of pre-trained models that include fine-grained details of architecture and training de- tails. We summarize major findings of the popular contributions and provide detailed discussion on the key design and development aspects of LLMs to help practitioners ffec- tively leverage this technology.",
    "chunk_index": 20,
    "start_pos": 9021,
    "end_pos": 9498,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 516
    }
  },
  "113": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_21",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "titioners ffec- tively leverage this technology. In this self-contained article. we cover range of con- cepts to present the general direction of LLMs compre- hensively. including background. pre-training. fine-tuning.",
    "chunk_index": 21,
    "start_pos": 9499,
    "end_pos": 9673,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 225,
      "word_count": 32,
      "optimized_for_embedding": true,
      "original_content_length": 225,
      "optimized_content_length": 218
    }
  },
  "114": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_22",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "including background. pre-training. fine-tuning. --- Page --- Figure 3. broader overview of LLMs. dividing LLMs into seven branches. 1. Pre-Training 2. Fine-Tuning 3. fficient 4. Inference 5. Evaluation 6. Applications 7. Challenges multi-modal LLMs. augmented LLMs. LLMs-powered agents. datasets. evaluation. etc. We loosely follow the existing terminology to ensure stan- dardized outlook of this research direction. For instance. fol- lowing 50 our survey discusses pre-trained LLMs with 10B",
    "chunk_index": 22,
    "start_pos": 9675,
    "end_pos": 10131,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 494
    }
  },
  "115": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_23",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "our survey discusses pre-trained LLMs with 10B parameters or more. We refer the readers interested in smaller pre-trained models to 51. 52. 53 The organization of this paper is as follows. Section discusses the background of LLMs. Section focuses on LLMs overview. architectures. training pipelines and strategies. fine-tuning. andutilization in di fferent domains. Section highlights the config- uration and parameters that play crucial role in the function- ing of these models. Summary and discussions are presented",
    "chunk_index": 23,
    "start_pos": 10132,
    "end_pos": 10614,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 518
    }
  },
  "116": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_24",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "hese models. Summary and discussions are presented in section 3.8. The LLM training and evaluation. datasets. and benchmarks are discussed in section 5. followed by challenges and future directions. and conclusion in sections and 8. re- spectively.",
    "chunk_index": 24,
    "start_pos": 10615,
    "end_pos": 10816,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 252,
      "word_count": 39,
      "optimized_for_embedding": true,
      "original_content_length": 252,
      "optimized_content_length": 248
    }
  },
  "117": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_25",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "conclusion in sections and 8. re- spectively. --- Page --- 2. Background We provide the relevant background to understand the fun- damentals related to LLMs in this section. We briefly discuss necessary components in LLMs and refer the readers interested in details to the original works. 2.1. Tokenization Tokenization 59 is an essential pre-processing step in LLM training that parses the text into non-decomposing units called tokens. Tokens can be characters. subwords 60 sym-",
    "chunk_index": 25,
    "start_pos": 10818,
    "end_pos": 11259,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 480
    }
  },
  "118": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_26",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ens. Tokens can be characters. subwords 60 sym- bols 61 or words. depending on the tokenization process. Some of the commonly used tokenization schemes in LLMs include wordpiece 62 byte pair encoding BPE 61 and un- igramLM 60 Readers are encouraged to refer to 63 for detailed survey. 2.2. Encoding Positions The transformer processes input sequences in parallel and independently of each other. Moreover. the attention mod- ule in the transformer does not capture positional information.",
    "chunk_index": 26,
    "start_pos": 11260,
    "end_pos": 11718,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 488
    }
  },
  "119": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_27",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ansformer does not capture positional information. As result. positional encodings were introduced in trans- former 64 where positional embedding vector is added to the token embedding. Variants of positional embedding include absolute. relative. or learned positional encodings. Within rel- ative encoding. Alibi and RoPE are two widely used positional embeddings in LLMs. Alibi 65 It subtracts scalar bias from the attention score that increases with the distance between token positions. This favors using recent tokens for attention.",
    "chunk_index": 27,
    "start_pos": 11719,
    "end_pos": 12217,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 537
    }
  },
  "120": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_28",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ns. This favors using recent tokens for attention. RoPE 66 It rotates query and key representations at an an- gle proportional to the token absolute position in the input sequence. resulting in relative positional encoding scheme which decays with the distance between the tokens. 2.3. Attention in LLMs Attention assigns weights to input tokens based on impor- tance so that the model gives more emphasis to relevant tokens. Attention in transformers 64 calculates query. key. and value",
    "chunk_index": 28,
    "start_pos": 12218,
    "end_pos": 12661,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 487
    }
  },
  "121": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_29",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "transformers 64 calculates query. key. and value mappings for input sequences. where the attention score is obtained by multiplying the query and key. and later used to weight values. We discuss di fferent attention strategies used in LLMs below. Self-Attention 64 Calculates attention using queries. keys. and values from the same block encoder or decoder Cross Attention. It is used in encoder-decoder architectures. where encoder outputs are the queries. and key-value pairs come from the decoder.",
    "chunk_index": 29,
    "start_pos": 12662,
    "end_pos": 13119,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 500
    }
  },
  "122": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_30",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ueries. and key-value pairs come from the decoder. Sparse Attention 67 Self-attention has n2 time complex- ity which becomes infeasible for large sequences. To speed up the computation. sparse attention 67 iteratively calculates attention in sliding windows for speed gains. Flash Attention 68 Memory access is the major bottleneck in calculating attention using GPUs. To speed up. flash attention employs input tiling to minimize the memory reads and writes between the GPU high bandwidth memory HBM",
    "chunk_index": 30,
    "start_pos": 13120,
    "end_pos": 13582,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 500
    }
  },
  "123": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_31",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "writes between the GPU high bandwidth memory HBM and the on-chip SRAM.2.4. Activation Functions The activation functions serve crucial role in the curve- fitting abilities of neural networks 69 We discuss activation functions used in LLMs in this section. ReLU 70 The Rectified linear unit ReLU is defined as. ReLU max 0.x GeLU 71 The Gaussian Error Linear Unit GeLU is the combination of ReLU. dropout 72 and zoneout 73 GLU variants 74 The Gated Linear Unit 75 is neural",
    "chunk_index": 31,
    "start_pos": 13583,
    "end_pos": 14041,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 471
    }
  },
  "124": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_32",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "iants 74 The Gated Linear Unit 75 is neural network layer that is an element-wise product of linear transformation and sigmoid transformed linear projection of the input given as. GLU x.W.V.b.c xW xV where Xis the input of layer and l.W.b.Vandcare learned parameters. Other GLU variants 74 used in LLMs are. ReGLU x.W.V.b.c max 0.xW GEGLU x.W.V.b.c GELU xW xV wiGLU x.W.V.b.c.\u03b2 wish\u03b2 xW xV 2.5. Layer Normalization Layer normalization leads to faster convergence and is an in-",
    "chunk_index": 32,
    "start_pos": 14042,
    "end_pos": 14538,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 476
    }
  },
  "125": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_33",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "lization leads to faster convergence and is an in- tegrated component of transformers 64 In addition to Layer- Norm 76 and RMSNorm 77 LLMs use pre-layer normal- ization 78 applying it before multi-head attention MHA Pre-norm is shown to provide training stability in LLMs. An- other normalization variant. DeepNorm 79 fixes the issue with larger gradients in pre-norm. 2.6. Distributed LLM Training This section describes distributed LLM training approaches briefly. More details are available in 13. 37. 80. 81",
    "chunk_index": 33,
    "start_pos": 14539,
    "end_pos": 15018,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 511
    }
  },
  "126": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_34",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "y. More details are available in 13. 37. 80. 81 Data Parallelism. Data parallelism replicates the model on multiple devices where data in batch gets divided across de- vices. At the end of each training iteration weights are synchro- nized across all devices. Tensor Parallelism. Tensor parallelism shards tensor compu- tation across devices. It is also known as horizontal parallelism or intra-layer model parallelism. Pipeline Parallelism. Pipeline parallelism shards model layers",
    "chunk_index": 34,
    "start_pos": 15019,
    "end_pos": 15457,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 489,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 489,
      "optimized_content_length": 482
    }
  },
  "127": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_35",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "allelism. Pipeline parallelism shards model layers across di fferent devices. This is also known as vertical paral- lelism. Model Parallelism. combination of tensor and pipeline par- allelism is known as model parallelism. 3D Parallelism. combination of data. tensor. and model par- allelism is known as 3D parallelism. Optimizer Parallelism. Optimizer parallelism also known as zero redundancy optimizer 37 implements optimizer state partitioning. gradient partitioning. and parameter partitioning",
    "chunk_index": 35,
    "start_pos": 15458,
    "end_pos": 15911,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 498
    }
  },
  "128": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_36",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "gradient partitioning. and parameter partitioning across devices to reduce memory consumption while keeping the communication costs as low as possible.",
    "chunk_index": 36,
    "start_pos": 15912,
    "end_pos": 16015,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 154,
      "word_count": 21,
      "optimized_for_embedding": true,
      "original_content_length": 153,
      "optimized_content_length": 151
    }
  },
  "129": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_37",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ping the communication costs as low as possible. --- Page --- 2.7. Libraries Some commonly used libraries for LLMs training are. Transformers 82 The library provides access to various pre- trained transformer models with APIs to train. fine-tune. infer. and develop custom models. DeepSpeed 36 library for scalable distributed training and inference of deep learning models. Megatron-LM 80 It provides GPU-optimized techniques for large-scale training of LLMs. JAX 83 Python library for high-performance numerical",
    "chunk_index": 37,
    "start_pos": 16017,
    "end_pos": 16499,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 513
    }
  },
  "130": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_38",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Python library for high-performance numerical computing and scaleable machine learning. It can di fferenti- ate native Python and NumPy functions and execute them on GPUs. Colossal-AI 84 collection of components to write dis- tributed deep learning models. BMTrain 81 library to write fficient stand-alone LLMs training code. FastMoE 85 Provides API to build mixture-of-experts MoE model in PyTorch. MindSpore 86 deep learning training and inference frame- work extendable to mobile. edge. and cloud computing.",
    "chunk_index": 38,
    "start_pos": 16500,
    "end_pos": 16986,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 510
    }
  },
  "131": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_39",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "extendable to mobile. edge. and cloud computing. PyTorch 87 framework developed by Facebook AI Re- search lab FAIR to build deep learning models. The main features of PyTorch include dynamic computation graph and pythonic coding style. Tensorflow 88 deep learning framework written by Google. The key features of TensorFlow are graph-based com- putation. eager execution. scalability. etc. MXNet 89 Apache MXNet is deep learning framework with support to write programs in multiple languages. includ-",
    "chunk_index": 39,
    "start_pos": 16987,
    "end_pos": 17459,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 500
    }
  },
  "132": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_40",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "to write programs in multiple languages. includ- ing. Python. Scala. R. etc. It also provides support for dynamic and static computation graphs. 2.8. Data PreProcessing This section briefly summarizes data preprocessing tech- niques used in LLMs training. Quality Filtering. For better results. training data quality is essential. Some approaches to filtering data are. classifier- based and heuristics-based. Classifier-based approaches train classifier on high-quality data and predict the quality of",
    "chunk_index": 40,
    "start_pos": 17460,
    "end_pos": 17927,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 502
    }
  },
  "133": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_41",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "er on high-quality data and predict the quality of text for filtering. whereas heuristics-based employ some rules for filtering like language. metrics. statistics. and keywords. Data Deduplication. Duplicated data can ffect model per- formance and increase data memorization. therefore. to train LLMs. data deduplication is one of the preprocessing steps. This can be performed at multiple levels. like sentences. documents. and datasets. Privacy Reduction. Most of the training data for LLMs is",
    "chunk_index": 41,
    "start_pos": 17928,
    "end_pos": 18374,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 495
    }
  },
  "134": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_42",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Reduction. Most of the training data for LLMs is collected through web sources. This data contains private information. therefore. many LLMs employ heuristics-based methods to filter information such as names. addresses. and phone numbers to avoid learning personal information. 2.9. Architectures Here we discuss the variants of the transformer architectures used in LLMs. The di fference arises due to the application of Figure 4. An example of attention patterns in language models. image is taken from 93",
    "chunk_index": 42,
    "start_pos": 18375,
    "end_pos": 18837,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 508
    }
  },
  "135": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_43",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "erns in language models. image is taken from 93 Figure 5. An example of language model training objectives. image from 93 the attention and the connection of transformer blocks. An il- lustration of attention patterns of these architectures is shown in Figure 4. Encoder Decoder. This architecture processes inputs through the encoder and passes the intermediate representation to the decoder to generate the output. Here. the encoder sees the complete sequence utilizing self-attention whereas the decoder",
    "chunk_index": 43,
    "start_pos": 18838,
    "end_pos": 19299,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 506
    }
  },
  "136": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_44",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "uence utilizing self-attention whereas the decoder processes the sequence one after the other with implementing cross-attention. Causal Decoder. type of architecture that does not have an encoder and processes and generates output using decoder. where the predicted token depends only on the previous time steps. Prefix Decoder. It is also known as non-causal decoder. where the attention calculation is not strictly dependent on the past information and the attention is bidirectional. An example",
    "chunk_index": 44,
    "start_pos": 19300,
    "end_pos": 19752,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 503,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 503,
      "optimized_content_length": 497
    }
  },
  "137": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_45",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ion and the attention is bidirectional. An example of non-causal attention mask is shown in Figure 4. Mixture-of-Experts. It is variant of transformer architecture with parallel independent experts and router to route tokens to experts. These experts are feed-forward layers after the at- tention block 90 Mixture-of-Experts MoE is an fficient sparse architecture that ffers comparable performance to dense models and allows increasing the model size without increas- ing the computational cost by activating only few experts at",
    "chunk_index": 45,
    "start_pos": 19753,
    "end_pos": 20249,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 528
    }
  },
  "138": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_46",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ational cost by activating only few experts at time 91. 92 2.10. Pre-Training Objectives This section describes LLMs pre-training objectives. For more details see the paper 93 Full Language Modeling. An autoregressive language model- ing objective where the model is asked to predict future tokens given the previous tokens. an example is shown in Figure 5. Prefix Language Modeling. non-causal training objective. where prefix is chosen randomly and only remaining target tokens are used to calculate the loss. An example is shown in",
    "chunk_index": 46,
    "start_pos": 20250,
    "end_pos": 20747,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 534
    }
  },
  "139": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_47",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "used to calculate the loss. An example is shown in Figure 5.",
    "chunk_index": 47,
    "start_pos": 20748,
    "end_pos": 20759,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 62,
      "word_count": 13,
      "optimized_for_embedding": true,
      "original_content_length": 62,
      "optimized_content_length": 60
    }
  },
  "140": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_48",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Figure 5. --- Page --- Figure 6. basic flow diagram depicting various stages of LLMs from pre-training to prompting utilization. Prompting LLMs to generate responses is possible at different training stages like pre-training. instruction-tuning. or alignment tuning. RL stands for reinforcement learning. RM represents reward-modeling. and RLHF represents reinforcement learning with human feedback. Masked Language Modeling. In this training objective. tokens",
    "chunk_index": 48,
    "start_pos": 20761,
    "end_pos": 21222,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 473,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 473,
      "optimized_content_length": 460
    }
  },
  "141": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_49",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "guage Modeling. In this training objective. tokens or spans sequence of tokens are masked randomly and the model is asked to predict masked tokens given the past and future context. An example is shown in Figure 5. Unified Language Modeling. Unified language modeling 94 is combination of causal. non-causal. and masked language training objectives. Here in masked language modeling. the attention is not bidirectional but unidirectional. attending either left-to-right or right-to-left context. 2.11. LLMs Scaling Laws",
    "chunk_index": 49,
    "start_pos": 21223,
    "end_pos": 21699,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 519
    }
  },
  "142": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_50",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "or right-to-left context. 2.11. LLMs Scaling Laws Scaling laws study the optimal combination of model param- eters. dataset size. and computational resources that predict the improvement in the model performance. It has been shown that the loss scales according to the power-law with model size. dataset size. and compute resources 95 This study suggests larger models are more important than big data for better perfor- mance. Another variant of scaling law 96 suggests the model",
    "chunk_index": 50,
    "start_pos": 21700,
    "end_pos": 22135,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 480
    }
  },
  "143": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_51",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "her variant of scaling law 96 suggests the model size and the number of training tokens should be scaled equally.2.12. LLMs Adaptation Stages This section discusses the fundamentals of LLMs adaptation stages. from pre-training to fine-tuning for downstream tasks and utilization. An example of di fferent training stages and in- ference in LLMs is shown in Figure 6. In this paper. we refer to alignment-tuning as aligning with human preferences. while occasionally the literature uses the term alignment for di fferent purposes.",
    "chunk_index": 51,
    "start_pos": 22136,
    "end_pos": 22616,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 529
    }
  },
  "144": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_52",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "uses the term alignment for di fferent purposes. 2.12.1. Pre-Training In the very first stage. the model is trained in self- supervised manner on large corpus to predict the next to- kens given the input. The design choices of LLMs vary from encoder-decoder to decoder-only architectures with di fferent building blocks and loss functions in sections 2.5. 2.4. 2.10. 2.12.2. Fine-Tuning There are di fferent styles to fine-tune an LLM. This section briefly discusses fine-tuning approaches.",
    "chunk_index": 52,
    "start_pos": 22617,
    "end_pos": 23062,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 490
    }
  },
  "145": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_53",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "section briefly discusses fine-tuning approaches. Transfer Learning. The pre-trained LLMs perform well for various tasks 6. 15 However. to improve the performance for",
    "chunk_index": 53,
    "start_pos": 23063,
    "end_pos": 23184,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 172,
      "word_count": 24,
      "optimized_for_embedding": true,
      "original_content_length": 171,
      "optimized_content_length": 166
    }
  },
  "146": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_54",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "6. 15 However. to improve the performance for --- Page --- downstream task. pre-trained models are fine-tuned with the task-specific data 10. 11 known as transfer learning. Instruction-tuning. To enable model to respond to user queries ffectively. the pre-trained model is fine-tuned on in- struction formatted data i.e. instruction and an input-output pair. Instructions generally comprise multi-task data in plain natural language. guiding the model to respond according to the",
    "chunk_index": 54,
    "start_pos": 23186,
    "end_pos": 23631,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 479
    }
  },
  "147": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_55",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "age. guiding the model to respond according to the prompt and the input. This type of fine-tuning improves zero- shot generalization and downstream task performance. Details on formatting instruction data and its various styles are avail- able in 16. 50. 97 Alignment-tuning. LLMs are prone to generating false. biased. and harmful text. To make them helpful. honest. and harmless. models are aligned using human feedback. Alignment involves asking LLMs to generate unexpected responses and then updat-",
    "chunk_index": 55,
    "start_pos": 23632,
    "end_pos": 24086,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 502
    }
  },
  "148": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_56",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "to generate unexpected responses and then updat- ing their parameters to avoid such responses 20. 21. 98 It ensures LLMs operate according to human intentions and values. model is defined to be an aligned model if the model fulfills three criteria of helpful. honest. and harmless or HHH 99 Researchers employ reinforcement learning with human feed- back RLHF 100 for model alignment. In RLHF. fine-tuned model on demonstrations is further trained with reward model- ing RM and reinforcement learning RL shown in Figure 6.",
    "chunk_index": 56,
    "start_pos": 24087,
    "end_pos": 24583,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 522
    }
  },
  "149": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_57",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nd reinforcement learning RL shown in Figure 6. Below we briefly discuss RM and RL pipelines in RLHF. Reward modeling. trains model to rank generated responses according to human preferences using classification objec- tive. To train the classifier humans annotate LLMs generated responses based on the HHH criteria. Reinforcement learning. in combination with the reward model is used for alignment in the next stage. The previously trained reward model ranks LLM-generated responses into preferred",
    "chunk_index": 57,
    "start_pos": 24584,
    "end_pos": 25039,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 499
    }
  },
  "150": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_58",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "model ranks LLM-generated responses into preferred vs. non-preferred. which is used to align the model with proxi- mal policy optimization PPO This process repeats iteratively until convergence. 2.12.3. Prompting Utilization Prompting is method to query trained LLMs for generating responses. as illustrated in Figure 6. LLMs can be prompted in various prompt setups. where they can be adapted to the instruc- tions without fine-tuning and in other cases with fine-tuning on data containing di fferent prompt styles 16. 101. 102 good",
    "chunk_index": 58,
    "start_pos": 25040,
    "end_pos": 25533,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 533
    }
  },
  "151": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_59",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng di fferent prompt styles 16. 101. 102 good guide on prompt engineering is available at 32 Below. we will discuss various widely used prompt setups. Zero-Shot Prompting. LLMs are zero-shot learners and ca- pable of answering queries never seen before. This style of prompting requires LLMs to answer user questions without see- ing any examples in the prompt. In-context Learning. Also known as few-shot learning. here. multiple input-output demonstration pairs are shown to the model to generate the desired response. This adaptation style",
    "chunk_index": 59,
    "start_pos": 25534,
    "end_pos": 26033,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 542
    }
  },
  "152": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_60",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nerate the desired response. This adaptation style is also called few-shot learning. discussion on formatting in- context learning ICL templates is available in 54. 50. 18. 16 Reasoning in LLMs. LLMs are zero-shot reasoners and can be provoked to generate answers to logical problems. task planning. critical thinking. etc. with reasoning. Generating reasons is possible only by using di fferent prompting styles.whereas to improve LLMs further on reasoning tasks many methods 16. 97 train them on reasoning datasets. We discuss",
    "chunk_index": 60,
    "start_pos": 26034,
    "end_pos": 26520,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 528
    }
  },
  "153": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_61",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "97 train them on reasoning datasets. We discuss various prompting techniques for reasoning below. Chain-of-Thought CoT special case of prompting where demonstrations contain reasoning information aggregated with inputs and outputs so that the model generates outcomes with step-by-step reasoning. More details on CoT prompts are avail- able in 55. 103. 101 Self-Consistency. Improves CoT performance by generat- ing multiple responses and selecting the most frequent an- swer 104 Tree-of-Thought ToT Explores multiple reasoning paths",
    "chunk_index": 61,
    "start_pos": 26521,
    "end_pos": 27020,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 533
    }
  },
  "154": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_62",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "f-Thought ToT Explores multiple reasoning paths with possibilities to look ahead and backtrack for problem- solving 105 Single-Turn Instructions. In this prompting setup. LLMs are queried only once with all the relevant information in the prompt. LLMs generate responses by understanding the con- text either in zero-shot or few-shot setting. Multi-Turn Instructions. Solving complex task requires mul- tiple interactions with LLMs. where feedback and responses from the other tools are given as input to the LLM for the next",
    "chunk_index": 62,
    "start_pos": 27021,
    "end_pos": 27505,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 525
    }
  },
  "155": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_63",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tools are given as input to the LLM for the next rounds. This style of using LLMs in the loop is common in autonomous agents. 3. Large Language Models This section reviews LLMs. briefly describing their architec- tures. training objectives. pipelines. datasets. and fine-tuning details. 3.1. Pre-Trained LLMs Here. we provide summaries of various well-known pre- trained LLMs with significant discoveries. changing the course of research and development in NLP. These LLMs have consid- erably improved the performance in NLU and NLG domains.",
    "chunk_index": 63,
    "start_pos": 27506,
    "end_pos": 27998,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 541
    }
  },
  "156": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_64",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "improved the performance in NLU and NLG domains. and are widely fine-tuned for downstream tasks. Moreover. We also identify key findings and insights of pre-trained LLMs in Table and that improve their performance. 3.1.1. General Purpose T5 10 An encoder-decoder model employing unified text- to-text training for all NLP problems is shown in Figure 7. T5 places layer normalization outside the residual path in conven- tional transformer model 64 It uses masked language mod-",
    "chunk_index": 64,
    "start_pos": 27999,
    "end_pos": 28440,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 476
    }
  },
  "157": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_65",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ansformer model 64 It uses masked language mod- eling as pre-training objective where spans consecutive to- kens are replaced with single mask instead of separate masks for each token. This type of masking speeds up the training as it produces shorter sequences. After pre-training. the model is fine-tuned using adapter layers 106 for downstream tasks. GPT-3 The GPT-3 architecture is the same as the GPT- but with dense and sparse attention in transformer layers similar to the Sparse Transformer 67 It shows that large mod-",
    "chunk_index": 65,
    "start_pos": 28441,
    "end_pos": 28941,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 526
    }
  },
  "158": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_66",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Sparse Transformer 67 It shows that large mod- els can train on larger batch sizes with lower learning rate to decide the batch size during training. GPT-3 uses the gradient noise scale as in 107 Overall. GPT-3 increases model param- eters to 175B showing that the performance of large language",
    "chunk_index": 66,
    "start_pos": 28942,
    "end_pos": 29196,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 305,
      "word_count": 52,
      "optimized_for_embedding": true,
      "original_content_length": 304,
      "optimized_content_length": 294
    }
  },
  "159": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_67",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "showing that the performance of large language --- Page --- Figure 7. Unified text-to-text training example. source image from 10 Figure 8. The image is the article of 108 showing an example of PanGu- architecture. models improves with the scale and is competitive with the fine- tuned models. mT5 11 multilingual T5 model 10 trained on the mC4 dataset with 101 languages. The dataset is extracted from the public common crawl scrape. The model uses larger vocab- ulary size of 250.000 to cover multiple languages. To avoid",
    "chunk_index": 67,
    "start_pos": 29198,
    "end_pos": 29693,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 523
    }
  },
  "160": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_68",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "of 250.000 to cover multiple languages. To avoid over-fitting or under-fitting for language. mT5 employs data sampling procedure to select samples from all languages. The paper suggests using small amount of pre-training datasets. including all languages when fine-tuning for task using En- glish language data. This allows the model to generate correct non-English outputs. PanGu-\u03b1 108 An autoregressive model that has query layer at the end of standard transformer layers. example shown",
    "chunk_index": 68,
    "start_pos": 29694,
    "end_pos": 30145,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 488
    }
  },
  "161": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_69",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "end of standard transformer layers. example shown in Figure 8. to predict the next token. Its structure is similar to the transformer layer but with an additional embedding for the next position in the attention mechanism. given in Eq. 3. pnWq hWk hT HT CPM-2 12 Cost-e fficient Pre-trained language Models CPM-2 pre-trains bilingual English and Chinese 11B and 198B mixture-of-experts MoE models on the WuDaoCor- pus 109 dataset. The tokenization process removes white",
    "chunk_index": 69,
    "start_pos": 30146,
    "end_pos": 30588,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 469
    }
  },
  "162": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_70",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ataset. The tokenization process removes white space tokens in the sentencepiece tokenizer. The models are trained with knowledge inheritance. starting with only the Chi- nese language in the first stage and then adding English and Chinese data. This trained model gets duplicated multiple times to initialize the 198B MoE model. Moreover. to use the model for downstream tasks. CPM-2 experimented with both com-plete fine-tuning and prompt fine-tuning as in 40 where only prompt-related parameters are updated by inserting prompts at",
    "chunk_index": 70,
    "start_pos": 30589,
    "end_pos": 31078,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 534
    }
  },
  "163": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_71",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ted parameters are updated by inserting prompts at various positions. front. middle. and back. CPM-2 also pro- poses the INFMOE. memory-e fficient framework with strat- egy to dynamically ffload parameters to the CPU for inference at 100B scale. It overlaps data movement with inference com- putation for lower inference time. ERNIE 3.0 110 ERNIE 3.0 takes inspiration from multi- task learning to build modular architecture using Transformer- XL 111 as the backbone. The universal representation mod-",
    "chunk_index": 71,
    "start_pos": 31079,
    "end_pos": 31544,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 501
    }
  },
  "164": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_72",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "as the backbone. The universal representation mod- ule is shared by all the tasks. which serve as the basic block for task-specific representation modules. which are all trained jointly for natural language understanding. natural language generation. and knowledge extraction. This LLM is primar- ily focused on the Chinese language. It claims to train on the largest Chinese text corpora for LLM training. and achieved state-of-the-art in 54 Chinese NLP tasks. Jurassic-1 112 pair of auto-regressive language mod-",
    "chunk_index": 72,
    "start_pos": 31545,
    "end_pos": 32013,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 514
    }
  },
  "165": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_73",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "c-1 112 pair of auto-regressive language mod- els. including 7B-parameter J1-Large model and 178B- parameter J1-Jumbo model. The training vocabulary of Jurassic-1 comprise word pieces. complete words. and multi- word expressions without any word boundaries. where possible out-of-vocabulary instances are interpreted as Unicode bytes. Compared to the GPT-3 counterparts. the Jurassic-1 models apply more balanced depth-to-width self-attention architec- ture 113 and an improved tokenizer for faster prediction",
    "chunk_index": 73,
    "start_pos": 32014,
    "end_pos": 32487,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 509
    }
  },
  "166": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_74",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "and an improved tokenizer for faster prediction based on broader resources. achieving comparable perfor- mance in zero-shot learning tasks and superior performance in few-shot learning tasks given the ability to feed more examples as prompt. HyperCLOVA 114 Korean language model with GPT-3 architecture. Yuan 1.0 115 Trained on Chinese corpus with 5TB of high-quality text collected from the Internet. Massive Data Filtering System MDFS built on Spark is developed to pro-",
    "chunk_index": 74,
    "start_pos": 32488,
    "end_pos": 32932,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 472
    }
  },
  "167": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_75",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "System MDFS built on Spark is developed to pro- cess the raw data via coarse and fine filtering techniques. To speed up the training of Yuan 1.0 to save energy expenses and carbon emissions. various factors that improve the performance of distributed training are incorporated in architecture and train- ing. like increasing the hidden state size improves pipeline and tensor parallelism performance. larger micro batches improve pipeline parallelism performance. and larger global batch size",
    "chunk_index": 75,
    "start_pos": 32933,
    "end_pos": 33377,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 492
    }
  },
  "168": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_76",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "allelism performance. and larger global batch size improve data parallelism performance. In practice. the Yuan 1.0 model performs well on text classification. Winograd Schema. natural language inference. and reading comprehension tasks. Gopher 116 The Gopher family of models ranges from 44M to 280B parameters in size to study the ffect of scale on the LLMs performance. The 280B model beats GPT-3 Jurrasic-1 112 MT-NLG 117 and others on 81 of the evaluated tasks. ERNIE 3.0 TITAN 35 ERNIE 3.0 Titan extends ERNIE 3.0",
    "chunk_index": 76,
    "start_pos": 33378,
    "end_pos": 33865,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 518
    }
  },
  "169": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_77",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "3.0 TITAN 35 ERNIE 3.0 Titan extends ERNIE 3.0 by training larger model with 26x the number of parameters of the latter. This bigger model outperformed other state-of-the- art models in 68 NLP tasks. LLMs produce text with incorrect facts. In order to have control of the generated text with fac- tual consistency. ERNIE 3.0 Titan adds another task. Credible and Controllable Generations to its multi-task learning setup.",
    "chunk_index": 77,
    "start_pos": 33866,
    "end_pos": 34246,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 431,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 430,
      "optimized_content_length": 421
    }
  },
  "170": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_78",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Generations to its multi-task learning setup. --- Page --- It introduces additional self-supervised adversarial and control- lable language modeling losses to the pre-training step. which enables ERNIE 3.0 Titan to beat other LLMs in their manually selected Factual QA task set evaluations. GPT-NeoX-20B 118 An auto-regressive model that largely follows GPT-3 with few deviations in architecture design. trained on the Pile dataset without any data deduplication. GPT- NeoX has parallel attention and feed-forward layers in trans-",
    "chunk_index": 78,
    "start_pos": 34248,
    "end_pos": 34741,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 530
    }
  },
  "171": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_79",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "llel attention and feed-forward layers in trans- former block. given in Eq. 4. that increases throughput by 15 It uses rotary positional embedding 66 applying it to only 25 of embedding vector dimension as in 119 This reduces the computation without performance degradation. As opposed to GPT-3. which uses dense and sparse layers. GPT-NeoX-20B uses only dense layers. The hyperparameter tuning at this scale is difficult. therefore. the model chooses hyperparameters from the method and interpolates values between 13B and 175B",
    "chunk_index": 79,
    "start_pos": 34742,
    "end_pos": 35234,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 528
    }
  },
  "172": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_80",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "and interpolates values between 13B and 175B models for the 20B model. The model training is distributed among GPUs using both tensor and pipeline parallelism. Attn LN1 FF LN2 OPT 14 It is clone of GPT-3. developed to open-source model that replicates GPT-3 performance. Training of OPT employs dynamic loss scaling 120 and restarts from an earlier checkpoint with lower learning rate whenever loss divergence is observed. Overall. the performance of OPT-175B models is comparable to the GPT3-175B model.",
    "chunk_index": 80,
    "start_pos": 35235,
    "end_pos": 35719,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 504
    }
  },
  "173": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_81",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "-175B models is comparable to the GPT3-175B model. BLOOM 13 causal decoder model trained on the ROOTS corpus to open-source an LLM. The architecture of BLOOM is shown in Figure 9. with di fferences like ALiBi positional em- bedding. an additional normalization layer after the embedding layer as suggested by the bitsandbytes1library. These changes stabilize training with improved downstream performance. GLaM 91 Generalist Language Model GLaM represents family of language models using sparsely activated decoder-",
    "chunk_index": 81,
    "start_pos": 35720,
    "end_pos": 36198,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 515
    }
  },
  "174": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_82",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "anguage models using sparsely activated decoder- only mixture-of-experts MoE structure 121. 90 To gain more model capacity while reducing computation. the experts are sparsely activated where only the best two experts are used to process each input token. The largest GLaM model. GLaM 64B 64E is about larger than GPT-3 while only part of the parameters are activated per input token. The largest GLaM 64B 64E model achieves better overall results as compared to GPT-3 while consuming only one-third of GPT-3 training energy.",
    "chunk_index": 82,
    "start_pos": 36199,
    "end_pos": 36694,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 525
    }
  },
  "175": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_83",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nsuming only one-third of GPT-3 training energy. MT-NLG 117 530B causal decoder based on the GPT- architecture that has roughly GPT-3 model parameters. MT-NLG is trained on filtered high-quality data collected from various public datasets and blends various types of datasets in single batch. which beats GPT-3 on several evaluations. Chinchilla 96 causal decoder trained on the same dataset as the Gopher 116 but with little di fferent data sampling distribution sampled from MassiveText The model architec-",
    "chunk_index": 83,
    "start_pos": 36695,
    "end_pos": 37178,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 508
    }
  },
  "176": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_84",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "on sampled from MassiveText The model architec- ture is similar to the one used for Gopher. with the exception of AdamW optimizer instead of Adam. Chinchilla identifies the 1https. github.com TimDettmers bitsandbytes Figure 9. The BLOOM architecture example sourced from 13 relationship that model size should be doubled for every dou- bling of training tokens. Over 400 language models ranging from 70 million to over 16 billion parameters on to 500 bil- lion tokens are trained to get the estimates for compute-optimal",
    "chunk_index": 84,
    "start_pos": 37179,
    "end_pos": 37660,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 520
    }
  },
  "177": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_85",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "trained to get the estimates for compute-optimal training under given budget. The authors train 70B model with the same compute budget as Gopher 280B but with times more data. It outperforms Gopher 116 GPT-3 and others on various downstream tasks. after fine-tuning. AlexaTM 122 An encoder-decoder model. where encoder weights and decoder embeddings are initialized with pre- trained encoder to speed up training. The encoder stays frozen for the initial 100k steps and is later unfrozen for end-to-end",
    "chunk_index": 85,
    "start_pos": 37661,
    "end_pos": 38135,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 502
    }
  },
  "178": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_86",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "al 100k steps and is later unfrozen for end-to-end training. The model is trained on combination of denoising and causal language modeling CLM objectives. concatenat- ing CLM token at the beginning for mode switching. Dur- ing training. the CLM task is applied for 20 of the time. which improves the in-context learning performance. PaLM 15 causal decoder with parallel attention and feed-forward layers similar to Eq. 4. speeding up training by factor of 15. Additional changes to the conventional trans-",
    "chunk_index": 86,
    "start_pos": 38136,
    "end_pos": 38608,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 505
    }
  },
  "179": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_87",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "15. Additional changes to the conventional trans- former model include SwiGLU activation. RoPE embeddings. multi-query attention that saves computation cost during decod- ing. and shared input-output embeddings. During training. loss spiking was observed. and to fix it. model training was restarted from 100-step earlier checkpoint by skipping 200-500 batches around the spike. Moreover. the model was found to memo- rize around 2.4 of the training data at the 540B model scale. whereas this number was lower for smaller models.",
    "chunk_index": 87,
    "start_pos": 38609,
    "end_pos": 39091,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 529
    }
  },
  "180": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_88",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "whereas this number was lower for smaller models. PaLM-2 123 smaller multi-lingual variant of PaLM. trained for larger iterations on better quality dataset. PaLM- shows significant improvements over PaLM. while reducing training and inference costs due to its smaller size. To lessen toxicity and memorization. it appends special tokens with fraction of pre-training data. which shows reduction in gener- ating harmful responses. U-PaLM 124 This method trains PaLM for 0.1 addi-",
    "chunk_index": 88,
    "start_pos": 39092,
    "end_pos": 39537,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 478
    }
  },
  "181": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_89",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "PaLM 124 This method trains PaLM for 0.1 addi- tional compute with the UL2 also named as UL2Restore ob- jective 125 using the same dataset it outperforms the baseline significantly on various NLP tasks. including zero-shot. few- shot. commonsense reasoning. CoT. etc. Training with UL2R involves converting causal decoder PaLM to non-causal de- coder PaLM and employing 50 sequential denoising. 25 regular denoising. and 25 extreme denoising loss functions.",
    "chunk_index": 89,
    "start_pos": 39538,
    "end_pos": 39962,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 475,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 475,
      "optimized_content_length": 457
    }
  },
  "182": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_90",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ising. and 25 extreme denoising loss functions. --- Page 10 --- UL2 125 An encoder-decoder architecture trained using mixture of denoisers MoD objective. Denoisers include R-Denoiser. regular span masking. S-Denoiser. which cor- rupts consecutive tokens of large sequence and X-Denoiser. which corrupts large number of tokens randomly. During pre- training. UL2 includes denoiser token from R.S.Xto rep- resent denoising setup. It helps improve fine-tuning perfor-",
    "chunk_index": 90,
    "start_pos": 39964,
    "end_pos": 40406,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 464
    }
  },
  "183": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_91",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "oising setup. It helps improve fine-tuning perfor- mance for downstream tasks that bind the task to one of the up- stream training modes. This MoD style of training outperforms the T5 model on many benchmarks. GLM-130B 33 GLM-130B is bilingual English and Chi- nese model trained using an auto-regressive mask infilling pre- training objective similar to the GLM 126 This training style makes the model bidirectional as compared to GPT-3. which is unidirectional. As opposed to GLM. the training of GLM-130B",
    "chunk_index": 91,
    "start_pos": 40407,
    "end_pos": 40873,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 507
    }
  },
  "184": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_92",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ional. As opposed to GLM. the training of GLM-130B includes small amount of multi-task instruction pre-training data of the total data along with self-supervised mask in- filling. To stabilize the training. it applies embedding layer gra- dient shrink. LLaMA 127. 21 set of decoder-only language models varying from 7B to 70B parameters. LLaMA models series is the most famous among the community for parameter fficiency and instruction tuning. LLaMA-1 127 Implements fficient causal attention 128",
    "chunk_index": 92,
    "start_pos": 40874,
    "end_pos": 41341,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 497
    }
  },
  "185": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_93",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "127 Implements fficient causal attention 128 by not storing and computing masked attention weights and key query scores. Another optimization is reducing the number of activations recomputed in the backward pass. as in 129 LLaMA-2 21 This work is more focused on fine-tuning safer and better LLaMA-2-Chat model for dialogue generation. The pre-trained model has 40 more training data with larger context length and grouped-query attention. LLaMA-3 3.1 130 collection of models trained on",
    "chunk_index": 93,
    "start_pos": 41342,
    "end_pos": 41802,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 487
    }
  },
  "186": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_94",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "A-3 3.1 130 collection of models trained on seven times larger dataset as compared to LLaMA-2 with dou- ble the context length. outperforming its previous variants and other models. PanGu- 92 An autoregressive model with parameters copied from PanGu- \u03b1and extended to trillion scale with Ran- dom Routed Experts RRE the architectural diagram is shown in Figure 10. RRE is similar to the MoE architecture. with distinctions at the second level. where tokens are randomly routed to experts in domain instead of using learnable gat-",
    "chunk_index": 94,
    "start_pos": 41803,
    "end_pos": 42301,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 529
    }
  },
  "187": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_95",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "erts in domain instead of using learnable gat- ing method. The model has bottom layers densely activated and shared across all domains. whereas top layers are sparsely ac- tivated according to the domain. This training style allows for extracting task-specific models and reduces catastrophic forget- ting ffects in the case of continual learning. Mixtral8x22b 131 mixture-of-experts MoE model with eight distinct experts routes each token to two experts at each layer and combines the outputs additively.",
    "chunk_index": 95,
    "start_pos": 42302,
    "end_pos": 42769,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 505
    }
  },
  "188": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_96",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "at each layer and combines the outputs additively. Snowflake Arctic 132 Arctic LLM is hybrid of dense and mixture-of-experts MoE architecture. The MoE 128 3.66B MLP experts is parallel to the dense transformer 10B with only two experts activated. The model has many experts. com- pared to other MoE LLMs 131. 133 to increase the model capacity and provide an opportunity to choose among many ex- perts for diverse configuration. The model has 480B param-",
    "chunk_index": 96,
    "start_pos": 42770,
    "end_pos": 43190,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 471,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 471,
      "optimized_content_length": 454
    }
  },
  "189": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_97",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "diverse configuration. The model has 480B param- eters. and only 17B are active during forward pass. reducingthe computation significantly. Grok 133. 134 Grok is family of LLMs including Grok-1 and Grok-1.5. released by XAI. Grok-1 133 Grok-1 is 314B parameters language MoE model eight experts where two experts are activated per to- ken. Grok-1.5 134 Grok-1.5 is multi-modal LLM with larger context length and improved performance. Gemini 135. 136 Gemini replaces Bard based on PaLM",
    "chunk_index": 97,
    "start_pos": 43191,
    "end_pos": 43653,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 484
    }
  },
  "190": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_98",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "135. 136 Gemini replaces Bard based on PaLM with multi-modal capabilities and significant language model- ing performance improvements. Gemini-1 135 The first-ever auto-regressive model to achieve human-level capabilities on the MMLU benchmark. Gemini-1.5 136 multi-modal LLM with MoE architec- ture builds on the findings of Gemini-1. The model has 2M context window and can reason over information up to 10M tokens. Such large context windows were never achieved pre- viously and shown to have huge impact on performance gain.",
    "chunk_index": 98,
    "start_pos": 43654,
    "end_pos": 44150,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 528
    }
  },
  "191": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_99",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "shown to have huge impact on performance gain. Nemotron-4 340B 137 decoder-only model that has been aligned on 98 synthetic data and only manually annotated data. Utilizing synthetic data at large proportion improves the model performance significantly. The paper suggested intro- ducing alignment data with smaller subset of previously seen data during the late stage of the model pre-training. enabling the smooth transition from the pre-trained stage to the final train-",
    "chunk_index": 99,
    "start_pos": 44151,
    "end_pos": 44590,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 473
    }
  },
  "192": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_100",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ion from the pre-trained stage to the final train- ing stage. To train better instruction-following models. weaker models are trained into stronger models iteratively. The syn- thetic data generated by the weaker instruction-tuned model is used to train base model which is later supervised fine-tuned outperforming the weaker model. DeepSeek 138 DeepSeek studies the LLMs scaling laws in detail to determine the optimal non-embedding model size and training data. The experiments were performed for bud-",
    "chunk_index": 100,
    "start_pos": 44591,
    "end_pos": 45051,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 504
    }
  },
  "193": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_101",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng data. The experiments were performed for bud- gets ranging from 1e17to 3e20training FLOPs. Each compute budget was tested against ten di fferent models data scales. The batch size and learning rates were also fitted for the given com- pute budget finding that the batch size should increase with the increased compute budget while decreasing the learning rate. Following are the equations for the optimal batch-size learning rate model size and data Bopt 0.2920.C0.3271 \u03b7opt 0.3118.C 0.1250 Mopt Mbase.Ca Dopt Dbase.Cb",
    "chunk_index": 101,
    "start_pos": 45052,
    "end_pos": 45549,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 521
    }
  },
  "194": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_102",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "\u03b7opt 0.3118.C 0.1250 Mopt Mbase.Ca Dopt Dbase.Cb Mbase 0.1715.Dbase 5.8316.a 0.5243.b 0.4757 DeepSeek-v2 139 An MoE model that introduces multi- head latent attention MLA to reduce inference costs. by com- pressing Key-Value KV cache into latent vector. MLA achieves better performance than multi-head attention MHA and other fficient attention mechanisms such as grouped query attention GQA multi-query attention MQA etc. Because of MLA. DeepSeek-v2 achieves 5.76 times faster inference",
    "chunk_index": 102,
    "start_pos": 45550,
    "end_pos": 46011,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 487
    }
  },
  "195": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_103",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "DeepSeek-v2 achieves 5.76 times faster inference throughput as compared to DeepSeek 138 10",
    "chunk_index": 103,
    "start_pos": 46012,
    "end_pos": 46056,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 95,
      "word_count": 14,
      "optimized_for_embedding": true,
      "original_content_length": 95,
      "optimized_content_length": 90
    }
  },
  "196": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_104",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "throughput as compared to DeepSeek 138 10 --- Page 11 --- 3.1.2. Coding CodeGen 140 CodeGen has similar architecture to PaLM 15 i.e. parallel attention. MLP layers. and RoPE em- beddings. The model is trained on both natural language and programming language data sequentially trained on the first dataset. then the second. and so on on the following datasets PILE. BIGQUERY and BIGPYTHON. CodeGen pro- posed multi-step approach to synthesizing code. The purpose",
    "chunk_index": 104,
    "start_pos": 46058,
    "end_pos": 46502,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 489,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 489,
      "optimized_content_length": 462
    }
  },
  "197": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_105",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ti-step approach to synthesizing code. The purpose is to simplify the generation of long sequences where the previ- ous prompt and generated code are given as input with the next prompt to generate the next code sequence. CodeGen open- source Multi-Turn Programming Benchmark MTPB to eval- uate multi-step program synthesis. Codex 141 This LLM is trained on subset of public Python Github repositories to generate code from docstrings. Com- puter programming is an iterative process where the programs",
    "chunk_index": 105,
    "start_pos": 46503,
    "end_pos": 46962,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 501
    }
  },
  "198": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_106",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ramming is an iterative process where the programs are often debugged and updated before fulfilling the require- ments. Similarly. Codex generates 100 versions of program by repetitive sampling for given description. which produces working solution for 77.5 of the problems passing unit tests. Its powerful version powers Github Copilot2. AlphaCode 142 set of large language models. ranging from 300M to 41B parameters. designed for competition-level code generation tasks. It uses the multi-query attention 143 to",
    "chunk_index": 106,
    "start_pos": 46963,
    "end_pos": 47440,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 514
    }
  },
  "199": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_107",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tasks. It uses the multi-query attention 143 to reduce memory and cache costs. Since competitive program- ming problems highly require deep reasoning and an under- standing of complex natural language algorithms. the Alpha- Code models are pre-trained on filtered GitHub code in popular languages and then fine-tuned on new competitive program- ming dataset named CodeContests. The CodeContests dataset mainly contains problems. solutions. and test cases collected from the Codeforces platform3. The pre-training employs stan-",
    "chunk_index": 107,
    "start_pos": 47441,
    "end_pos": 47921,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 526
    }
  },
  "200": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_108",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "deforces platform3. The pre-training employs stan- dard language modeling objectives. while GOLD 144 with tempering 145 serves as the training objective for the fine- tuning on CodeContests data. To evaluate the performance of AlphaCode. simulated programming competitions are hosted on the Codeforces platform. overall. AlphaCode ranks at the top 54.3 among over 5000 competitors. where its Codeforces rating is within the top 28 of recently participated users. CodeT5 34 CodeT5 is based on CodeT5 146 with",
    "chunk_index": 108,
    "start_pos": 47922,
    "end_pos": 48392,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 507
    }
  },
  "201": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_109",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "deT5 34 CodeT5 is based on CodeT5 146 with shallow encoder and deep decoder. trained in multiple stages initially unimodal data code and later bimodal data text-code pairs Each training stage has di fferent training objectives and activates di fferent model blocks encoder. decoder. or both ac- cording to the task. The unimodal pre-training includes span denoising and CLM objectives. whereas bimodal pre-training objectives contain contrastive learning. matching. and CLM for",
    "chunk_index": 109,
    "start_pos": 48393,
    "end_pos": 48832,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 477
    }
  },
  "202": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_110",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ontain contrastive learning. matching. and CLM for text-code pairs. CodeT5 adds special tokens with the text to enable task modes. for example. CLS for contrastive loss. Match for text-code matching. etc. StarCoder 147 decoder-only model with the SantaCoder architecture. employing Flash attention to scale up the context length to 8k. The StarCoder trains an encoder to filter names. 2https. github.com features copilot 3https. codeforces.com emails. and other personal data from the training data. Its fine-",
    "chunk_index": 110,
    "start_pos": 48833,
    "end_pos": 49311,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 509
    }
  },
  "203": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_111",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "er personal data from the training data. Its fine- tuned variant outperforms PaLM. LLaMA. and LAMDA on HumanEval and MBPP benchmarks. 3.1.3. Scientific Knowledge Galactica 148 large curated corpus of human scientific knowledge with 48 million papers. textbooks. lecture notes. millions of compounds and proteins. scientific websites. en- cyclopedias. and more are trained using the metaseq library3. which is built on PyTorch and fairscale 149 The model wraps reasoning datasets with the work token to provide step-by-",
    "chunk_index": 111,
    "start_pos": 49312,
    "end_pos": 49788,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 518
    }
  },
  "204": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_112",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "datasets with the work token to provide step-by- step reasoning context to the model. which has been shown to improve the performance on reasoning tasks. 3.1.4. Dialog LaMDA 150 decoder-only model pre-trained on pub- lic dialog data. public dialog utterances. and public web doc- uments. where more than 90 of the pre-training data is in English. LaMDA is trained with the objective of producing re- sponses that exhibit high levels of quality. safety. and grounded- ness. To achieve this. discriminative and generative fine-tuning",
    "chunk_index": 112,
    "start_pos": 49789,
    "end_pos": 50277,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 531
    }
  },
  "205": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_113",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ve this. discriminative and generative fine-tuning techniques are incorporated to enhance the model safety and quality aspects. As result. the LaMDA models can be utilized as general language model performing various tasks. 3.1.5. Finance BloombergGPT 151 non-causal decoder model trained using both financial FINPILE from the Bloomberg archive and general-purpose datasets. The model architecture is sim- ilar to the BLOOM 13 and OPT 14 It allocates 50B param- eters to di fferent blocks of the model using the approach 113",
    "chunk_index": 113,
    "start_pos": 50278,
    "end_pos": 50776,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 524
    }
  },
  "206": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_114",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rent blocks of the model using the approach 113 For ffective training. BloombergGPT packs documents to- gether with endo text to use the maximum sequence length. uses warmup batch size starting from 1024 to 2048. and manually reduces the learning rate multiple times during the training. Xuan Yuan 2.0 152 Chinese financial chat model with BLOOM 13 architecture trained on combination of general purpose. financial. general purpose instructions. and financial institutions datasets. Xuan Yuan 2.0 combined the pre-training",
    "chunk_index": 114,
    "start_pos": 50777,
    "end_pos": 51269,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 522
    }
  },
  "207": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_115",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "datasets. Xuan Yuan 2.0 combined the pre-training and fine-tuning stages to avoid catastrophic forgetting. 3.2. Fine-Tuned LLMs Pre-trained LLMs have excellent generalization abilities to unseen tasks. However. because they are generally trained with the objective of next token prediction. LLMs have limited ca- pacity to follow user intent and are prone to generate unethical. toxic or inaccurate responses 20 For their ffective utiliza- tion. LLMs are fine-tuned to follow instructions 16. 17. 97 and",
    "chunk_index": 115,
    "start_pos": 51270,
    "end_pos": 51730,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 503
    }
  },
  "208": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_116",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "fine-tuned to follow instructions 16. 17. 97 and generate safe responses 20 which also results in increasing zero-shot. few-shot. and cross-task generalization 97. 16. 18 with minimal compute increment. e.g. 0.2 of the total pre- training for PaLM 540B 16 We review various fine-tuned LLMs and strategies for ffective fine-tuning in this section. 11",
    "chunk_index": 116,
    "start_pos": 51731,
    "end_pos": 52044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 364,
      "word_count": 55,
      "optimized_for_embedding": true,
      "original_content_length": 364,
      "optimized_content_length": 349
    }
  },
  "209": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_117",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ies for ffective fine-tuning in this section. 11 --- Page 12 --- Table 1. Noteworthy findings and insights of pre-trained Large Language Models. Models Findings Insights T5 Encoder and decoder with shared parameters perform equivalently when parameters are not shared Fine-tuning model layers adapter layers work better than the conventional way of training on only classification layers GPT-3 Few-shot performance of LLMs is better than the zero-shot. suggesting that LLMs are meta- learners",
    "chunk_index": 117,
    "start_pos": 52046,
    "end_pos": 52494,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 492
    }
  },
  "210": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_118",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "zero-shot. suggesting that LLMs are meta- learners mT5 Large multi-lingual models perform equivalently to single language models on downstream tasks. However. smaller multi-lingual models perform worse PanGu-\u03b1 LLMs have good few shot capabilities CPM-2 Prompt fine-tuning requires updating very few parameters while achieving performance compara- ble to full model fine-tuning Prompt fine-tuning takes more time to converge as compared to full model fine-tuning",
    "chunk_index": 118,
    "start_pos": 52495,
    "end_pos": 52907,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 463,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 463,
      "optimized_content_length": 461
    }
  },
  "211": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_119",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "to converge as compared to full model fine-tuning Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long sequences In an analysis. CPM-2 finds that prompts work as provider additional context and aggregator aggregate information with the input text for the model ERNIE 3.0 modular LLM architecture with universal representation module and task-specific representa- tion module helps in the finetuning phase",
    "chunk_index": 119,
    "start_pos": 52908,
    "end_pos": 53337,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 479,
      "optimized_content_length": 467
    }
  },
  "212": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_120",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "resenta- tion module helps in the finetuning phase Optimizing the parameters of task-specific representation network during the fine-tuning phase is an efficient way to take advantage of the powerful pre-trained model Jurassic-1 The performance of LLM is highly related to the network size To improve runtime performance. more operations can be performed in parallel width rather than sequential depth To efficiently represent and fit more text in the same context length. the model uses larger vo-",
    "chunk_index": 120,
    "start_pos": 53338,
    "end_pos": 53796,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 498
    }
  },
  "213": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_121",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "same context length. the model uses larger vo- cabulary to train SentencePiece tokenizer without restricting it to word boundaries. This further benefits in few-shot learning tasks HyperCLOV By employing prompt-based tuning. the performances of models can be improved. often surpassing those of state-of-the-art models when the backward gradients of inputs are accessible Yuan 1.0 The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and few-shot learning",
    "chunk_index": 121,
    "start_pos": 53797,
    "end_pos": 54275,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 521
    }
  },
  "214": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_122",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "asting behavior in zero-shot and few-shot learning Gopher Relative encodings enable the model to evaluate for longer sequences than training. ERNIE 3.0 Titan Additional self-supervised adversarial loss to distinguish between real and generated text improves the model performance as compared to ERNIE 3.0 GPT-NeoX-20B Parallel attention FF layers speed-up training 15 with the same performance as with cascaded layers Initializing feed-forward output layers before residuals with scheme in 153 avoids activations",
    "chunk_index": 122,
    "start_pos": 54276,
    "end_pos": 54743,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 512
    }
  },
  "215": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_123",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "residuals with scheme in 153 avoids activations from growing with increasing depth and width Training on Pile outperforms GPT-3 on five-shot Table Continued on Next Page 12",
    "chunk_index": 123,
    "start_pos": 54744,
    "end_pos": 54869,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 176,
      "word_count": 27,
      "optimized_for_embedding": true,
      "original_content_length": 175,
      "optimized_content_length": 172
    }
  },
  "216": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_124",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "GPT-3 on five-shot Table Continued on Next Page 12 --- Page 13 --- Models Findings Insights OPT Restart training from an earlier checkpoint with lower learning rate if loss diverges Model is prone to generate repetitive text and stuck in loop Galactica Galactica performance has continued to improve across validation set. in-domain. and out-of- domain benchmarks. even with multiple repetitions of the corpus. which is superior to existing research on LLMs",
    "chunk_index": 124,
    "start_pos": 54871,
    "end_pos": 55286,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 466,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 466,
      "optimized_content_length": 457
    }
  },
  "217": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_125",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "us. which is superior to existing research on LLMs working memory token approach can achieve strong performance over existing methods on mathematical MMLU and MATH benchmarks. It sets new state-of-the-art on several downstream tasks such as PubMedQA 77.6 and MedMCQA dev 52.9 GLaM The model capacity can be maintained at reduced computation by replacing the feed-forward layer in each transformer layer with mixture-of-experts MoE The model trained on filtered data shows consistently better performances on both NLG and NLU",
    "chunk_index": 125,
    "start_pos": 55287,
    "end_pos": 55776,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 524
    }
  },
  "218": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_126",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nsistently better performances on both NLG and NLU tasks. where the ffect of filtering is more significant on the former tasks Filtered pretraining corpora play crucial role in the generation capability of LLMs. especially for the downstream tasks The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in the MoE layer. Given fixed budget of computation. more experts contribute to better perfor- mance",
    "chunk_index": 126,
    "start_pos": 55777,
    "end_pos": 56175,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 449,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 449,
      "optimized_content_length": 439
    }
  },
  "219": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_127",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "more experts contribute to better perfor- mance LaMDA The model can be fine-tuned to learn to call di fferent external information resources and tools AlphaCode For higher ffectiveness and fficiency. transformer model can be asymmetrically constructed with shallower encoder and deeper decoder To achieve better performances. it is necessary to employ strategies such as massively scaling upsampling. followed by the filtering and clustering of samples into compact set",
    "chunk_index": 127,
    "start_pos": 56176,
    "end_pos": 56611,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 469
    }
  },
  "220": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_128",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ering and clustering of samples into compact set The utilization of novel sampling-e fficient transformer architectures designed to facilitate large- scale sampling is crucial Simplifying problem descriptions can ffectively improve the model performance Chinchilla The model size and the number of training tokens should be scaled proportionately. for each dou- bling of the model size. the number of training tokens should be doubled as well",
    "chunk_index": 128,
    "start_pos": 56612,
    "end_pos": 57011,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 450,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 450,
      "optimized_content_length": 442
    }
  },
  "221": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_129",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "umber of training tokens should be doubled as well PaLM English-centric models produce better translations when translating to English as compared to non- English Generalized models can have equivalent performance for language translation to specialized small models Larger models have higher percentage of training data memorization Performance has not yet saturated even at 540B scale. which means larger models are likely to perform better",
    "chunk_index": 129,
    "start_pos": 57012,
    "end_pos": 57408,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 447,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 447,
      "optimized_content_length": 442
    }
  },
  "222": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_130",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "means larger models are likely to perform better AlexaTM Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the context than decoder-only Causal Language Modeling CLM task can be added to benefit the model with fficient in-context learning Placing layer norm at the beginning of each transformer layer improves the training stability Table Continued on Next Page 13",
    "chunk_index": 130,
    "start_pos": 57409,
    "end_pos": 57773,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 415,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 415,
      "optimized_content_length": 407
    }
  },
  "223": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_131",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "training stability Table Continued on Next Page 13 --- Page 14 --- Models Findings Insights U-PaLM Training with mixture of denoisers outperforms PaLM when trained further for few more FLOPs Training with mixture of denoisers improves the infilling ability and open-ended text generation diversity UL2 Mode switching training enables better performance on downstream tasks CoT prompting outperforms standard prompting for UL2 GLM-130B Pre-training data with small proportion of multi-task instruction data improves the overall model",
    "chunk_index": 131,
    "start_pos": 57775,
    "end_pos": 58268,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 532
    }
  },
  "224": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_132",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "i-task instruction data improves the overall model performance CodeGen Multi-step prompting for code synthesis leads to better user intent understanding and code gen- eration LLaMA constant performance improvement is observed when scaling the model Smaller models can achieve good performances with more training data and computing time PanGu- Sparse models provide the benefits of large models at lower computation cost Randomly Routed Experts reduces catastrophic forgetting ffects which in turn is essential for continual learning",
    "chunk_index": 132,
    "start_pos": 58269,
    "end_pos": 58763,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 533
    }
  },
  "225": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_133",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "which in turn is essential for continual learning Randomly Routed Experts allow extracting domain-specific sub-model in deployment which is cost-e fficient while maintaining performance similar to the original BloombergGPT Pre-training with general-purpose and task-specific data improves task performance without hurt- ing other model capabilities XuanYuan 2.0 Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting",
    "chunk_index": 133,
    "start_pos": 58764,
    "end_pos": 59177,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 464,
      "word_count": 58,
      "optimized_for_embedding": true,
      "original_content_length": 463,
      "optimized_content_length": 457
    }
  },
  "226": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_134",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "in single training avoids catastrophic forgetting CodeT5 Causal LM is crucial for model generation capability in encoder-decoder architectures Multiple training objectives like span corruption. Causal LM. matching. etc complement each other for better performance StarCoder HHH prompt by Anthropic allows the model to follow instructions without fine-tuning LLaMA-2 Model trained on unfiltered data is more toxic but may perform better on downstream tasks after fine-tuning",
    "chunk_index": 134,
    "start_pos": 59178,
    "end_pos": 59609,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 482,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 473
    }
  },
  "227": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_135",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rform better on downstream tasks after fine-tuning Model trained on unfiltered data requires fewer samples for safety alignment PaLM-2 Data quality is important to train better models Model and data size should be scaled with 1.1 proportions Smaller models trained for larger iterations outperform larger models LLaMA-3 3.1 Increasing batch size gradually stabilizes the training without loss spikes High-quality data at the final stages of training improves the model performance",
    "chunk_index": 135,
    "start_pos": 59610,
    "end_pos": 60044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 480
    }
  },
  "228": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_136",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "stages of training improves the model performance Increasing model context length windows step-wise allows it to better adapt to various sequence lengths Nemotron-40B Model aligned iteratively on synthetic data with data generated from the previously aligned model achieves competitive performance DeepSeek Batch size should increase with the increase in compute budget while decreasing the learning rate DeepSeek-v2 Mult-head latent attention MLA performs better than multi-head attention MHA while requiring",
    "chunk_index": 136,
    "start_pos": 60045,
    "end_pos": 60510,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 509
    }
  },
  "229": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_137",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "er than multi-head attention MHA while requiring significantly smaller KV cache. therefore achieving faster data generation 14",
    "chunk_index": 137,
    "start_pos": 60511,
    "end_pos": 60590,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 130,
      "word_count": 18,
      "optimized_for_embedding": true,
      "original_content_length": 130,
      "optimized_content_length": 126
    }
  },
  "230": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_138",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "che. therefore achieving faster data generation 14 --- Page 15 --- Table 2. Key insights and findings from the study of instruction-tuned Large Language Models. Models Findings Insights T0 Multi-task prompting enables zero-shot generalization and outperforms baselines Even single prompt per dataset task is enough to improve performance WebGPT To aid the model in ffectively filtering and utilizing relevant information. human labelers play crucial role in answering questions regarding the usefulness of the retrieved documents",
    "chunk_index": 138,
    "start_pos": 60592,
    "end_pos": 61079,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 529
    }
  },
  "231": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_139",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "egarding the usefulness of the retrieved documents Interacting fine-tuned language model with text-based web-browsing environment can improve end-to-end retrieval and synthesis via imitation learning and reinforcement learning Generating answers with references can make labelers easily judge the factual accuracy of answers Tk-INSTRUCT Instruction tuning leads to stronger generalization of unseen tasks More tasks improve generalization whereas only increasing task instances does not help",
    "chunk_index": 139,
    "start_pos": 61080,
    "end_pos": 61529,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 491
    }
  },
  "232": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_140",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ereas only increasing task instances does not help Supervised trained models are better than generalized models Models pre-trained with instructions and examples perform well for di fferent types of inputs mT0 and BLOOMZ Instruction tuning enables zero-shot generalization to tasks never seen before Multi-lingual training leads to even better zero-shot generalization for both English and non- English Training on machine-translated prompts improves performance for held-out tasks with non-English prompts",
    "chunk_index": 140,
    "start_pos": 61530,
    "end_pos": 61989,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 506
    }
  },
  "233": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_141",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rmance for held-out tasks with non-English prompts English only fine-tuning on multilingual pre-trained language model is enough to generalize to other pre-trained language tasks OPT-IML Creating batch with multiple task examples is important for better performance Only example proportional sampling is not enough. training datasets should also be proportional for better generalization performance Fully held-out and partially supervised tasks performance improves by scaling tasks or categories whereas fully supervised tasks have no ffect",
    "chunk_index": 141,
    "start_pos": 61990,
    "end_pos": 62489,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 542
    }
  },
  "234": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_142",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ies whereas fully supervised tasks have no ffect Including small amounts i.e. of pretraining data during fine-tuning is ffective Only reasoning data improves the performance. adding more deteriorates performance Adding dialogue data makes the performance worse Sparrow Labelers judgment and well-defined alignment rules help the model generate better responses Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters",
    "chunk_index": 142,
    "start_pos": 62490,
    "end_pos": 62919,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 480,
      "optimized_content_length": 465
    }
  },
  "235": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_143",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "atural language rules for the agent and the raters The combination of reinforcement learning RL with reranking yields optimal performance in terms of preference win rates and resilience against adversarial probing Flan Finetuning with CoT improves performance on held-out tasks Fine-tuning along with CoT data improves reasoning abilities CoT tuning improves zero-shot reasoning Performance improves with more tasks Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models",
    "chunk_index": 143,
    "start_pos": 62920,
    "end_pos": 63388,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 512
    }
  },
  "236": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_144",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ch otherwise is challenging for pre-trained models Improving the model performance with instruction tuning is compute-e fficient Multitask prompting enables zero-shot generalization abilities in LLM WizardCoder Fine-tuning with re-written instruction-tuning data into complex set improves performance LLaMA-2-Chat Model learns to write safe responses with fine-tuning on safe demonstrations. while additional RLHF step further improves model safety and make it less prone to jailbreak attacks",
    "chunk_index": 144,
    "start_pos": 63389,
    "end_pos": 63837,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 492
    }
  },
  "237": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_145",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "safety and make it less prone to jailbreak attacks LIMA Less high quality data is enough for fine-tuned model generalization 15",
    "chunk_index": 145,
    "start_pos": 63838,
    "end_pos": 63915,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 128,
      "word_count": 21,
      "optimized_for_embedding": true,
      "original_content_length": 128,
      "optimized_content_length": 127
    }
  },
  "238": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_146",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "is enough for fine-tuned model generalization 15 --- Page 16 --- Figure 10. This example illustrates the PanGu-Parchitecture. as depicted in the image sourced from 92 3.2.1. Instruction-Tuning with Manually Created Datasets Numerous hand-crafted instruction-tuning datasets with different design choices are proposed in the literature to instruction-tune LLMs. The performance of fine-tuned LLMs depends on multiple factors. such as dataset. instruction diver- sity. prompting templates. model size. and training objectives.",
    "chunk_index": 146,
    "start_pos": 63917,
    "end_pos": 64395,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 524
    }
  },
  "239": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_147",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng templates. model size. and training objectives. Keeping this in view. diverse fine-tuned models have emerged in the literature using manually created datasets. The models T0 17 and mT0 multi-lingual 154 employ templates to convert existing datasets into prompt datasets. They have shown improvements in generalization to zero-shot and held-out tasks. Tk-Instruct 18 fine-tuned the T5 model with in-context instructions to study generalization on unseen tasks when given in-context instructions during test time. The",
    "chunk_index": 147,
    "start_pos": 64396,
    "end_pos": 64871,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 518
    }
  },
  "240": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_148",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "iven in-context instructions during test time. The model outperformed Instruct-GPT. despite being smaller in size. i.e. 11B parameters as compared to 175B of GPT-3. Increasing Tasks and Prompt Setups. Zero-shot and few-shot performance improves significantly by expanding task collec- tion and prompt styles. OPT-IML 97 and Flan 16 curated larger 2k and 1.8k task datasets. respectively. While increasing task size alone is not enough. OPT-IML and Flan add more prompting setups in their datasets. zero-shot. few-shot. and",
    "chunk_index": 148,
    "start_pos": 64872,
    "end_pos": 65348,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 522
    }
  },
  "241": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_149",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "setups in their datasets. zero-shot. few-shot. and CoT. In continuation. CoT Collection 101 fine-tunes Flan-T5 further on 1.88M CoT samples. Another method 102 uses symbolic tasks with tasks in T0. Flan. etc. 3.2.2. Instruction-Tuning with LLMs Generated Datasets Generating an instruction-tuning dataset requires carefully writing instructions and input-output pairs. which are often written by humans. smaller in size. and less diverse. To over- come this. self-instruct 19 proposed an approach to prompt",
    "chunk_index": 149,
    "start_pos": 65349,
    "end_pos": 65810,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 506
    }
  },
  "242": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_150",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "self-instruct 19 proposed an approach to prompt available LLMs to generate instruction-tuning datasets. Self- instruct outperformed models trained on manually created dataset SUPER-NATURALINSTRUCTIONS dataset with 1600 tasks 18 by 33 It starts with seed of 175 tasks. instruction. and sample per task and iteratively generates new instructions 52k and instances 82k input-output pairs using Figure 11. An example image shows an instance of the Flan training paradigm. taken from 16",
    "chunk_index": 150,
    "start_pos": 65811,
    "end_pos": 66265,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 481
    }
  },
  "243": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_151",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ce of the Flan training paradigm. taken from 16 GPT-3 Contrary to this. Dynosaur 155 uses the meta-data of datasets on Huggingface to prompt LLMs to generate multi- ple task instruction-tuning datasets. LLaMA Tuned. Various models in the literature instruction- tune LLaMA 156 with GPT-3 or GPT-4 157 gener- ated datasets. Among these. Alpaca 158 Vicuna 159 and LLaMA-GPT-4 160 are few general-purpose fine-tuned models. where Alpaca is trained on 52k samples from text-",
    "chunk_index": 151,
    "start_pos": 66266,
    "end_pos": 66713,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 470
    }
  },
  "244": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_152",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "where Alpaca is trained on 52k samples from text- davinci-003. Vicuna on 70k samples from ShareGPT.com. and LLaMA-GPT-4 by re-creating Alpaca instructions from GPT- 4. Goat 161 fine-tunes LLaMA for arithmetic tasks million samples by generating data from ChatGPT and outperforms GPT-4. PaLM. BLOOM. OPT. etc. attributing its success to the LLaMA consistent tokenization of numbers. HuaTuo 162 is medical knowledge model. fine-tuned with generated QA dataset of 8k instructions.",
    "chunk_index": 152,
    "start_pos": 66714,
    "end_pos": 67156,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 477
    }
  },
  "245": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_153",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ed with generated QA dataset of 8k instructions. Complex Instructions. Evol-Instruct 163. 164 prompts LLMs to convert given instructions into more complex set. The in- structions are iteratively evolved with re-writing instructions in complex wording and creating new instructions. With this style of automated instruction generation. WizardLM 163 fine- tuned LLaMA on 250k instructions outperforms Vicuna and Alpaca. and WizardCoder 164 fine-tuned StarCoder beats Claude-Plus. Bard. and others. 3.2.3. Aligning with Human Preferences",
    "chunk_index": 153,
    "start_pos": 67157,
    "end_pos": 67655,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 534
    }
  },
  "246": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_154",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "and others. 3.2.3. Aligning with Human Preferences Incorporating human preferences into LLMs presents significant advantage in mitigating undesirable behaviors and ensuring accurate outputs. The initial work on alignment. such as InstructGPT 20 aligns GPT-3 using 3-step approach. instruction-tuning. reward modeling. and fine-tuning with reinforcement learning RL The supervised fine-tuned GPT-3 on demonstrations is queried to generate responses. which human labelers rank according to human values. and reward",
    "chunk_index": 154,
    "start_pos": 67656,
    "end_pos": 68128,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 512
    }
  },
  "247": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_155",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "elers rank according to human values. and reward model is trained on the ranked data. Lastly. the GPT-3 is trained with proximal policy optimization PPO using rewards on the generated data from the reward model. LLaMA 2-Chat 21 improves alignment by dividing reward modeling into help- fulness and safety rewards and using rejection sampling in addition to PPO. The initial four versions of LLaMA 2-Chat are fine-tuned with rejection sampling and then with PPO on 16",
    "chunk_index": 155,
    "start_pos": 68129,
    "end_pos": 68550,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 472,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 472,
      "optimized_content_length": 466
    }
  },
  "248": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_156",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ed with rejection sampling and then with PPO on 16 --- Page 17 --- top of rejection sampling. Aligning with Supported Evidence. This style of alignment allows the model to generate responses with proofs and facts. reduces hallucination. and assists humans more ffectively. which increases trust in the model output. Similar to the RLHF training style. reward model is trained to rank generated responses containing web citations in answers to questions. which is later used to train the model. as in",
    "chunk_index": 156,
    "start_pos": 68552,
    "end_pos": 69006,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 499
    }
  },
  "249": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_157",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ons. which is later used to train the model. as in GopherCite 165 WebGPT 166 and Sparrow 167 The ranking model in Sparrow 167 is divided into two branches. preference reward and rule reward. where human annotators adversarial probe the model to break rule. These two rewards together rank response to train with RL. Aligning Directly with SFT. The PPO in the RLHF pipeline is complex. memory-intensive. and unstable. requiring mul- tiple models. reward. value. policy. and reference models.",
    "chunk_index": 157,
    "start_pos": 69007,
    "end_pos": 69461,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 490
    }
  },
  "250": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_158",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "dels. reward. value. policy. and reference models. Avoiding this sophisticated alignment pipeline is possible by incorporating minimal changes in the supervised fine-tuning SFT pipeline as in 168. 169. 170 with better or compa- rable performance to PPO. Direct preference optimization DPO 168 trains model directly on the human-preferred responses to maximize the likelihood of preferred against unpreferred responses. with per-sample importance weight. Reward ranked fine-tuning RAFT 169 fine-tunes the model",
    "chunk_index": 158,
    "start_pos": 69462,
    "end_pos": 69933,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 509
    }
  },
  "251": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_159",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ranked fine-tuning RAFT 169 fine-tunes the model on ranked responses by the reward model. Preference ranking optimization PRO 171 and RRHF 170 penalize the model to rank responses with human preferences and supervised loss. On the other hand. chain-of-hindsight CoH 172 provides feedback to the model in language rather than reward. to learn good versus bad responses. Aligning with Synthetic Feedback. Aligning LLMs with human feedback is slow and costly. The literature suggests",
    "chunk_index": 159,
    "start_pos": 69934,
    "end_pos": 70377,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 480
    }
  },
  "252": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_160",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "back is slow and costly. The literature suggests semi-automated process to align LLMs by prompting LLMs to generate helpful. honest. and ethical responses to the queries. and fine-tuning using the newly created dataset. Constitutional AI 173 replaces human feedback in RLHF with AI. calling it RL from AI feedback RLAIF AlpacaFarm 174 designs prompts to imitate human feedback using LLMs APIs. Oppo- site to constitutional AI. AlpacaFarm injects noise in feedback to replicate human mistakes. Self-Align 98 prompts the",
    "chunk_index": 160,
    "start_pos": 70378,
    "end_pos": 70856,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 518
    }
  },
  "253": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_161",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "licate human mistakes. Self-Align 98 prompts the LLM with ICL examples. instructing the LLM about what the response should contain to be considered useful and ethical. The same LLM is later fine-tuned with the new dataset. Aligning with Prompts. LLMs can be steered with prompts to generate desirable responses without training 175. 176 The self-correction prompting in 176 concatenates instructions and CoT with questions. guiding the model to answer its instruction following strategy to ensure moral safety before",
    "chunk_index": 161,
    "start_pos": 70857,
    "end_pos": 71331,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 516
    }
  },
  "254": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_162",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "following strategy to ensure moral safety before the actual answer. This strategy is shown to reduce the harm in generated responses significantly. Red-Teaming Jailbreaking Adversarial Attacks. LLMs exhibit harmful behaviors. hallucinations. leaking personal in- formation. and other shortcomings through adversarial probing. The models are susceptible to generating harmful responses even though they are aligned for safety 177. 178 Red- teaming is common approach to address illicit outputs. where",
    "chunk_index": 162,
    "start_pos": 71332,
    "end_pos": 71789,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 499
    }
  },
  "255": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_163",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "common approach to address illicit outputs. where the LLMs are prompted to generate harmful outputs 178. 179 .The dataset collected through red-teaming is used to fine-tune models for safety. While red-teaming largely relies on human annotators. another work 180 red-team LLMs to find prompts that lead to harmful outputs for other LLMs. 3.2.4. Continue Pre-Training Although fine-tuning boosts model performance. it leads to catastrophic forgetting of previously learned information.",
    "chunk_index": 163,
    "start_pos": 71790,
    "end_pos": 72231,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 484
    }
  },
  "256": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_164",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "phic forgetting of previously learned information. Concatenating fine-tuning data with few randomly selected pre-training samples in every iteration avoids network forget- ting 181. 152 This is also ffective in adapting LLMs for cases where fine-tuning data is small and the original capac- ity is to be maintained. Prompt-based continued pre-training PCP 182 trains the model with text and instructions related to tasks and then finally instruction-tunes the model for down- stream tasks. 3.2.5. Sample fficiency",
    "chunk_index": 164,
    "start_pos": 72232,
    "end_pos": 72707,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 513
    }
  },
  "257": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_165",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "for down- stream tasks. 3.2.5. Sample fficiency While fine-tuning data is generally many-fold smaller than the pre-training data. it still has to be large enough for accept- able performance 16. 97. 18 and requires proportional com- puting resources. Studying the ffects on performance with less data. existing literature 183. 184 finds that models trained on less data can outperform models trained with more data. In 183 25 of the total downstream data is found enough for state-of-the-art performance. Selecting coreset-based 0.5",
    "chunk_index": 165,
    "start_pos": 72708,
    "end_pos": 73203,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 532
    }
  },
  "258": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_166",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "-the-art performance. Selecting coreset-based 0.5 of the total instruction-tuning data improves the model perfor- mance by in 184 as compared to the complete data tun- ing. Less is more for alignment LIMA 185 uses only 1000 carefully created demonstrations to fine-tune the model and has achieved comparable performance to GPT-4. 3.3. Increasing Context Window LLMs are trained with limited context windows due to ex- pensive attention and high memory requirements. model trained on limited sequence lengths fails to generalize to unseen",
    "chunk_index": 166,
    "start_pos": 73204,
    "end_pos": 73703,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 537
    }
  },
  "259": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_167",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ted sequence lengths fails to generalize to unseen lengths at inference time 186. 49 Alternatively. LLMs with ALiBi 65 positional encodings can perform zero-shot length extrapolation. However. ALiBi has less expressive power 66 and inferior performance on multiple benchmarks 46 and many LLMs use RoPE positional embedding that is unable to perform zero-shot extrapolation. larger context length has benefits such as better understanding of longer documents. more samples in in-context learning. execution of bigger rea-",
    "chunk_index": 167,
    "start_pos": 73704,
    "end_pos": 74187,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 520
    }
  },
  "260": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_168",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "in in-context learning. execution of bigger rea- soning processes. etc. Expanding context length during fine- tuning is slow. ine fficient. and computationally expensive 49 Therefore. researchers employ various context window extrap- olation techniques discussed below. Position Interpolation. Rather than extrapolating. 49 shows that interpolating position encodings within the pre-trained con- text window are more ffective. The work demonstrates that only 1000 steps of fine-tuning are enough to achieve better re-",
    "chunk_index": 168,
    "start_pos": 74188,
    "end_pos": 74663,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 517
    }
  },
  "261": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_169",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ps of fine-tuning are enough to achieve better re- sults on larger windows without reducing performance com- pared to the original context size. Gira ffe 46 uses power scal- ing in RoPE. and YaRN 47 proposed NTK-aware interpola- tion. 17",
    "chunk_index": 169,
    "start_pos": 74664,
    "end_pos": 74854,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 241,
      "word_count": 40,
      "optimized_for_embedding": true,
      "original_content_length": 241,
      "optimized_content_length": 237
    }
  },
  "262": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_170",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "YaRN 47 proposed NTK-aware interpola- tion. 17 --- Page 18 --- Efficient Attention Mechanism. Dense global attention is one of the major constraints in training larger context win- dow LLMs. Using fficient attention variants. such as lo- cal. sparse. and dilated attention. reduces the computation cost significantly. LongT5 48 proposes transient global atten- tion TGlobal applying attention to local and global tokens windowed token averaging The model replaces attention in T5 10 with TGlobal attention. pre-trains the model on",
    "chunk_index": 170,
    "start_pos": 74856,
    "end_pos": 75351,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 530
    }
  },
  "263": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_171",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "with TGlobal attention. pre-trains the model on 4098 sequence length. fine-tunes on larger window sizes. as large as 16k. and improves task performance on longer inputs. This shows the extrapolation ability of TGlobal attention with only fine-tuning. COLT5 187 uses two branches. one with lightweight and the other with heavyweight attention and feed- forward layers. All tokens are processed from the lightweight branch. and only important tokens are routed to the heavy- weight branch. LongNet 188 replaces standard attention with",
    "chunk_index": 171,
    "start_pos": 75352,
    "end_pos": 75840,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 532
    }
  },
  "264": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_172",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ch. LongNet 188 replaces standard attention with dilated attention. expanding sequence length to billion tokens. LongLoRA 189 proposes shift-short attention. used during fine-tuning to reduce dense attention costs. However. the model during inference uses dense attention and achieves similar per- formance as full attention fine-tuning. Extrapolation without Training. LM-Infinite 186 and par- allel context windows PCW 190 show length extrapolation is possible using pre-trained LLMs. LM-Infinite suggested \u039b-",
    "chunk_index": 172,
    "start_pos": 75841,
    "end_pos": 76313,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 511
    }
  },
  "265": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_173",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "using pre-trained LLMs. LM-Infinite suggested \u039b- shaped attention applied within the original context window limits. Likewise. PCW chunks larger inputs into the pre-trained context lengths and applies the same positional encodings to each chunk. 3.4. Augmented LLMs LLMs are capable of learning from the examples concate- nated with the input. known as context augmentation. in- context learning ICL or few-shot prompting. They show ex- cellent generalization to unseen tasks with few-shot prompt-",
    "chunk_index": 173,
    "start_pos": 76314,
    "end_pos": 76765,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 497
    }
  },
  "266": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_174",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "neralization to unseen tasks with few-shot prompt- ing. enabling LLMs to answer queries beyond the capacity ac- quired during training 6. 55 These emergent abilities allow for adapting the model without fine-tuning costly process. Aside from this. hallucination. producing inaccurate. unsafe. or factually incorrect responses. is common for LLMs. which is avoided by augmenting contextual data. While the user can pro- vide in-context samples in the query 54. 32 here we specifi-",
    "chunk_index": 174,
    "start_pos": 76766,
    "end_pos": 77202,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 487,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 487,
      "optimized_content_length": 479
    }
  },
  "267": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_175",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "xt samples in the query 54. 32 here we specifi- cally refer to the methods that access external storage program- matically. calling them augmented LLMs. The literature suggests various external memory designs to aug- ment LLMs. long-term 191. 192. 193. 194 short-term 195 symbolic 196 and non-symbolic 197. 198 The memory can be maintained in di fferent formats such as documents. vec- tors. or databases. few systems maintain intermediate mem- ory representations to retain information across multiple iter-",
    "chunk_index": 175,
    "start_pos": 77203,
    "end_pos": 77677,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 508
    }
  },
  "268": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_176",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ations to retain information across multiple iter- ations 194. 192 while others extract important information from the datasets and save it in memory for recall 199 The memory read and write operations are performed either with or without LLMs cooperation 192. 200. 194. 201 acting as feedback signal in 195 We discuss di fferent types of aug- mented LLMs below. Figure 12. flow diagram of Retrieval Augmented LLMs. The retriever ex- tracts similar context to the input and forwards it to the LLM either in simple",
    "chunk_index": 176,
    "start_pos": 77678,
    "end_pos": 78158,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 513
    }
  },
  "269": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_177",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "input and forwards it to the LLM either in simple language or encoded through Fusion-in-Decoder FiD Depending on the task. retrieval and generation may repeat multiple times. 3.4.1. Retrieval Augmented LLMs LLMs may have limited memory and outdated information. leading to inaccurate responses. Retrieving relevant informa- tion from external up-to-date storage enables the LLMs to accurately answer with references and utilize more informa- tion. With retrieval augmentation. smaller models have been",
    "chunk_index": 177,
    "start_pos": 78159,
    "end_pos": 78613,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 501
    }
  },
  "270": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_178",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "retrieval augmentation. smaller models have been shown to perform at par with larger models. For instance. the 11B model can become competitive to 540B PaLM in 25 and 7.5B to 280B Gopher in 193 Retrieval augmented language modeling RALM has two major components. shown in Figure 12. namely. retriever and language model. In RALM. the retriever plays crucial role in driving LLM response. where incorrect information can steer LLMs to false behavior. This leads to the development of various methods to",
    "chunk_index": 178,
    "start_pos": 78614,
    "end_pos": 79081,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 501
    }
  },
  "271": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_179",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "his leads to the development of various methods to retrieve accurate information and fuse with the query for better performance. Zero-Shot Retrieval Augmentation. This kind of augmen- tation keeps the original LLM architecture and weights unchanged and uses BM25 202 nearest neighbors. or frozen pre-trained models like Bert as retriever. The retrieved information is provided as input to the model for response generation. shown to improve performance over LLMs without retrieval 198. 203 In some scenarios. multiple retrieval",
    "chunk_index": 179,
    "start_pos": 79082,
    "end_pos": 79570,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 527
    }
  },
  "272": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_180",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "198. 203 In some scenarios. multiple retrieval iterations are required to complete the task. The output generated in the first iteration is forwarded to the retriever to fetch similar documents. Forward-looking active retrieval FLARE 197 initially generates the response and corrects the output by retrieving relevant documents if the response contains low-confidence tokens. Similarly. RepoCoder 204 fetches code snippets recursively for code completion. Training with Retrieval Augmentation. To reduce failures in",
    "chunk_index": 180,
    "start_pos": 79571,
    "end_pos": 80045,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 515
    }
  },
  "273": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_181",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "with Retrieval Augmentation. To reduce failures in retrieval augmentation generation RAG researchers train or fine-tune retrievers and LLMs with retrieval augmentation pipeline. We discuss the literature below based on their focus on the respective training processes of the pipeline. Training LLM. Retrieval-enhanced transformer RETRO 193 shows pre-training smaller LLMs with RAG pipeline outper- forms larger LLMs. such as GPT-3 trained without RAG. RETRO uses 2-trillion token subset of MassiveText as 18",
    "chunk_index": 181,
    "start_pos": 80046,
    "end_pos": 80513,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 507
    }
  },
  "274": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_182",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ses 2-trillion token subset of MassiveText as 18 --- Page 19 --- database. The retrieval pipeline divides the input query into subsets and retrieves relevant chunks from the database for each subset. encoded together with input intermediate representations for generating tokens. It uses cross-chunked attention to attend to previous chunks auto-regressively. study on RETRO 205 shows models pre-trained without RAG but fine-tuned using RAG lack the performance gains obtained by pre-training with RAG.",
    "chunk_index": 182,
    "start_pos": 80515,
    "end_pos": 80974,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 502
    }
  },
  "275": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_183",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rformance gains obtained by pre-training with RAG. Training Retriever. Quality of responses generated by LLMs is highly dependent on the in-context examples. There- fore. 206. 207. 208. 209 train retrievers to retrieve accurate few-shot samples while keeping the LLM frozen for gener- ation. Retrieved samples are ranked to build ground-truth data to train retrievers with contrastive learning in 206. 208 RoBERTa is trained for downstream tasks in 207 for ICL samples retrieval. REPLUG 209 trains the retriever with",
    "chunk_index": 183,
    "start_pos": 80975,
    "end_pos": 81449,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 516
    }
  },
  "276": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_184",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "retrieval. REPLUG 209 trains the retriever with supervised signals from the frozen LLM-generated outputs. Training Retriever and LLM. Further benefits are achieved by training both the retriever and the model in 25. 210. 211 In this case. the error propagates back to the retriever. updating both the language model and the retriever. While masked language modeling MLM is common pre-training objec- tive 25. 211 retrieval pre-trained transformer RPT 210 used document chunk prediction as pre-training objective for",
    "chunk_index": 184,
    "start_pos": 81450,
    "end_pos": 81933,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 515
    }
  },
  "277": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_185",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "chunk prediction as pre-training objective for long text modeling. Encoded Context Augmentation. Concatenating retrieved documents with the query becomes infeasible as the sequence length and sample size grow. Encoding the context and fusing it with the decoder Fusion-in-Decoder using cross-attention makes it possible to augment more samples without increasing computation costs significantly 212. 193. 210. 25 Web Augmented. Locally stored memory. but external to LLM. has limited information. However. large amount of",
    "chunk_index": 185,
    "start_pos": 81934,
    "end_pos": 82415,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 521
    }
  },
  "278": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_186",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "as limited information. However. large amount of information is available on the internet. which gets updated regularly. Rather than storing information locally. various methods retrieve query-related context through web search and forward it to LLMs 213. 214. 166 3.4.2. Tool Augmented LLMs While RAG relies on the retriever to provide context to the LLM to answer queries. tool augmented LLMs capitalize on the reasoning abilities of LLMs to iteratively plan by dividing tasks into sub-tasks. selecting necessary tools. and taking actions to",
    "chunk_index": 186,
    "start_pos": 82416,
    "end_pos": 82915,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 543
    }
  },
  "279": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_187",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "selecting necessary tools. and taking actions to complete the task 215. 216. 217. 27 generic pipeline of tool-augmented LLMs is shown in Figure 13. where di fferent modules in Figure 13 are selected in loop until the task com- pletion. Zero-Shot Tool Augmentation. LLMs in-context learning and reasoning abilities enable them to interact with tools with- out training. Automatic reasoning and tool-use ART 217 builds task library with demonstrations of reasoning steps and calling external tools. It retrieves similar task examples and",
    "chunk_index": 187,
    "start_pos": 82916,
    "end_pos": 83415,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 535
    }
  },
  "280": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_188",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rnal tools. It retrieves similar task examples and provides the context to the LLM for inference. Aside from this. 218 shows tool documentation is enough to teach LLMs to use tools without demonstrations. RestGPT 219 integrates LLMs with RESTful APIs by decomposing tasks into planning Figure 13. basic flow diagram of tool augmented LLMs. Given an input and set of available tools. the model generates plan to complete the task. The tool augmented LLMs utilize di fferent modules iteratively. such as retriever.",
    "chunk_index": 188,
    "start_pos": 83416,
    "end_pos": 83887,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 512
    }
  },
  "281": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_189",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "di fferent modules iteratively. such as retriever. tool execution. read-write to memory. feedback. etc. depending on the task. and API selection steps. The API selector understands the API documentation to select suitable API for the task and plan the execution. ToolkenGPT 220 uses tools as tokens by concate- nating tool embeddings with other token embeddings. During inference. the LLM generates the tool tokens representing the tool call. stops text generation. and restarts using the tool exe- cution output.",
    "chunk_index": 189,
    "start_pos": 83888,
    "end_pos": 84355,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 513
    }
  },
  "282": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_190",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "n. and restarts using the tool exe- cution output. Training with Tool Augmentation. LLMs are trained to inter- act with diverse tools. enhancing planning abilities to overcome the limitations of zero-shot tool augmentation 221. 27. 222. 223 Gorilla 221 instruction-tunes LLaMA with information retrieval from API documentation. It uses the self-instruct 19 data generation pipeline with GPT-4 by providing in-context examples retrieved from API documentation. Tool augmented language model TALM 27 fine-tunes T5 10 for tool use",
    "chunk_index": 190,
    "start_pos": 84356,
    "end_pos": 84845,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 527
    }
  },
  "283": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_191",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "model TALM 27 fine-tunes T5 10 for tool use with self-play approach. where it iteratively completes tool manipulation tasks and includes them back in the training set. ToolLLM 223 collects 16k APIs from RapidAPI. It samples APIs from the list to generate an instruction-tuning dataset us- ing ChatGPT in single-tool and multi-tool scenarios. For high- quality datasets. ToolLLM suggested depth-first search-based decision tree DFSDT method to generate ground-truths with diverse reasoning and planning.",
    "chunk_index": 191,
    "start_pos": 84846,
    "end_pos": 85312,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 502
    }
  },
  "284": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_192",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ground-truths with diverse reasoning and planning. Multimodal Tool Augmentation. The compositional reasoning capacity of LLMs allows them to manipulate tools in multi- modal settings 215. 216. 224 Following the pipeline shown in Figure 13. the LLM outlines plan. generally executing in sequence. Plan Tool selection Execute Inspect Generate. to respond to the user query. Here. the database of tools is rich in modalities. including text. images. etc. Many of the multimodal tool augmentation systems employ multimodal",
    "chunk_index": 192,
    "start_pos": 85313,
    "end_pos": 85788,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 518
    }
  },
  "285": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_193",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "imodal tool augmentation systems employ multimodal LLMs 31. 225. 224. 216 while others utilize single modality 19",
    "chunk_index": 193,
    "start_pos": 85789,
    "end_pos": 85854,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 116,
      "word_count": 17,
      "optimized_for_embedding": true,
      "original_content_length": 116,
      "optimized_content_length": 113
    }
  },
  "286": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_194",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "224. 216 while others utilize single modality 19 --- Page 20 --- LLMs and generate plan on using di fferent modality tools to solve multimodal queries 226 3.5. LLMs-Powered Agents AI agents are autonomous entities. capable of planning. decision-making. and performing actions to achieve complex goals. In the early days. AI agents were rule-based. de- signed for narrow tasks. and had limited capabilities. such as Clippy 227 and Deep Blue 228 In contrast to this. LLMs abilities to respond to dynamic scenarios have made it",
    "chunk_index": 194,
    "start_pos": 85856,
    "end_pos": 86341,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 524
    }
  },
  "287": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_195",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ities to respond to dynamic scenarios have made it possible to incorporate them in diverse applications. includ- ing LLMs-powered agents 224. 216 where LLMs behave as the brain of agents. LLMs have been incorporated in web agents 166. 167 coding agents 229 tool agents 27. 223 embodied agents 26 and conversational agents 195 requir- ing minimal to no fine-tuning Below we summarize the re- search in LLMs-based autonomous agents. For more detailed discussion. please refer to 230. 231",
    "chunk_index": 195,
    "start_pos": 86342,
    "end_pos": 86801,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 485
    }
  },
  "288": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_196",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "detailed discussion. please refer to 230. 231 LLMs Steering Autonomous Agents. LLMs are the cognitive controllers of the autonomous agents. They generate plans. rea- son about tasks. incorporate memory to complete tasks. and adapt the outline depending on the feedback from the environ- ment. Depending on the acquired capabilities of LLMs. many methods fine-tune. propose better prompting approach. or uti- lize di fferent modules to enhance agents performance. Mod- ules and strategies employed in autonomous agents are briefly",
    "chunk_index": 196,
    "start_pos": 86802,
    "end_pos": 87288,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 529
    }
  },
  "289": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_197",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rategies employed in autonomous agents are briefly discussed below. Planning and Reasoning. Completing complex task requires human-like logical thinking. planning necessary steps. and reasoning current and future directions. Prompting methods like chain-of-thoughts 103 tree-of-thoughts 105 and self- consistency 104 are central to agents. eliciting LLMs to rea- son its actions and choose among di fferent paths for task com- pletion. When LLMs are prompted with task description and",
    "chunk_index": 197,
    "start_pos": 87289,
    "end_pos": 87734,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 484
    }
  },
  "290": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_198",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "When LLMs are prompted with task description and sequence of actions. they can accurately generate plan ac- tions without any fine-tuning 232 Reasoning via planning RAP 233 incorporates re-purposed LLM as world model to reason about future outcomes and explore alternative paths for task completion. Retroformer 234 uses retrospective LLM to improve main LLM planning and reasoning capabil- ities by providing helpful task cues. Feedback. LLMs in open-loop systems generate plans and as-",
    "chunk_index": 198,
    "start_pos": 87735,
    "end_pos": 88190,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 487
    }
  },
  "291": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_199",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "LLMs in open-loop systems generate plans and as- sume that the agent will complete them successfully. However. the actual scenario is di fferent with failures and variable re- sponses from the environment. To correctly complete tasks. many methods use LLMs in closed-loop where the action re- sponse is provided as feedback to the LLMs to re-assess and update the plan as required 235. 236. 237. 195 Another di- rection of research exploits LLMs as reward functions to train reinforcement learning RL policies instead of humans 238",
    "chunk_index": 199,
    "start_pos": 88191,
    "end_pos": 88683,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 531
    }
  },
  "292": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_200",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nt learning RL policies instead of humans 238 Memory. LLMs can learn from the context provided in the prompt. In addition to internal memory. various systems em- ploy external memory to save the response history. Reflex- ion 195 maintains an episodic memory to use previous re- sponses as feedback to improve future decision-making. Retro- former 234 improves its responses by employing short-termand long-term memory. where short-term memory contains re- cent responses and long-term memory keeps summarized failed",
    "chunk_index": 200,
    "start_pos": 88684,
    "end_pos": 89157,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 515
    }
  },
  "293": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_201",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "onses and long-term memory keeps summarized failed attempts to add in the prompt as reflection. Multi-Agents Systems. LLMs can play user-defined roles and behave like specific domain expert. In multi-agent systems. each LLM is assigned unique role. simulating human behav- ior and collaborating with other agents to complete complex task 229. 239 LLMs in Physical Environment. LLMs are good at instruction-following. however. utilizing them for physically grounded tasks requires adaptation. as they lack real-world",
    "chunk_index": 201,
    "start_pos": 89158,
    "end_pos": 89631,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 515
    }
  },
  "294": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_202",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tasks requires adaptation. as they lack real-world knowledge. This could lead to generating illogical responses for particular physical situation 240. 26 SayCan 240 make LLMs aware of the available low-level task operations. LLM Say builds high-level plan to complete the task and learned ffordance function Can explores the possibility of executing the plan in the real world. SayCan uses RL to train the language-conditioned ffordance function. PaLM-E enables the LLM to solve grounded tasks by training multi-modal LLM",
    "chunk_index": 202,
    "start_pos": 89632,
    "end_pos": 90121,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 521
    }
  },
  "295": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_203",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "solve grounded tasks by training multi-modal LLM feeding inputs directly from the sensors. Manipulation. In the area of manipulation 236. 241 LLMs enhance robot dexterity and adaptability. excelling in tasks like object recognition. grasping. and collaboration. They ana- lyze visual and spatial information to determine the most ffec- tive approach to interact with objects. Navigation. LLMs enhance robot ability to navigate com- plex environments with precision and adaptability 242. 243.",
    "chunk_index": 203,
    "start_pos": 90122,
    "end_pos": 90578,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 491
    }
  },
  "296": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_204",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "onments with precision and adaptability 242. 243. 244. 245 They generate feasible paths and trajectories for robots. accounting for intricate environmental details 246 This ability is valuable in scenarios requiring precise and dynamically adaptable navigation in environments like ware- houses. transport. healthcare facilities. and residences. 3.6. fficient LLMs Deploying LLMs in production is expensive. Reducing their running costs while preserving performance is an appealing area of research. This section summarizes the approaches sug-",
    "chunk_index": 204,
    "start_pos": 90579,
    "end_pos": 91079,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 543
    }
  },
  "297": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_205",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "earch. This section summarizes the approaches sug- gested to enhance LLMs fficiency. 3.6.1. Parameter fficient Fine-Tuning Fine-tuning LLMs with tens or hundreds of billions of pa- rameters. such as GPT-3 175B BLOOM 176B MT-NLG 540B etc. is computationally intensive and time-consuming. To avoid complete model fine-tuning. numerous parameter- efficient fine-tuning PEFT techniques 40. 247. 41. 38. 39 try to achieve acceptable model fine-tuning performance at reduced costs. As compared to full fine-tuning 248 PEFT performs",
    "chunk_index": 205,
    "start_pos": 91080,
    "end_pos": 91576,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 525
    }
  },
  "298": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_206",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "compared to full fine-tuning 248 PEFT performs better in low-resource setups. achieves comparable perfor- mance on medium-resource scenarios. and performs worse than full fine-tuning under high-resource availability. An overview of different PEFT approaches is shown in Figure 14. Adapter Tuning. Adds few trainable parameters within the transformer block. The adapter layer is sequence of feature downscaling. non-linearity. and upscaling 106 Variants of adapter tuning inject adapter layers sequentially 106 and in",
    "chunk_index": 206,
    "start_pos": 91577,
    "end_pos": 92055,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 516
    }
  },
  "299": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_207",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng inject adapter layers sequentially 106 and in parallel 38 whereas the mixture of adapter AdaMix 249 20",
    "chunk_index": 207,
    "start_pos": 92056,
    "end_pos": 92119,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 114,
      "word_count": 18,
      "optimized_for_embedding": true,
      "original_content_length": 114,
      "optimized_content_length": 105
    }
  },
  "300": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_208",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "whereas the mixture of adapter AdaMix 249 20 --- Page 21 --- Figure 14. Illustration of parameter-e fficient fine-tuning paradigms. where xis input and his hidden state. figure courtesy 38 Parallel adapter and LoRA fall in the adapter tuning category. employs multiple adapter modules in single layer. AdaMix routes input instances randomly to one of the multiple down- scale and upscale modules. The mixture of adapters is averaged out for inference to avoid additional latency. Low-Rank Adap-",
    "chunk_index": 208,
    "start_pos": 92121,
    "end_pos": 92575,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 494
    }
  },
  "301": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_209",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "erence to avoid additional latency. Low-Rank Adap- tation LoRA 250 learns low-rank decomposed matrices to freeze original weights. The learned weights are fused with the original weights for inference. avoiding latency. Prompt Tuning. Prompting is an ffective way to adapt pre-trained LLM for the downstream task. However. manual prompts bring uncertainty in the model prediction. where change in single word drops the performance 247 Prompt tuning alleviates this problem by fine-tuning only 0.001 -3",
    "chunk_index": 209,
    "start_pos": 92576,
    "end_pos": 93044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 501
    }
  },
  "302": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_210",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "eviates this problem by fine-tuning only 0.001 -3 additional parameters 251 It concatenates trainable prompt parameters with the model embeddings 247. 40. 251 Task- specific fixed discrete prompts are concatenated with input em- beddings in 40 As discrete prompts bring instability. prompts are encoded through learnable mapping in P-Tuning 247 naming continuous prompts. which are appended with the dis- crete prompts. Only the prompt encoder is trainable in the model. In an extension of P-Tuning. continuous prompts are",
    "chunk_index": 210,
    "start_pos": 93045,
    "end_pos": 93531,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 522
    }
  },
  "303": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_211",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "an extension of P-Tuning. continuous prompts are concatenated with each layer of the network in 251 Progres- sive prompts 252 avoid catastrophic forgetting and transfer previously learned knowledge by sequentially adding trainable prompt embeddings to the previously frozen task embeddings. Prefix Tuning. set of trainable task-specific prefix vectors are appended to the frozen transformer layers in prefix tun- ing 41 The prefix vectors are virtual tokens attended by the context tokens on the right. In addition. adaptive prefix tun-",
    "chunk_index": 211,
    "start_pos": 93532,
    "end_pos": 94029,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 536
    }
  },
  "304": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_212",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ns on the right. In addition. adaptive prefix tun- ing 253 applies gating mechanism to control the information from the prefix and actual tokens. Bias Tuning. Fine-tuning only bias terms in small to medium training data has been found ffective in BitFit 254 This method achieves full fine-tuning performance for tasks with less training data and comparable performance with more training data. 3.6.2. Quantization LLMs require extensive computing and memory for infer- ence. Deploying 175B parameter GPT-3 model needs at",
    "chunk_index": 212,
    "start_pos": 94030,
    "end_pos": 94510,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 520
    }
  },
  "305": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_213",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "e. Deploying 175B parameter GPT-3 model needs at least five 80GB A100 GPUs and 350GB of memory to store inFP16 format 44 Such demanding requirements for deploying LLMs make it harder for smaller organizations to utilize them. Model compression is an ffective solution but comes at the cost of degraded performance. especially at large scales greater than 6B. These models exhibit very large magnitude outliers that do not exist in smaller models 255 making it challenging and re- quiring specialized methods for quantizing LLMs 44. 256",
    "chunk_index": 213,
    "start_pos": 94511,
    "end_pos": 95008,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 535
    }
  },
  "306": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_214",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "specialized methods for quantizing LLMs 44. 256 Post-Training Quantization. Minimal or no training is re- quired in this type of quantization. without significantly com- promising the model performance. LLM-8-bit 255 uses full- precision matrix multiplication for weights associated with out- lier features and 8-bit for remaining features. The lower pre- cision multiplication outputs are converted to FP-16 and con- catenated with others. The quantized models have homogenous word embeddings. which may degrade their performance. To",
    "chunk_index": 214,
    "start_pos": 95009,
    "end_pos": 95497,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 534
    }
  },
  "307": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_215",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "mbeddings. which may degrade their performance. To fix this. token-level knowledge distillation is employed in 45 along with independent quantization scaling factors for each module due to varying weight distribution. Feature distribu- tions are asymmetric and appear in di fferent channels. outlier suppression 257 shifts and scales per-channel activation dis- tributions for ffective quantization. SmoothQuant 44 quan- tizes activations and weights to INT8 format by smoothing activations and migrating the quantization di fficulty toward",
    "chunk_index": 215,
    "start_pos": 95498,
    "end_pos": 95995,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 540
    }
  },
  "308": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_216",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "and migrating the quantization di fficulty toward weights. It multiplies the inverse of the smoothing factor with weights. which introduces few outliers in the weights but is easier to quantify than unsmoothed activations. OPTQ 256 uses the optimal brain compression OBC 258 algorithm to quantize the model layer-by-layer and update weights to com- pensate for quantization error. To improve speed and per- formance. OPTQ updates weights in arbitrary order. employs lazy updates. and uses better Cholesky kernels. Outlier-aware",
    "chunk_index": 216,
    "start_pos": 95996,
    "end_pos": 96481,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 527
    }
  },
  "309": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_217",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "s. and uses better Cholesky kernels. Outlier-aware weight quantization OWQ 259 uses the OPTQ algorithm for quantization but assigns higher precision to vulnerable weights. causing outliers and lower precision for others. Quantization-Aware Training. To compensate for perfor- mance degradation. quantized model is fine-tuned in quantization-aware training QAT 260. 261. 262 Al- pha Tuning quantizes the model using binary coding quan- tization BCQ 263 and fine-tunes only quantization scal-",
    "chunk_index": 217,
    "start_pos": 96482,
    "end_pos": 96936,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 490
    }
  },
  "310": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_218",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "BCQ 263 and fine-tunes only quantization scal- ing factors. This approach improves performance over 21",
    "chunk_index": 218,
    "start_pos": 96937,
    "end_pos": 96992,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 106,
      "word_count": 15,
      "optimized_for_embedding": true,
      "original_content_length": 106,
      "optimized_content_length": 102
    }
  },
  "311": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_219",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "actors. This approach improves performance over 21 --- Page 22 --- parameter-e fficient fine-tuning of the pre-trained model. Sim- ilarly. parameter-e fficient and quantization-aware adaptation PEQA 264 reduces the precision of fully-connected layers and fine-tunes only quantization scaling parameters. LLM- QAT 262 generates training data from the pre-trained network and trains quantized student model with knowledge distilla- tion. QLoRA 261 fine-tunes 4-bit quantized pre-trained LLM",
    "chunk_index": 219,
    "start_pos": 96994,
    "end_pos": 97441,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 488
    }
  },
  "312": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_220",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "261 fine-tunes 4-bit quantized pre-trained LLM with LoRA 250 using 4-bit normal float. which shows better performance over 4-bit integer and float. 3.6.3. Pruning Pruning is an alternative approach to quantization to com- press model size. thereby reducing LLMs deployment costs significantly. Compared to task-agnostic pruning. task-specific pruning is easily achievable with good performance. where model is fine-tuned on the downstream task and pruned for faster inference. It is possible to prune LLMs for individual",
    "chunk_index": 220,
    "start_pos": 97442,
    "end_pos": 97923,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 520
    }
  },
  "313": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_221",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rence. It is possible to prune LLMs for individual tasks. but the cost of pruning and deploying task-specific mod- els is high. To overcome this. many structured and unstructured pruning methods for LLMs have been proposed to maintain rea- sonable performance across all tasks while shrinking the model size 265. 42. 266 Unstructured Pruning. This kind of pruning removes less im- portant weights without maintaining any structure. Existing LLM pruning methods take advantage of the unique charac-",
    "chunk_index": 221,
    "start_pos": 97924,
    "end_pos": 98373,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 497
    }
  },
  "314": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_222",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "uning methods take advantage of the unique charac- teristics of LLMs. uncommon for smaller models. where small subset of hidden states are activated with large magni- tude 255 Pruning by weights and activations Wanda 265 prunes weights in every row based on importance. calculated by multiplying the weights with the norm of input. The pruned model does not require fine-tuning. thereby saving computa- tional costs. Outlier weighed layerwise sparsity OWL 267 extends Wanda with non-uniform layer pruning. It shows that",
    "chunk_index": 222,
    "start_pos": 98374,
    "end_pos": 98855,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 519
    }
  },
  "315": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_223",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "anda with non-uniform layer pruning. It shows that the number of outliers varies for di fferent layers. therefore. the model should have variable pruning ratios for better perfor- mance for every layer. Contrastive pruning CAP 43 itera- tively prunes the model by training the sparse model using con- trastive loss between pre-trained. fine-tuned. and snapshots of previous sparse models to learn task-specific and task-agnostic knowledge. Structured Pruning. Here. the parameters are removed in",
    "chunk_index": 223,
    "start_pos": 98856,
    "end_pos": 99304,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 495
    }
  },
  "316": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_224",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tured Pruning. Here. the parameters are removed in groups. rows. columns. or matrices. which speeds up the inference because of ffective hardware tensor core utiliza- tion 265 LLM-Pruner 42 employs 3-stage structured pruning strategy. identifying the groups of hidden states caus- ing each other to activate during the forward-pass. keeping im- portant groups and removing less important ones. and fine- tuning the pruned model with LoRA. Sparsity-induced mask learning SIMPLE 268 prunes the network using learnable",
    "chunk_index": 224,
    "start_pos": 99305,
    "end_pos": 99782,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 515
    }
  },
  "317": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_225",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "SIMPLE 268 prunes the network using learnable masks. Similarly. another method prunes LLMs by learning masks and removing unimportant rank-1 components of the factorized weight matrix 266 3.7. Multimodal LLMs Inspired by the success of LLMs in natural language process- ing applications. an increasing number of research works arenow facilitating LLMs to perceive di fferent modalities of infor- mation like image 269. 270. 271 video 272. 273. 274 au- dio 275. 274. 276 etc. Multimodal LLMs MLLMs present",
    "chunk_index": 225,
    "start_pos": 99783,
    "end_pos": 100255,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 504
    }
  },
  "318": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_226",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "5. 274. 276 etc. Multimodal LLMs MLLMs present substantial benefits compared to standard LLMs that process only text. By incorporating information from various modal- ities. MLLMs can achieve deeper understanding of context. leading to more intelligent responses infused with variety of expressions. Importantly. MLLMs align closely with human perceptual experiences. leveraging the synergistic nature of our multisensory inputs to form comprehensive understanding of the world 276. 26 Coupled with user-friendly interface.",
    "chunk_index": 226,
    "start_pos": 100256,
    "end_pos": 100743,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 523
    }
  },
  "319": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_227",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "276. 26 Coupled with user-friendly interface. MLLMs can ffer intuitive. flexible. and adaptable interactions. allowing users to engage with intelligent assistants through spectrum of input methods. According to the ways of construct- ing models. current MLLMs can be generally divided into three streams. pre-training. fine-tuning. and prompting. In this sec- tion. we will discuss more details of these main streams. as well as the important application of MLLMs in visual reasoning.",
    "chunk_index": 227,
    "start_pos": 100744,
    "end_pos": 101186,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 484
    }
  },
  "320": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_228",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "mportant application of MLLMs in visual reasoning. Pre-training. This stream of MLLMs intends to support di ffer- ent modalities using unified end-to-end models. For instance. Flamingo 269 applies gated cross-attention to fuse vision and language modalities. which are collected from pre-trained and frozen visual encoder and LLM. respectively. Moreover. BLIP- 270 proposes two-stage strategy to pre-train Querying Transformer Q-Former for the alignment between vision and language modalities. in the first stage. vision-language repre-",
    "chunk_index": 228,
    "start_pos": 101187,
    "end_pos": 101684,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 536
    }
  },
  "321": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_229",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "lities. in the first stage. vision-language repre- sentation learning is bootstrapped from frozen visual encoder. and in the second stage. frozen LLM bootstraps vision-to- language generative learning for zero-shot image-to-text gen- eration. Similarly. MiniGPT-4 277 deploys pre-trained and frozen ViT 278 Q-Former and Vicuna LLM 159 only train- ing the linear projection layer for vision and language modali- ties alignment. Fine-tuning. Derived from instruction tuning 16 for NLP",
    "chunk_index": 229,
    "start_pos": 101685,
    "end_pos": 102130,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 482
    }
  },
  "322": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_230",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ning. Derived from instruction tuning 16 for NLP tasks 20. 16. 97 researchers are fine-tune pre-trained LLMs using multimodal instructions. Following this method. LLMs can be easily and ffectively extended as multimodal chat- bots 277. 271. 29 and multimodal task solvers 279. 30. 280 The key issue of this stream of MLLMs is to collect multi- modal instruction-following data for fine-tuning 58 To ad- dress this issue. the solutions of benchmark adaptation 279. 281. 282 self-instruction 19. 31. 283 and hybrid composi-",
    "chunk_index": 230,
    "start_pos": 102131,
    "end_pos": 102622,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 521
    }
  },
  "323": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_231",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "elf-instruction 19. 31. 283 and hybrid composi- tion 284. 280 are employed. respectively. To mitigate the gap between the original language modality and additional modal- ities. the learnable interface is introduced to connect di ffer- ent modalities from frozen pre-trained models. Particularly. the learnable interface is expected to work in parameter- efficient tuning manner. e.g. LLaMA-Adapter 285 applies an efficient transformer-based adapter module for training. and LaVIN 284 dynamically learns the multimodal feature",
    "chunk_index": 231,
    "start_pos": 102623,
    "end_pos": 103110,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 526
    }
  },
  "324": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_232",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "IN 284 dynamically learns the multimodal feature weights using mixture-of-modality adapter. Di fferent from the learnable interface. the expert models can directly convert multimodalities into language. e.g. VideoChat-Text 272 in- corporates Whisper 286 speech recognition expert model. to generate the captions of given videos for the understanding of following LLMs. Prompting. Different from the fine-tuning technique that 22",
    "chunk_index": 232,
    "start_pos": 103111,
    "end_pos": 103500,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 440,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 440,
      "optimized_content_length": 428
    }
  },
  "325": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_233",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Different from the fine-tuning technique that 22 --- Page 23 --- directly updates the model parameters given task-specific datasets. the prompting technique provides certain context. ex- amples. or instructions to the model. fulfilling specialized tasks without changing the model parameters. Since prompting can significantly reduce the need for large-scale multimodal data. this technique is widely used to construct MLLMs. Particularly. to solve multimodal Chain of Thought CoT problems 103",
    "chunk_index": 233,
    "start_pos": 103502,
    "end_pos": 103951,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 493
    }
  },
  "326": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_234",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "multimodal Chain of Thought CoT problems 103 LLMs are prompted to generate both the reasoning process and the answer given multimodal inputs 287 On this front. di ffer- ent learning paradigms are exploited in practice. for example. Multimodal-CoT 287 involves two stages of rationale genera- tion and answer inference. where the input of the second stage is combination of the original input and the output of the first stage. and CoT-PT 288 applies both prompt tuning and spe-",
    "chunk_index": 234,
    "start_pos": 103952,
    "end_pos": 104393,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 477
    }
  },
  "327": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_235",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "CoT-PT 288 applies both prompt tuning and spe- cific visual bias to generate chain of reasoning implicitly. In addition to CoT problems. LLMs can also be prompted with multimodal descriptions and tools. ffectively dividing complex tasks into sub-tasks 289. 290 Visual Reasoning Application. Recent visual reasoning sys- tems 291. 292. 216. 293 tend to apply LLMs for better visual information analysis and visual-language integration. Di ffer- ent from previous works 294. 295 that rely on limited VQA",
    "chunk_index": 235,
    "start_pos": 104394,
    "end_pos": 104859,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 501
    }
  },
  "328": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_236",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "previous works 294. 295 that rely on limited VQA datasets and small-scale neural networks. current LLM-aided methods ffer benefits of stronger generalization ability. emer- gent ability. and interactivity 58 To realize visual reasoning with the help of LLMs. prompting and fine-tuning techniques can also be utilized. for example. PointClip V2 292 applies LLMs to generate 3D-specific prompts. which are encoded as textual features and then combined with visual features for 3D recognition. and GPT4Tools 31 employs LoRA 250 to",
    "chunk_index": 236,
    "start_pos": 104860,
    "end_pos": 105349,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 527
    }
  },
  "329": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_237",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ognition. and GPT4Tools 31 employs LoRA 250 to fine-tune LLMs following tool-related instructions. Serving as controller 293 decision maker 296 or semantics re- finer 291. 297 LLMs significantly facilitates the progress of visual reasoning research. 3.8. Summary and Discussion 3.8.1. Architecture Due to the gigantic scale of LLMs. minor changes in archi- tecture and training strategies have big impact on performance and stability. Here. we summarize key architectural modules",
    "chunk_index": 237,
    "start_pos": 105350,
    "end_pos": 105795,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 479
    }
  },
  "330": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_238",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "lity. Here. we summarize key architectural modules used in various LLMs. leading to better performance. reduced training time and memory. and better training stability. Layer Normalization. The performance and training stability of LLMs are ffected significantly by layer normalization. Pre- norm. that is normalizing inputs rather than outputs. is more common among LLMs stabilizing the training 6. 127. 108 BLOOM 13 and AlexaTM 122 utilize an additional layer normalization before embedding layer to stabilize the training",
    "chunk_index": 238,
    "start_pos": 105796,
    "end_pos": 106278,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 524
    }
  },
  "331": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_239",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "before embedding layer to stabilize the training of large-scale models. while the model zero-shot generaliza- tion ability can be negatively impacted 13 However. another study 33 finds that pre-norm degrades fine-tuned model per- formance as compared to post-norm. and there are no stability benefits of pre-norm beyond the 100B scale. Therefore. GLM- 130B 33 used deep-norm which is variant of post-norm for better downstream task performance after fine-tuning.",
    "chunk_index": 239,
    "start_pos": 106279,
    "end_pos": 106703,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 475,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 475,
      "optimized_content_length": 462
    }
  },
  "332": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_240",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ter downstream task performance after fine-tuning. Positional Encoding. Like other building blocks of the model.positional encoding also ffects the performance and training stability of LLMs. BLOOM 13 finds ALiBi outperforms learned and rotary positional encodings. Contrary to this. GLM-130B 33 identifies rotary positional encoding as being better than ALiBi. So. there is no conclusion in the literature about positional encodings yet. Parallel Attention. In this type of attention. feed-forward and",
    "chunk_index": 240,
    "start_pos": 106704,
    "end_pos": 107161,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 502
    }
  },
  "333": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_241",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ntion. In this type of attention. feed-forward and attention layers are parallel to each other rather than sequen- tial in transformer block. It has been shown to reduce train- ing time by 15 There is no evidence of performance drop due to this change in the literature and it is used by the models PaLM 15 GPT-NeoX 118 and CodeGen 140 Multi-Query Attention It has shared key and value attention heads in transformer block while query attention heads are projected as usual. This reduces memory usage and speeds up",
    "chunk_index": 241,
    "start_pos": 107162,
    "end_pos": 107640,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 514
    }
  },
  "334": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_242",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "as usual. This reduces memory usage and speeds up sampling in autoregressive decoding. No performance degrada- tion has been observed with this change and it makes the train- ing efficient allowing larger batch sizes. Multi-query attention is used in 15. 142 Mixture of Experts. This type of architecture enables eas- ily scaling models to trillions of parameters 92. 91 Only few experts are activated during the computation making them compute-e fficient. The performance of MoE models is better",
    "chunk_index": 242,
    "start_pos": 107641,
    "end_pos": 108095,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 496
    }
  },
  "335": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_243",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "fficient. The performance of MoE models is better than dense models for the same amount of data and requires less computation during fine-tuning to achieve performance similar to dense models as discussed in 91 MoE architectures are less prone to catastrophic forgetting. therefore are more suited for continual learning 92 Extracting smaller sub-models for downstream tasks is possible without losing any performance. making MoE architecture hardware-friendly 92 Sparse vs Dense Activated. GPT-3 uses sparse transform-",
    "chunk_index": 243,
    "start_pos": 108096,
    "end_pos": 108578,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 519
    }
  },
  "336": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_244",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Dense Activated. GPT-3 uses sparse transform- ers 67 whereas GLaM 91 and PanGu-P 92 use MoE 121 architectures to lower computational costs and increase the model size and capacity. According to the literature. sparse modules do not degrade the model performance 67 How- ever. more experiments are required to verify this statement. 3.8.2. Training Strategies Training models at huge scale require tricks to reduce train- ing costs. avoid loss divergence. and achieve better perfor-",
    "chunk_index": 244,
    "start_pos": 108579,
    "end_pos": 109028,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 481
    }
  },
  "337": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_245",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "avoid loss divergence. and achieve better perfor- mance. We summarize and discuss some of these key tricks used in di fferent LLMs. Mixed Precision. It is famous method for LLMs to reduce memory usage and improve training fficiency. In mixed pre- cision. forward and backward passes are performed in FP16 format whereas optimizer states and master weights are kept in FP32 format 120 drawback associated with this for- mat change is training instability due to smaller value range",
    "chunk_index": 245,
    "start_pos": 109029,
    "end_pos": 109470,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 480
    }
  },
  "338": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_246",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "training instability due to smaller value range resulting in loss spikes 33 An alternative to FP16 is BF16 which has comparatively larger range and performs precision- sensitive operations like gradient accumulation and softmax in FP32 13 BF16 has better performance and training stability but uses more memory and is supported on specific hardware. for example. A100 GPUs. Therefore. its adoption in LLMs is limited. Training Instability. Loss divergence or spiking is common",
    "chunk_index": 246,
    "start_pos": 109471,
    "end_pos": 109909,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 489,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 488,
      "optimized_content_length": 476
    }
  },
  "339": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_247",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nstability. Loss divergence or spiking is common issue in LLMs that occurs multiple times during training. This 23",
    "chunk_index": 247,
    "start_pos": 109910,
    "end_pos": 109975,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 116,
      "word_count": 19,
      "optimized_for_embedding": true,
      "original_content_length": 116,
      "optimized_content_length": 114
    }
  },
  "340": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_248",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "hat occurs multiple times during training. This 23 --- Page 24 --- happens in the presence of gradient clipping 15 To mitigate this problem. many approaches suggest restarting training from an earlier checkpoint 15. 33. 91 skipping 200-500 earlier data batches at the point of divergence in 15 and re-shu ffling batches in 91 The embedding layer gradient shrink proves to further stabilize the training as its gradient norm is significantly larger than the other layers 33 Another suggestion to improve",
    "chunk_index": 248,
    "start_pos": 109977,
    "end_pos": 110442,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 502
    }
  },
  "341": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_249",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "other layers 33 Another suggestion to improve training stability for larger models is not to use biases in dense and norm layers as in 15 Weight Initialization. It plays significant role in model con- vergence and training stability. GPT-NeoX 118 initializes feed-forward layers before residuals with2 das in 153 and other layers with the small initialization scheme 298 This avoids activations growing exponentially with increasing depth. MT-NLG 117 found higher variance for weight initialization",
    "chunk_index": 249,
    "start_pos": 110443,
    "end_pos": 110912,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 498
    }
  },
  "342": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_250",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "found higher variance for weight initialization leads to unstable training. hence validating small initialization scheme 298 Various models perform random weight initial- ization which can cause bad initialization. Galactica 148 sug- gests longer warmup to negate the ffect. Learning Rate. suitable learning rate is important for sta- ble training. It is suggested to use lower value 13. 15. 124 with warmup and decay cosine or linear Usually. the learn- ing rate is within the range 4to 8e 4. Moreover. MT-NLG",
    "chunk_index": 250,
    "start_pos": 110913,
    "end_pos": 111397,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 510
    }
  },
  "343": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_251",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "is within the range 4to 8e 4. Moreover. MT-NLG 530B 117 and GPT-NeoX 20B 118 suggest interpolat- ing learning rates based on the model size using the GPT-3 models ranging between 13B and 175B. This avoids tuning the learning rate hyperparameter. Training Parallelism. 3D parallelism. combination of data. pipeline. and tensor parallelism. is the most utilized training parallelism approach in LLMs 33. 15. 14. 13. 117. 115. 112 In addition to 3D parallelism. BLOOM 13 uses zero op-",
    "chunk_index": 251,
    "start_pos": 111398,
    "end_pos": 111853,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 481
    }
  },
  "344": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_252",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tion to 3D parallelism. BLOOM 13 uses zero op- timizer 37 to shard optimizer states. PanGu- 108 and PanGu- 92 go beyond 3D parallelism and apply 5D paral- lelism which additionally contains optimizer parallelism and rematerialization. Mode Switching. It adds task-related tokens at the beginning of the text during training. These tokens refer to the natural language understanding and natural language generation tasks which are shown to improve downstream task performance in 125. 124. 122 During fine-tuning and inference. tokens",
    "chunk_index": 252,
    "start_pos": 111854,
    "end_pos": 112350,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 532
    }
  },
  "345": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_253",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "24. 122 During fine-tuning and inference. tokens are appended based on the downstream tasks. Controllable Text Generation. Generating credible and con- trolled text from pre-trained model is challenging. GPT-3 and other LLMs use in-context learning to control generated text. While in-context learning helps in controlling the gener- ated text. ERNIE 3.0 Titan 35 suggests using adversarial loss to rank its generated text for credibility and soft prompts such as genre. topic. keywords. sentiment. and length for better control",
    "chunk_index": 253,
    "start_pos": 112351,
    "end_pos": 112838,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 528
    }
  },
  "346": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_254",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "keywords. sentiment. and length for better control on generated text. 3.8.3. Supervised Models vs Generalized Models Although generalized models are capable of performing di- verse tasks with good performance they have not yet outper- formed models trained in supervised settings. The supervised trained models are still state-of-the-art in various NLP tasks by large margin as shown in 6. 15. 18 .3.8.4. Zero-Shot vs Few-Shot LLMs perform well in zero-shot and few-shot settings. But the performance di fference between zero-shot and few-shot is",
    "chunk_index": 254,
    "start_pos": 112839,
    "end_pos": 113337,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 546
    }
  },
  "347": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_255",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ance di fference between zero-shot and few-shot is large for pre-trained models 6. 15 naming LLMs as meta- learners LLMs zero-shot evaluations underperform unsu- pervised methods in neural machine translation The liter- ature shows pre-training is not enough for good zero-shot per- formance 15. 16 To improve the zero-shot performance the literature suggests using instruction fine-tuning that improves the zero-shot performance significantly and outperforms base- lines. Instruction fine-tuning has also been shown to improve",
    "chunk_index": 255,
    "start_pos": 113338,
    "end_pos": 113830,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 527
    }
  },
  "348": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_256",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ruction fine-tuning has also been shown to improve zero-shot generalization to unseen tasks. Another model. Flan- PaLM 16 unlocks zero-shot reasoning with CoT training. 3.8.5. Encoder vs Decoder vs Encoder-Decoder Traditionally. these architectures perform well for di fferent tasks. for example. encoder-only for NLU tasks. decoder-only for NLG. and encoder-decoder for sequence2sequence model- ing. Encoder-only models are famous for smaller models such as Bert RoBERTa 299 etc. whereas LLMs are either",
    "chunk_index": 256,
    "start_pos": 113831,
    "end_pos": 114296,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 504
    }
  },
  "349": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_257",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "RoBERTa 299 etc. whereas LLMs are either decoder-only 6. 118. 13 or encoder-decoder 10. 11. 122 While decoder-only models are good at NLG tasks. various LLMs. PaLM 15 OPT 14 GPT-3 BLOOM 13 LLaMA 156 are decoder-only models with significant per- formance gains on both NLU and NLG tasks. In contradic- tion to this. T5 10 and UL2 125 identify encoder-decoder models out-performing decoder-only models. In another study. PaLM 15 finds increasing the size of decoder-only models",
    "chunk_index": 257,
    "start_pos": 114297,
    "end_pos": 114759,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 475
    }
  },
  "350": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_258",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "finds increasing the size of decoder-only models can reduce the performance gap between decoder-only and encoder-decoder architectures. Although decoder-only architectures have become trend for LLMs. many recently proposed approaches 125. 122 use mode-switching tokens in text with encoder-decoder architec- tures to enable task-specific modes. Similarly. CodeT5 34 uses an encoder-decoder architecture with multiple training ob- jectives for di fferent tasks. activating the encoder. decoder. or",
    "chunk_index": 258,
    "start_pos": 114760,
    "end_pos": 115214,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 496
    }
  },
  "351": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_259",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "fferent tasks. activating the encoder. decoder. or both according to the tasks. These variations in architecture and training objectives allow model to perform well in di ffer- ent settings. Because of this dynamic configuration. the future of LLMs can be attributed to encoder-decoder architectures. 4. Model Configurations We provide di fferent statistics of pre-trained and instruction- tuned models in this section. This includes information such as publication venue. license type. model creators. steps trained.",
    "chunk_index": 259,
    "start_pos": 115215,
    "end_pos": 115683,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 517
    }
  },
  "352": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_260",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "enue. license type. model creators. steps trained. parallelism. etc in Table and Table 4. Architecture details of pre-trained LLMs are available in Table 5. Providing these details for instruction-tuned models is unnecessary because it fine-tunes pre-trained models for instruction datasets. Hence. architectural details are the same as the baselines. Moreover. optimization settings for various LLMs are available in Table and Table 7. We do not include details on precision. warmup.",
    "chunk_index": 260,
    "start_pos": 115684,
    "end_pos": 116121,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 488,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 488,
      "optimized_content_length": 484
    }
  },
  "353": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_261",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "7. We do not include details on precision. warmup. and weight decay in Table 7. These details are not as important as others to mention for instruction-tuned models. and are not provided by the papers. 24",
    "chunk_index": 261,
    "start_pos": 116122,
    "end_pos": 116275,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 204,
      "word_count": 36,
      "optimized_for_embedding": true,
      "original_content_length": 204,
      "optimized_content_length": 204
    }
  },
  "354": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_262",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ned models. and are not provided by the papers. 24 --- Page 25 --- Table 3. Summary of pre-trained LLMs 10B Only the LLMs discussed individually in the previous sections are summarized. Data Tokens is the model pre-training data. which is either the number of tokens or data size. Data Cleaning indicates whether data cleaning is performed or not. This includes heuristics Heur deduplication Dedup quality filtering QF and privacy filtering PF Cost is the calculated training cost obtained by multiplying the GPUs TPUs",
    "chunk_index": 262,
    "start_pos": 116277,
    "end_pos": 116771,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 518
    }
  },
  "355": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_263",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "aining cost obtained by multiplying the GPUs TPUs hourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting discounted rate. re-training. number of employees working on the problem. etc. Training Parallelism indicates distributed training using data parallelism tensor parallelism pipeline parallelism context parallelism model parallelism optimizer parallelism OP and rematerialization where for Library column.",
    "chunk_index": 263,
    "start_pos": 116772,
    "end_pos": 117258,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 497
    }
  },
  "356": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_264",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rematerialization where for Library column. DS is short form for Deep Speed. In column Commercial Use we assumed model is for non-commercial purposes if its license is unavailable. ModelsPublication VenueLicense TypeModel Creators PurposeNo. of ParamsCommercial UseSteps TrainedData TokensData CleaningNo. of Processing UnitsProcessing Unit TypeTraining TimeCalculated Train. CostTraining Parallelism Library T5 10 JMLR 20 Apache-2.0 Google General 11B 1M 1T Heur Dedup 1024 TPU v3 Mesh TensorFlow",
    "chunk_index": 264,
    "start_pos": 117259,
    "end_pos": 117735,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 497
    }
  },
  "357": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_265",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "1T Heur Dedup 1024 TPU v3 Mesh TensorFlow GPT-3 NeurIPS 20 OpenAI General 175B 300B Dedup QF V100 mT5 11 NAACL 21 Apache-2.0 Google General 13B 1M 1T PanGu-\u03b1 108 arXiv 21 Apache-2.0 Huawei General 200B 260k 1.1TB Heur Dedup 2048 Ascend 910 OP MindSpore CPM-2 12 AI Open 21 MIT Tsinghua General 198B 1M 2.6TB Dedup JAXFormer Codex 141 arXiv 21 OpenAI Coding 12B 100B Heur",
    "chunk_index": 265,
    "start_pos": 117736,
    "end_pos": 118152,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 467,
      "word_count": 95,
      "optimized_for_embedding": true,
      "original_content_length": 466,
      "optimized_content_length": 370
    }
  },
  "358": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_266",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "iv 21 OpenAI Coding 12B 100B Heur ERNIE 3.0 110 arXiv 21 Baidu General 10B 120k 375B Heur Dedup 384 V100 PaddlePaddle Jurassic-1 112 White-Paper 21 Apache-2.0 AI21 General 178B 300B 800 GPU Megatron DS HyperCLOV 114 EMNLP 21 Naver General 82B 300B Clf Dedup PF 1024 A100 321h 1.32 Mil Megatron Yuan 1.0 115 arXiv 21 Apache-2.0 General 245B 26k 180B Heur Clf Dedup 2128 GPU Gopher 116 arXiv 21 Google General 280B 300B QF Dedup 4096 TPU v3 920h 13.19 Mil JAX Haiku",
    "chunk_index": 266,
    "start_pos": 118153,
    "end_pos": 118653,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 101,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 463
    }
  },
  "359": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_267",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "QF Dedup 4096 TPU v3 920h 13.19 Mil JAX Haiku ERNIE 3.0 Titan 35 arXiv 21 Baidu General 260B 300B Heur Dedup Ascend 910 PaddlePaddle GPT-NeoX-20B 118 BigScience 22 Apache-2.0 EleutherAI General 20B 150k 825GB None 96 40G A100 Megatron DS PyTorch OPT 14 arXiv 22 MIT Meta General 175B 150k 180B Dedup 992 80G A100 Megatron BLOOM 13 arXiv 22 RAIL-1.0 BigScience General 176B 366B Dedup PR 384 80G A100 2520h 3.87 Mil Megatron DS",
    "chunk_index": 267,
    "start_pos": 118654,
    "end_pos": 119093,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 489,
      "optimized_content_length": 426
    }
  },
  "360": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_268",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "PR 384 80G A100 2520h 3.87 Mil Megatron DS Galactica 148 arXiv 22 Apache-2.0 Meta Science 120B 225k 106B Dedup 128 80GB A100 Metaseq GLaM 91 ICML 22 Google General 1.2T 600k 600B Clf 1024 TPU v4 GSPMD LaMDA 150 arXiv 22 Google Dialog 137B 3M 2.81T Filtered 1024 TPU v3 1384h 4.96 Mil Lingvo MT-NLG 117 arXiv 22 Apache-v2.0 MS. Nvidia General 530B 270B 4480 80G A100 Megatron DS AlphaCode 142 Science 22 Apache-v2.0 Google Coding 41B 205k 967B Heur Dedup TPU v4 JAX Haiku",
    "chunk_index": 268,
    "start_pos": 119094,
    "end_pos": 119580,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 97,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 470
    }
  },
  "361": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_269",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "1B 205k 967B Heur Dedup TPU v4 JAX Haiku Chinchilla 96 arXiv 22 Google General 70B 1.4T QF Dedup TPUv4 JAX Haiku PaLM 15 arXiv 22 Google General 540B 255k 780B Heur 6144 TPU v4 JAX T5X AlexaTM 122 arXiv 22 Apache v2.0 Amazon General 20B 500k 1.1T Filtered 128 A100 2880h 1.47 Mil DS U-PaLM 124 arXiv 22 Google General 540B 20k 512 TPU v4 120h 0.25 Mil UL2 125 ICLR 23 Apache-2.0 Google General 20B 2M 1T 512 TPU v4 JAX T5X",
    "chunk_index": 269,
    "start_pos": 119581,
    "end_pos": 120020,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 99,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 422
    }
  },
  "362": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_270",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "gle General 20B 2M 1T 512 TPU v4 JAX T5X GLM 33 ICLR 23 Apache-2.0 Multiple General 130B 400B 768 40G A100 1440h 3.37 Mil CodeGen 140 ICLR 23 Apache-2.0 Salesforce Coding 16B 650k 577B Heur Dedup TPU v4 JAXFormer LLaMA 127 arXiv 23 Meta General 65B 350k 1.4T Clf Heur Dedup 2048 80G A100 504h 4.12 Mil xFormers PanGu 92 arXiv 23 Huawei General 1.085T 329B 512 Ascend 910 2400h OP MindSpore",
    "chunk_index": 270,
    "start_pos": 120021,
    "end_pos": 120423,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 453,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 453,
      "optimized_content_length": 389
    }
  },
  "363": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_271",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "329B 512 Ascend 910 2400h OP MindSpore BloombergGPT 151 arXiv23 Bloomberg Finance 50B 139k 569B Dedup 512 40G A100 1272h 1.97 Mil PyTorch Xuan Yuan 2.0 152 arXiv23 RAIL-1.0 Du Xiaoman Finance 176B 366B Filtered 80GB A100 DS CodeT5 34 arXiv 23 BSD-3 Salesforce Coding 16B 110k 51.5B Dedup 16 40G A100 DS StarCoder 147 arXiv 23 OpenRAIL-M BigCode Coding 15.5B 250k 1T Dedup QF PF 512 80G A100 624h 1.28 Mil Megatron-LM",
    "chunk_index": 271,
    "start_pos": 120424,
    "end_pos": 120844,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 471,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 471,
      "optimized_content_length": 416
    }
  },
  "364": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_272",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "QF PF 512 80G A100 624h 1.28 Mil Megatron-LM LLaMA-2 21 arXiv 23 LLaMA-2.0 Meta General 70B 500k 2T Minimal Filtering 80G A100 1.7Mh PaLM-2 123 arXiv 23 Google General -Ddedup PF QF LLaMA-3.1 130 arXiv 24 LLaMA-3.0 Meta General 405B 1.2M 15T Dedup QF 16k 80G H100 30.84Mh PyTorch Mixtral 8x22B 131 web 24 Apache-2.0 Mistral AI General 141B Snowflake Arctic 132 web 24 Apache-2.0 Snowflake General 480B 3.5T DS",
    "chunk_index": 272,
    "start_pos": 120845,
    "end_pos": 121298,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 99,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 409
    }
  },
  "365": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_273",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "2.0 Snowflake General 480B 3.5T DS Nemotron-4 340B 137 web 24 Nvidia Nvidia General 340B 9T 6144 80G H100 DeepSeek 138 arXiv 24 MIT DeepSeek General 67B 2T Dedup QF 300.6Kh DS DeepSeek-v2 139 arXiv 24 MIT DeepSeek General 67B 8.1T QF H800 172.8Kh HAI-LLM Table 4. Summary of instruction tuned LLMs 10B All abbreviations are the same as Table 3. Entries in Data Tokens starting with S- represent the number of training samples. ModelsPublication VenueLicense TypeModel",
    "chunk_index": 273,
    "start_pos": 121299,
    "end_pos": 121794,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 100,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 467
    }
  },
  "366": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_274",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "samples. ModelsPublication VenueLicense TypeModel Creators PurposeNo. of ParamsCommercial UsePre-trained ModelsSteps TrainedData TokensNo. of Processing UnitsProcessing Unit TypeTrain. TimeCalculated Train. CostTrain. Parallelism Library WebGPT 166 arXiv 21 OpenAI General 175B GPT-3 T0 17 ICLR 22 Apache-2.0 BigScience General 11B T5 250B 512 TPU v3 270h 0.48 Mil Tk-Instruct 18 EMNLP 22 MIT AI2 General 11B T5 1000 256 TPU v3 4h 0.0036 Mil Google T5",
    "chunk_index": 274,
    "start_pos": 121795,
    "end_pos": 122238,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 451
    }
  },
  "367": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_275",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "T5 1000 256 TPU v3 4h 0.0036 Mil Google T5 OPT-IML 97 arXiv 22 Meta General 175B OPT 8k 2B 128 40G A100 Megatron Flan-U-PaLM 16 ICLR 22 Apache-2.0 Google General 540B U-PaLM 30k 512 TPU v4 JAX T5X mT0 154 ACL 23 Apache-2.0 HuggingFace General 13B mT5 Sparrow 167 arXiv 22 Google Dialog 70B Chinchilla 64 TPU v3 WizardCoder 164 arXiv 23 Apache-2.0 HK Bapt. Coding 15B StarCoder 200 S-78k",
    "chunk_index": 275,
    "start_pos": 122239,
    "end_pos": 122660,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 472,
      "word_count": 100,
      "optimized_for_embedding": true,
      "original_content_length": 472,
      "optimized_content_length": 386
    }
  },
  "368": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_276",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Bapt. Coding 15B StarCoder 200 S-78k Alpaca 158 Github 23 Apache-2.0 Stanford General 13B LLaMA 3-Epoch S-52k 80G A100 3h 600 FSDP PyTorch Vicuna 159 Github 23 Apache-2.0 LMSYS General 13B LLaMA 3-Epoch S-125k FSDP PyTorch LIMA 185 arXiv 23 Meta General 65B LLaMA 15-Epoch S-1000 Koala 300 Github 23 Apache-2.0 UC-Berkley General 13B LLaMA 2-Epoch S-472k A100 6h 100 JAX FLAX 5. Datasets and Evaluation Generating training and evaluation datasets is expensive be-",
    "chunk_index": 276,
    "start_pos": 122661,
    "end_pos": 123131,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 463
    }
  },
  "369": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_277",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "training and evaluation datasets is expensive be- cause of the large-scale data demand of LLMs. Hence. datasets for training and benchmarking these models are topics of key importance. summary of datasets commonly used by LLMs is provided next.5.1. Training Datasets The performance of LLMs largely depends on the training data quality. size. and diversity. Preparing training datasets of high quality at large scale is laborious. Researchers have suggested various pre-training and fine-tuning datasets to en-",
    "chunk_index": 277,
    "start_pos": 123132,
    "end_pos": 123598,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 510
    }
  },
  "370": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_278",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rious pre-training and fine-tuning datasets to en- hance LLMs capabilities. We summarize these fforts in Ta- ble 8. While numerous training datasets are available in the literature. we cover the most widely used ones in our summary. 25",
    "chunk_index": 278,
    "start_pos": 123599,
    "end_pos": 123785,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 237,
      "word_count": 39,
      "optimized_for_embedding": true,
      "original_content_length": 237,
      "optimized_content_length": 235
    }
  },
  "371": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_279",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "cover the most widely used ones in our summary. 25 --- Page 26 --- Table 5. Architecture details of LLMs. Here. PE is the positional embedding. nL is the number of layers. nH is the number of attention heads. HS is the size of hidden states. Models TypeTraining ObjectiveAttention Vocab Tokenizer Norm PE Activation Bias nL nH HS T5 11B Enc-Dec Span Corruption Standard 32k SentencePiece Pre-RMS Relative ReLU 24 128 1024 GPT3 175B Causal-Dec Next Token Dense Sparse Layer Learned GeLU 96 96 12288",
    "chunk_index": 279,
    "start_pos": 123787,
    "end_pos": 124253,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 497
    }
  },
  "372": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_280",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Dense Sparse Layer Learned GeLU 96 96 12288 mT5 13B Enc-Dec Span Corruption Standard 250k SentencePiece Pre-RMS Relative ReLU PanGu-\u03b1 200B Causal-Dec Next Token Standard 40k BPE Layer 64 128 16384 CPM-2 198B Enc-Dec Span Corruption Standard 250k SentencePiece Pre-RMS Relative ReLU 24 64 Codex 12B Causal-Dec Next Token Standard BPE Pre-Layer Learned GeLU 96 96 12288 ERNIE 3.0 10B Causal-Dec Next Token Standard WordPiece Post-Layer Relative GeLU 48 64 4096",
    "chunk_index": 280,
    "start_pos": 124254,
    "end_pos": 124704,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 458
    }
  },
  "373": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_281",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "WordPiece Post-Layer Relative GeLU 48 64 4096 Jurassic-1 178B Causal-Dec Next Token Standard 256k SentencePiece Pre-Layer Learned GeLU 76 96 13824 HyperCLOV 82B Causal-Dec Next Token Dense Sparse BPE Pre-Layer Learned GeLU 64 80 10240 Yuan 1.0 245B Causal-Dec Next Token Standard 76 -16384 Gopher 280B Causal-Dec Next Token Standard 32k SentencePiece Pre-RMS Relative GeLU 80 128 16384 ERNIE 3.0 Titan 260B Causal-Dec Next Token Standard WordPiece Post-Layer Relative GeLU 48 192 12288",
    "chunk_index": 281,
    "start_pos": 124705,
    "end_pos": 125182,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 485
    }
  },
  "374": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_282",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "WordPiece Post-Layer Relative GeLU 48 192 12288 GPT-NeoX-20B Causal-Dec Next Token Parallel 50k BPE Layer Rotary GeLU 44 64 OPT 175B Causal-Dec Next Token Standard BPE ReLU 96 96 BLOOM 176B Causal-Dec Next Token Standard 250k BPE Layer ALiBi GeLU 70 112 14336 Galactica 120B Causal-Dec Next Token Standard 50k BPE custom Layer Learned GeLU 96 80 10240 GLaM 1.2T MoE-Dec Next Token Standard 256k SentencePiece Layer Relative GeLU 64 128 32768",
    "chunk_index": 282,
    "start_pos": 125183,
    "end_pos": 125603,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 471,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 470,
      "optimized_content_length": 441
    }
  },
  "375": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_283",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "SentencePiece Layer Relative GeLU 64 128 32768 LaMDA 137B Causal-Dec Next Token Standard 32k BPE Layer Relative GeGLU 64 128 8192 MT-NLG 530B Causal-Dec Next Token Standard 50k BPE Pre-Layer Learned GeLU 105 128 20480 AlphaCode 41B Enc-Dec Next Token Multi-query 8k SentencePiece 64 128 6144 Chinchilla 70B Causal-Dec Next Token Standard 32k SentencePiece-NFKC Pre-RMS Relative GeLU 80 64 8192 PaLM 540B Causal-Dec Next Token Parallel Multi-query 256k SentencePiece Layer RoPE SwiGLU 118 48 18432",
    "chunk_index": 283,
    "start_pos": 125604,
    "end_pos": 126078,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 496
    }
  },
  "376": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_284",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "256k SentencePiece Layer RoPE SwiGLU 118 48 18432 AlexaTM 20B Enc-Dec Denoising Standard 150k SentencePiece Pre-Layer Learned GeLU 78 32 4096 Sparrow 70B Causal-Dec Pref. Rule RM 32k SentencePiece-NFKC Pre-RMS Relative GeLU 16 64 8192 U-PaLM 540B Non-Causal-Dec MoD Parallel Multi-query 256k SentencePiece Layer RoPE SwiGLU 118 48 18432 UL2 20B Enc-Dec MoD Standard 32k SentencePiece 64 16 4096 GLM 130B Non-Causal-Dec AR Blank Infilling Standard 130k SentencePiece Deep RoPE GeGLU 70 96 12288",
    "chunk_index": 284,
    "start_pos": 126079,
    "end_pos": 126549,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 493
    }
  },
  "377": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_285",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "130k SentencePiece Deep RoPE GeGLU 70 96 12288 CodeGen 16B Causal-Dec Next Token Parallel BPE Layer RoPE 34 24 LLaMA 65B Causal-Dec Next Token Standard 32k BPE Pre-RMS RoPE SwiGLU 80 64 8192 PanGu- 1085B Causal-Dec Next Token Standard BPE Fused Layer FastGeLU 40 40 5120 BloombergGPT 50B Causal-Dec Next Token Standard 131k Unigram Layer ALiBi GeLU 70 40 7680 Xuan Yuan 2.0 176B Causal-Dec Next Token Self 250k BPE Layer ALiBi GeLU 70 112 14336",
    "chunk_index": 285,
    "start_pos": 126550,
    "end_pos": 126978,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 479,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 479,
      "optimized_content_length": 444
    }
  },
  "378": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_286",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "oken Self 250k BPE Layer ALiBi GeLU 70 112 14336 CodeT5 16B Enc-Dec SC NT Cont. Match Standard Code-Specific StarCoder 15.5B Causal-Dec FIM Multi-query 49k BPE Learned 40 48 6144 LLaMA-2 70B Causal-Dec Next Token Grouped-query 32k BPE Pre-RMS RoPE SwiGLUE PaLM-2 MoD Parallel LLaMA-3.1 405B Causal-Dec Next Token Grouped-query 128k BPE Pre-RMS RoPE SwiGLU -126 128 16384 Nemotron-4 340B Causal-Dec Next Token Standard 256k SentencePiece RoPE ReLU 96 96 18432",
    "chunk_index": 286,
    "start_pos": 126979,
    "end_pos": 127453,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 96,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 458
    }
  },
  "379": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_287",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "andard 256k SentencePiece RoPE ReLU 96 96 18432 DeepSeek 67B Causal-Dec Next Token Grouped-query 100k BBPE Pre-RMS RoPE SwiGLU 95 64 8192 DeepSeek-v2 67B MoE-Dec Next Token Multi-Head Latent 100k BBPE Pre-RMS RoPE SwiGLU 60 128 5120 5.2. Evaluation Datasets and Tasks The evaluation of LLMs is important in gauging their profi- ciency and limitations. This process measures the model abil- ity to comprehend. generate. and interact with human language across spectrum of tasks. Evaluating language model LM",
    "chunk_index": 287,
    "start_pos": 127454,
    "end_pos": 127928,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 506
    }
  },
  "380": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_288",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "pectrum of tasks. Evaluating language model LM is divided into two broader categories. natural language un- derstanding NLU and natural language generation NLG It is emphasized that tasks in NLU and NLG are softly catego- rized and are often used interchangeably in the literature. Natural Language Understanding. It measures the language understanding capacity of LMs. It encompasses multiple tasks. including sentiment analysis. text classification. natural lan- guage inference NLI question answering QA common-",
    "chunk_index": 288,
    "start_pos": 127929,
    "end_pos": 128413,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 514
    }
  },
  "381": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_289",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "inference NLI question answering QA common- sense reasoning CR mathematical reasoning MR reading comprehension RC etc. Natural Language Generation. It assesses the language gener- ation capabilities of LLMs by understanding the provided input context. It includes tasks such as summarization. sentence com- pletion. machine translation MT dialogue generation. etc. Numerous datasets are proposed for each task. evaluating LLMs against di fferent characteristics. To provide an overview",
    "chunk_index": 289,
    "start_pos": 128414,
    "end_pos": 128867,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 503,
      "optimized_content_length": 485
    }
  },
  "382": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_290",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "di fferent characteristics. To provide an overview of evaluation datasets. we briefly discuss few famous datasets within each category and ffer comprehensive list of datasets in Table 9. Moreover. we show detailed overview of the train- ing datasets and evaluation tasks and benchmarks used by vari-ous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta- ble 11. We also compare the top-performing LLMs in various NLP tasks in Table 12. 5.2.1. Multi-task MMLU 307 benchmark that measures the knowledge",
    "chunk_index": 290,
    "start_pos": 128868,
    "end_pos": 129335,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 505
    }
  },
  "383": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_291",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "MLU 307 benchmark that measures the knowledge acquired by models during pretraining and evaluates models in zero-shot and few-shot settings across 57 subjects. testing both world knowledge and problem-solving ability. SuperGLUE more challenging and diverse successor to the GLUE 309 benchmark. SuperGLUE includes variety of language understanding tasks. such as question answering. natural language inference. and co-reference resolution. It is designed to provide rigorous test of language understanding",
    "chunk_index": 291,
    "start_pos": 129336,
    "end_pos": 129807,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 504
    }
  },
  "384": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_292",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "provide rigorous test of language understanding and requires significant progress in areas like sample-e fficient. transfer. multi-task. and unsupervised or self-supervised learn- ing. BIG-bench 308 The BIG-bench Behavior of Intelligent Generative Models Benchmark is large-scale benchmark de- signed to test the abilities of LLMs across wide range of tasks. including reasoning. creativity. ethics. and understanding of specific domains. GLUE 309 The General Language Understanding Evalua-",
    "chunk_index": 292,
    "start_pos": 129808,
    "end_pos": 130262,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 490
    }
  },
  "385": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_293",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "309 The General Language Understanding Evalua- tion GLUE benchmark is collection of resources for train- ing. evaluating. and analyzing natural language understanding 26",
    "chunk_index": 293,
    "start_pos": 130263,
    "end_pos": 130389,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 177,
      "word_count": 24,
      "optimized_for_embedding": true,
      "original_content_length": 176,
      "optimized_content_length": 169
    }
  },
  "386": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_294",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "g. and analyzing natural language understanding 26 --- Page 27 --- Table 6. Summary of optimization settings used for pre-trained LLMs. The values for weight decay. gradient clipping. and dropout are 0.1. 1.0. and 0.1. respectively. for most of the LLMs. Sequence LR Optimizers Precision Weight Grad Models Batch Size Length LR Warmup Decay AdaFactor Adam AdamW FP16 BF16 Mixed Decay Clip Dropout T5 11B 211512 0.01 inverse square root GPT3 175B 32K 6e-5 cosine",
    "chunk_index": 294,
    "start_pos": 130391,
    "end_pos": 130834,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 461
    }
  },
  "387": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_295",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "GPT3 175B 32K 6e-5 cosine mT5 13B 1024 1024 0.01 inverse square root PanGu-\u03b1 200B 1024 2e-5 CPM-2 198B 1024 1024 0.001 Codex 12B 6e-5 cosine ERNIE 3.0 12B 6144 512 1e-4 linear Jurassic-1 178B 3.2M 2048 6e-5 cosine HyperCLOV 82B 1024 6e-5 cosine Yuan 1.0 245B 10M 2048 1.6e-4 cosine decay to 10 Gopher 280B 3M 2048 4e-5 cosine decay to 10",
    "chunk_index": 295,
    "start_pos": 130835,
    "end_pos": 131315,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 147,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 337
    }
  },
  "388": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_296",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "280B 3M 2048 4e-5 cosine decay to 10 ERNIE 3.0 Titan 260B 512 1e-4 linear GPT-NeoX-20B 1538 2048 0.97e-5 cosine OPT 175B 2M 2048 1.2e-4 linear BLOOM 176B 2048 2048 6e-5 cosine Galactica 120B 2M 2048 7e-6 linear decay to 10 GLaM 1.2T 1M 1024 0.01 inverse square root FP32 LaMDA 137B 256K MT-NLG 530B 1920 2048 5e-5 cosine decay to 10 AlphaCode 41B 2048 1536 768 1e-4 cosine decay to 10",
    "chunk_index": 296,
    "start_pos": 131316,
    "end_pos": 131808,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 138,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 384
    }
  },
  "389": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_297",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "2048 1536 768 1e-4 cosine decay to 10 Chinchilla 70B 1.5M 2048 1e-4 cosine decay to 10 PaLM 540B 2048 2048 0.01 inverse square root AlexaTM 20B 2M 1024 1e-4 linear decay to U-PaLM 540B 32 2048 1e-4 cosine UL2 20B 1024 1024 inverse square root GLM 130B 4224 2048 8e-5 cosine CodeGen 16B 2M 2048 5e-5 cosine LLaMA 65B 4M Tokens 2048 1.5e-4 cosine decay to 10",
    "chunk_index": 297,
    "start_pos": 131809,
    "end_pos": 132268,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 133,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 356
    }
  },
  "390": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_298",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ns 2048 1.5e-4 cosine decay to 10 PanGu- 1.085T 512 1024 2e-5 BloombergGPT 50B 2048 2048 6e-5 cosine Xuan Yuan 2.0 176B 2048 2048 6e-5 cosine CodeT5 16B 2048 1024 2e-4 linear StarCoder 15.5B 512 8k 3e-4 cosine LLaMA-2 70B 4M Tokens 4k 1.5e-4 cosine LLaMA-3.1 405B 16M 8192 8e-5 linear cosine Nemotron-4 340B 2304 4096 linear DeepSeek 67B 4608 4096 3.2e-4 cosine",
    "chunk_index": 298,
    "start_pos": 132269,
    "end_pos": 132731,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 127,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 361
    }
  },
  "391": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_299",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "DeepSeek 67B 4608 4096 3.2e-4 cosine DeepSeek-v2 67B 9216 4k 2.4e-4 step-decay Table 7. Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models. while no model uses weight decay for instruction tuning. Sequence Optimizers Grad Models Batch Size Length LR Warmup LR_Decay AdaFactor Adam AdamW Clip Dropout WebGPT 175B BC.512. RM.32 -6e-5 T0 11B 1024 1280 1e-3",
    "chunk_index": 299,
    "start_pos": 132732,
    "end_pos": 133185,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 448
    }
  },
  "392": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_300",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "-6e-5 T0 11B 1024 1280 1e-3 Tk-Instruct 11B 1024 -1e-5 constant OPT-IML 175B 128 2048 5e-5 linear Flan-U-PaLM 540B 32 -1e-3 constant Sparrow 70B RM. 16. RL.16 -2e-6 cosine decay to 10 WizardCoder 15B 512 2048 2e-5 cosine Alpaca 13B 128 512 1e-5 cosine Vicuna 13B 128 -2048 2e-5 cosine LIMA 65B 32 2048 1e-5 linear systems. It includes variety of tasks that test wide range of",
    "chunk_index": 300,
    "start_pos": 133186,
    "end_pos": 133628,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 113,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 375
    }
  },
  "393": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_301",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ludes variety of tasks that test wide range of linguistic phenomena. making it comprehensive tool for eval- uating language understanding in AI. 5.2.2. Language Understanding WinoGrande 354 large-scale dataset inspired by the orig- inal Winograd 357 Schema Challenge tests models on their ability to resolve pronoun ambiguity and encourages the devel- opment of models that understand the broad context in naturallanguage text. CoQA 316 conversational question-answering dataset.",
    "chunk_index": 301,
    "start_pos": 133629,
    "end_pos": 134075,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 479
    }
  },
  "394": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_302",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "316 conversational question-answering dataset. CoQA challenges models with questions that rely on conver- sation history and require free-form text answers. Its diverse content from seven domains makes it rigorous test for mod- els ability to handle wide range of topics and conversational contexts. WiC 317 This dataset assesses model ability to dis- cern word meanings based on context. aiding in tasks related 27",
    "chunk_index": 302,
    "start_pos": 134076,
    "end_pos": 134456,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 431,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 431,
      "optimized_content_length": 415
    }
  },
  "395": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_303",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nings based on context. aiding in tasks related 27 --- Page 28 --- Table 8. Details of various well-known pre-training and fine-tuning datasets. Here. alignment means aligning with human preferences. Dataset Type Size Samples Tasks Source Creation Comments C4 10 Pretrain 806GB Common Crawl Automated clean. multilingual dataset with billions of tokens mC4 11 Pretrain 38.49TB Common Crawl Automated multilingual extension of the C4 dataset. mC4 identifies over 100 lan- guages using cld3 from 71 monthly web scrapes of Common Crawl.",
    "chunk_index": 303,
    "start_pos": 134458,
    "end_pos": 134952,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 533
    }
  },
  "396": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_304",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "cld3 from 71 monthly web scrapes of Common Crawl. PILE 301 Pretrain 825GB -Common Crawl. PubMed Central. OpenWebText2. ArXiv. GitHub. Books3. and othersAutomated massive dataset comprised of 22 con- stituent sub-datasets ROOTs 302 Pretrain 1.61TB 498 Hugging Face datasets Automated 46 natural and 13 programming lan- guages MassiveText 116 Pretrain 10.5TB -MassiveWeb. Books. News. Wikipedia. Github. C4Automated 99 of the data is in English Wikipedia 303 Pretrain Wikipedia Automated Dump of wikipedia",
    "chunk_index": 304,
    "start_pos": 134953,
    "end_pos": 135423,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 503
    }
  },
  "397": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_305",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Pretrain Wikipedia Automated Dump of wikipedia RedPajama 304 Pretrain 5TB -CommonCrawl. C4. Wikipedia. Github. Books. StackExchangeAutomated Open-source replica of LLaMA dataset PushShift.io Reddit Pretrain 21.1GB Reddit Automated Submissions and comments on Reddit from 2005 to 2019 BigPython 140 Pretrain 5.5TB Coding GitHub Automated Pool of Prompt P3 17 Instructions 12M 62 PromptSource Manual Subset of PromptSource. created from 177 datasets including summarization. QA. classification. etc.",
    "chunk_index": 305,
    "start_pos": 135424,
    "end_pos": 135888,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 515,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 497
    }
  },
  "398": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_306",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "including summarization. QA. classification. etc. xP3 154 Instructions 81M 71 P3 Multilingual datasets Manual Extending P3 to total 46 languages Super-NaturalInstructions SNI 18 Instructions 12.4M 1616 Multiple datasets Manual Extending P3 with additional multi- lingual datasets. total 46 languages Flan 16 Instructions 15M 1836 Muffin T0-SF NIV2 Manual Total 60 languages OPT-IML 97 Instructions 18.1M 1667 Manual Self-Instruct 19 Instructions 82k 175 Automated Generated 52k instructions with 82k sam-",
    "chunk_index": 306,
    "start_pos": 135889,
    "end_pos": 136362,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 504
    }
  },
  "399": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_307",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Automated Generated 52k instructions with 82k sam- ples from 175 seed tasks using GPT-3 Alpaca 158 Instructions 52k Automated Employed self-instruct method to gener- ate data from text-davinci-003 Vicuna 159 Instructions 125k ShareGPT Automated Conversations shared by users on ShareGPT using public APIs LLaMA-GPT-4 160 Instructions 52k Alpaca Automated Recreated Alpaca dataset with GPT-4 in English and Chinese Unnatural Instructions 305 Instructions 68k 15-Seeds SNI Automated",
    "chunk_index": 307,
    "start_pos": 136363,
    "end_pos": 136814,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 480
    }
  },
  "400": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_308",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "305 Instructions 68k 15-Seeds SNI Automated LIMA 185 Instructions 1k Multiple datasets Manual Carefully created samples to test perfor- mance with fine-tuning on less data Anthropic-HH-RLHF 306 Alignment 142k Manual Anthropic-HH-RLHF-2 178 Alignment 39k Manual to Word Sense Disambiguation. Wikitext103 318 With over 100 million tokens from Wikipedia top articles. this dataset is rich resource for tasks that require understanding long-term dependencies. such as lan- guage modeling and translation.",
    "chunk_index": 308,
    "start_pos": 136815,
    "end_pos": 137294,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 500
    }
  },
  "401": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_309",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "cies. such as lan- guage modeling and translation. PG19 319 This is digital library of diverse books from Project Gutenberg. It is specifically designed to facilitate re- search in unsupervised learning and language modeling. with special focus on long-form content. C4 10 clean. multilingual dataset. C4 ffers billions of to- kens from web-crawled data. It is comprehensive resource for training advanced Transformer models on various languages. LCQMC 320 The Large-scale Chinese Question Matching",
    "chunk_index": 309,
    "start_pos": 137295,
    "end_pos": 137761,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 498
    }
  },
  "402": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_310",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "320 The Large-scale Chinese Question Matching Corpus LCQMC is dataset for evaluating the performance of models in semantic matching tasks. It contains pairs of ques- tions in Chinese and their matching status. making it valuable resource for research in Chinese language understanding. 5.2.3. Story Cloze and Sentence Completion StoryCloze 334 It introduces new StoryCloze Test commonsense reasoning framework for evaluating story under-",
    "chunk_index": 310,
    "start_pos": 137762,
    "end_pos": 138169,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 458,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 458,
      "optimized_content_length": 437
    }
  },
  "403": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_311",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "se reasoning framework for evaluating story under- standing. generation. and script learning. It considers model sability to understand and generate coherent and sensible stories. LAMBADA 335 This dataset evaluates contextual text un- derstanding through word prediction task. Models must pre- dict the last word of passage. which is easy for humans when given the whole passage. but not when given only the last sen- tence. 5.2.4. Physical Knowledge and World Understanding PIQA 340 dataset that probes the physical knowledge of",
    "chunk_index": 311,
    "start_pos": 138170,
    "end_pos": 138662,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 529
    }
  },
  "404": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_312",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "dataset that probes the physical knowledge of models. aiming to understand how well they are learning about the real world. TriviaQA 341 dataset that tests models on reading com- prehension and open domain question answering QA tasks. with focus on Information Retrieval IR -style QA. ARC 342 larger version of the ARC-Challenge. this dataset contains both easy and challenging grade-school level. multiple-choice science questions. It is comprehensive test of model ability to understand and answer complex questions.",
    "chunk_index": 312,
    "start_pos": 138663,
    "end_pos": 139156,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 518
    }
  },
  "405": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_313",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "bility to understand and answer complex questions. ARC-Easy 342 subset of the ARC dataset. ARC- Easy. contains questions that are answered correctly by either retrieval-based algorithm or word co-occurrence algorithm. 28",
    "chunk_index": 313,
    "start_pos": 139157,
    "end_pos": 139335,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 229,
      "word_count": 34,
      "optimized_for_embedding": true,
      "original_content_length": 229,
      "optimized_content_length": 220
    }
  },
  "406": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_314",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ed algorithm or word co-occurrence algorithm. 28 --- Page 29 --- Table 9. Categorized evaluation datasets used in evaluating LLMs. Type Datasets Benchmarks Multi-Task MMLU 307 SuperGLUE BIG-bench 308 GLUE 309 BBH 308 CUGE 310 Zero- CLUE 311 FewCLUE 312 Blended Skill Talk 313 HELM 314 KLUE-STS 315 Language Understanding CoQA 316 WiC 317 Wikitext103 318 PG19 319 LCQMC 320 QQP 321 WinoGender 322 CB 323 FinRE 324 SanWen 325 AFQMC 311 BQ Corpus 326 CNSS 327 CKBQA 13 328",
    "chunk_index": 314,
    "start_pos": 139337,
    "end_pos": 139834,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 469
    }
  },
  "407": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_315",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "311 BQ Corpus 326 CNSS 327 CKBQA 13 328 CLUENER 311 Weibo 329 AQuA 330 OntoNotes 331 HeadQA 332 Twitter Dataset 333 Story Cloze and Sentence CompletionStoryCloze 334 LAMBADA 335 LCSTS 336 AdGen 337 E2E 338 CHID 339 CHID- FC 312 Physical Knowledge and World UnderstandingPIQA 340 TriviaQA 341 ARC 342 ARC-Easy 342 ARC-Challenge 342 PROST 343 Open- BookQA 344 WebNLG 345 DogWhistle Insider Outsider 346 Contextual Language",
    "chunk_index": 315,
    "start_pos": 139835,
    "end_pos": 140280,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 420
    }
  },
  "408": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_316",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "istle Insider Outsider 346 Contextual Language UnderstandingRACE 347 RACE-Middle 347 RACE-High 347 QuAC 348 StrategyQA 349 Quiz Bowl 350 cMedQA 351 .cMedQA2 352 MATINF-QA 353 Commonsense Reasoning WinoGrande 354 HellaSwag 355 COPA 356 WSC 357 CSQA 358 SIQA 359 C3 360 CLUEWSC2020 311 CLUEWSC 311 CLUEWSC-FC 312 ReCoRD 361 Reading Comprehension SQuAD 362 BoolQ 363 SQUADv2 364 DROP 365 RTE 366 WebQA 367 CMRC2017 368",
    "chunk_index": 316,
    "start_pos": 140281,
    "end_pos": 140725,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 415
    }
  },
  "409": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_317",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ROP 365 RTE 366 WebQA 367 CMRC2017 368 CMRC2018 369 CMRC2019 370 COTE-BD 371 COTE-DP 371 COTE-MFW 371 Mul- tiRC 372 Natural Questions 373 CNSE 327 DRCD 374 DuReader 375 Dureader robust 376 DuReader-QG 375 SciQ 377 Sogou-log 378 Dureader robust-QG 376 QA4MRE 379 KorQuAD 1.0 380 CAIL2018-Task1 Task2 381 Mathematical Reasoning MATH 382 Math23k 383 GSM8K 384 MathQA 385 MGSM 386 MultiArith 387 AS- Div 388 MAWPS 389 SV AMP 390",
    "chunk_index": 317,
    "start_pos": 140726,
    "end_pos": 141192,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 424
    }
  },
  "410": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_318",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "th 387 AS- Div 388 MAWPS 389 SV AMP 390 Problem Solving HumanEval 141 DS-1000 391 MBPP 392 APPS 382 CodeContests 142 Natural Language Inference Logical ReasoningANLI 393 MNLI-m 394 MNLI-mm 394 .QNLI 362 WNLI 357 OCNLI 311 CMNLI 311 ANLI R1 393 ANLI R2 393 ANLI R3 393 HANS 395 OCNLI-FC 312 LogiQA 396 Strate- gyQA 349 Cross-Lingual Understanding MLQA 397 XNLI 398 PAWS-X 399 XSum 400 XCOPA 401 XWinograd 402 TyDiQA- GoldP 403 MLSum 404",
    "chunk_index": 318,
    "start_pos": 141193,
    "end_pos": 141666,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 435
    }
  },
  "411": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_319",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "XWinograd 402 TyDiQA- GoldP 403 MLSum 404 Truthfulness and Fact Checking TruthfulQA 405 MultiFC 406 Fact Checking on Fever 407 Biases and Ethics in AI ETHOS 408 StereoSet 409 BBQ 410 Winobias 411 CrowS-Pairs 412 Toxicity RealToxicityPrompts 413 CivilComments toxicity classification 414 Language Translation WMT 415 WMT20 416 WMT20-enzh 416 EPRSTMT 312 CCPM 417 Scientific Knowledge AminoProbe 148 BioLAMA 148 Chemical Reactions 148 Galaxy Clusters 148 Mineral Groups 148",
    "chunk_index": 319,
    "start_pos": 141667,
    "end_pos": 142151,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 471
    }
  },
  "412": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_320",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "148 Galaxy Clusters 148 Mineral Groups 148 Dialogue Wizard of Wikipedia 418 Empathetic Dialogues 419 DPC-generated 96 dialogues. ConvAI2 420 KdConv 421 Topic Classification TNEWS-FC 312 YNAT 315 KLUE-TC 315 CSL 311 CSL-FC 312 IFLYTEK 422 It is great starting point for models beginning to explore ad- vanced question-answering. ARC-Challenge 342 rigorous question-answering dataset. ARC-Challenge includes complex. grade-school level questions that demand reasoning beyond simple retrieval. test-",
    "chunk_index": 320,
    "start_pos": 142152,
    "end_pos": 142642,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 496
    }
  },
  "413": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_321",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "at demand reasoning beyond simple retrieval. test- ing the true comprehension capabilities of models. 5.2.5. Contextual Language Understanding RACE 347 The RACE dataset is reading comprehension dataset collected from English examinations in China. which benchmarks AI models for understanding and answering ques- tions on long and complex passages. simulating the challenge of real-world examination. RACE-Middle 347 Another subset of the RACE 347 dataset. RACE-Middle. contains middle school-level English",
    "chunk_index": 321,
    "start_pos": 142643,
    "end_pos": 143110,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 506
    }
  },
  "414": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_322",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "RACE-Middle. contains middle school-level English exam questions. It ffers slightly less challenging but academ- ically oriented evaluation of model comprehension skills. RACE-High 347 subset of the RACE 347 dataset. RACE-High consists of high school-level English exam ques-tions. It is designed to evaluate the comprehension ability of models in more academic and challenging context. QuAC 348 This dataset simulates an information-seeking dialog between students and teachers using hidden Wikipedia",
    "chunk_index": 322,
    "start_pos": 143111,
    "end_pos": 143582,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 501
    }
  },
  "415": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_323",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tween students and teachers using hidden Wikipedia text. It introduces unique challenges not found in machine com- prehension datasets. making it valuable resource for advanc- ing dialog systems. 5.2.6. Commonsense Reasoning HellaSwag 355 dataset that challenges models to pick the best ending to context uses Adversarial Filtering to create Goldilocks zone of complexity. where generated text is absurd to humans but often misclassified by models. COPA 401 This dataset evaluates model progress in",
    "chunk_index": 323,
    "start_pos": 143583,
    "end_pos": 144050,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 498
    }
  },
  "416": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_324",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "401 This dataset evaluates model progress in open-domain commonsense causal reasoning. Each question comprises premise and two alternatives. and the model must select the more plausible alternative. testing model ability to understand and reason about cause and ffect. WSC 357 The Winograd Schema Challenge WSC is 29",
    "chunk_index": 324,
    "start_pos": 144051,
    "end_pos": 144337,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 337,
      "word_count": 52,
      "optimized_for_embedding": true,
      "original_content_length": 337,
      "optimized_content_length": 316
    }
  },
  "417": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_325",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "357 The Winograd Schema Challenge WSC is 29 --- Page 30 --- Table 10. An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here. QA is question-answering. Clf is classification. NLI is natural language inference. MT is machine translation. RC is reading comprehension. CR is commonsense reasoning. MR is mathematical reasoning. Mem. is memorization. Benchmark Models Training DatasetBIG- benchMMLUSuper GLUEQA Clf NLI MTCloze CompletionRC CR MR CodingTruthful Bias Toxicity Mem.",
    "chunk_index": 325,
    "start_pos": 144339,
    "end_pos": 144832,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 513
    }
  },
  "418": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_326",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "onRC CR MR CodingTruthful Bias Toxicity Mem. T5 C4 10 GPT-3 Common Crawl. WebText. Books Cor- pora. Wikipedia mT5 mC4 11 PanGu-\u03b1 1.1TB Chinese Text Corpus CPM-2 WuDaoCorpus 109 Codex 54 million public repositories from Github ERNIE-3.0 Chinese text corpora. Baidu Search. Web text. QA-long. QA-short. Poetry and Cou- plet Domain-specific data from medical. law. and financial area Baidu knowledge graph with more than 50 million facts",
    "chunk_index": 326,
    "start_pos": 144833,
    "end_pos": 145284,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 434
    }
  },
  "419": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_327",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "graph with more than 50 million facts Jurassic-1 Wikipedia. OWT. Books. C4. Pile 301 arXiv. GitHub HyperCLOV Korean blogs. Community sites. News. KiN Korean Wikipedia. Wikipedia En- glish and Japanese Modu-Corpus. Mes- senger. News. Spoken and written lan- guage corpus. Web corpus Yuan 1.0 Common Crawl. SogouT. Sogou News. Baidu Baike. Wikipedia. Books Gopher subsets of MassiveWeb Books. C4. News. GitHub and Wikipedia samples from Mas- siveText ERNIE-3.0 TITAN Same as ERNIE 3.0 and ERNIE 3.0 ad-",
    "chunk_index": 327,
    "start_pos": 145285,
    "end_pos": 145780,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 500
    }
  },
  "420": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_328",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "RNIE-3.0 TITAN Same as ERNIE 3.0 and ERNIE 3.0 ad- versarial dataset. ERNIE 3.0 controllable dataset GPT-NeoX-20B Pile 301 OPT RoBERTa 299 Pile 301 PushShift.io Reddit 423 BLOOM ROOTs 13 Galactica arXiv. PMC. Semantic Scholar. Wikipedia. StackExchange. LibreText. Open Text- books. RefSeq Genome. OEIS. LIPID MAPS. NASAExoplanet. Common Crawl. ScientificCC. AcademicCC. GitHub repos- itories Khan Problems. GSM8K. OneS- mallStep GLaM Filtered Webpages. Social media conversa-",
    "chunk_index": 328,
    "start_pos": 145781,
    "end_pos": 146261,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 475
    }
  },
  "421": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_329",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "GLaM Filtered Webpages. Social media conversa- tions Wikipedia. Forums. Books. News LaMDA Infiniset Public documents. Dialogs. Ut- terances MT-NLG Two snapshots of Common Crawl and Books3. OpenWebText2. Stack Exchange. PubMed Abstracts. Wikipedia. PG-19 242 BookCorpus2. NIH ExPorter. Pile. CC-Stories. RealNews AlphaCode Selected GitHub repositories. CodeCon- tests. Codeforces. Description2Code. Co- deNet Chinchilla MassiveWeb. MassiveText Books. C4. News. GitHub. Wikipedia",
    "chunk_index": 329,
    "start_pos": 146262,
    "end_pos": 146725,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 477
    }
  },
  "422": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_330",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "eText Books. C4. News. GitHub. Wikipedia PaLM webpages. books. Wikipedia. news. arti- cles. source code. social media conversa- tions AlexaTM Wikipedia. mC4 U-PaLM Same as PaLM UL2 GLM-130B CodeGen Pile. BigQuery. BigPython LLaMA CommonCrawl. C4. Github. Wikipedia. Books. arXiv. StackExchange PanGu- WuDaoCorpora. CLUE. Pile. C4. Python code BloombergGPT inPile. Pile. C4. Wikipedia CodeT5 CodeSearchNet. Github Code",
    "chunk_index": 330,
    "start_pos": 146726,
    "end_pos": 147197,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 101,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 417
    }
  },
  "423": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_331",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "CodeT5 CodeSearchNet. Github Code StarCoder The Stack v1.2 LLaMA-2 PaLM-2 Web documents. Code. Books. Maths. Conversation 30",
    "chunk_index": 331,
    "start_pos": 147198,
    "end_pos": 147324,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 177,
      "word_count": 39,
      "optimized_for_embedding": true,
      "original_content_length": 177,
      "optimized_content_length": 124
    }
  },
  "424": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_332",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ode. Books. Maths. Conversation 30 --- Page 31 --- Table 11. An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. SNI is short of Super-NaturalInsturctions. Models Training DatasetBIG- benchMMLU BBH RAFT FLAN SNI PromptSource TyDiQA HumanEval MBPPTruthful Bias Toxicity T0 Pool of Prompts WebGPT ELI5 424 ELI5 fact- check 166 TriviaQA 341 ARC-Challenge 342 ARC- Easy 342 Hand-written data. Demonstrations of humans. Com- parisons between model-generated answers",
    "chunk_index": 332,
    "start_pos": 147326,
    "end_pos": 147813,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 496
    }
  },
  "425": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_333",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ns. Com- parisons between model-generated answers Tk-INSTRUCT SNI 18 mT0 xP3 154 OPT-IML PromptSource 17 FLAN 16 SNI 425 UnifiedSKG 426 CrossFit 427 ExMix 428 T5 10 Reasoning Flan Muffin. T0-SF. NIv2. CoT WizardCoder Code Alpaca reading comprehension task in which system must resolve references in text. often requiring world knowledge and rea- soning about the text. CSQA 358 The CommonsenseQA is question-answering dataset that requires commonsense knowledge to evaluate the",
    "chunk_index": 333,
    "start_pos": 147814,
    "end_pos": 148298,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 477
    }
  },
  "426": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_334",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "hat requires commonsense knowledge to evaluate the ability of AI models to understand and answer questions. 5.2.7. Reading Comprehension BoolQ 363 dataset derived from Google search queries. BoolQ challenges models to answer binary yes no questions. The questions are naturally occurring and are paired with paragraph from Wikipedia article containing the answer. It is test of reading comprehension and reasoning. SQUADv2 364 The Stanford Question Answering Dataset SQuAD 362 is collection of questions posed by crowd",
    "chunk_index": 334,
    "start_pos": 148299,
    "end_pos": 148789,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 518
    }
  },
  "427": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_335",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "362 is collection of questions posed by crowd workers on set of Wikipedia articles. where the answer to ev- ery question is segment of text from the corresponding reading passage. SQuADv2 combines the original SQuAD1.1 dataset with over 50.000 unanswerable questions. The aim is to evalu- ate model ability to understand and answer questions based on given context and to determine when question is unan- swerable. DROP 365 DROP. or Discrete Reasoning Over the con- tent of Paragraphs. is designed to test model ability to un-",
    "chunk_index": 335,
    "start_pos": 148790,
    "end_pos": 149289,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 526
    }
  },
  "428": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_336",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "aphs. is designed to test model ability to un- derstand wide variety of reading phenomena. It encourages comprehensive and reliable evaluation of reading comprehen- sion capabilities. RTE 366 The Recognizing Textual Entailment RTE datasets come from series of annual competitions on textual entailment. predicting whether given sentence logically fol- lows from another and evaluating model understanding of logical relationships in text. WebQA 367 dataset for open-domain question answering.",
    "chunk_index": 336,
    "start_pos": 149290,
    "end_pos": 149757,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 492
    }
  },
  "429": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_337",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "67 dataset for open-domain question answering. WebQA ffers large collection of web-based question-answer pairs. It is designed to assess the ability of AI models to under- stand and answer questions based on web content. CMRC2018 369 This dataset is test of Chinese language models ability to reason comprehensively and is designed with challenging span-extraction format that pushes the boundariesof machine performance. 5.2.8. Mathematical Reasoning MATH 382 This dataset is platform for evaluating the",
    "chunk_index": 337,
    "start_pos": 149758,
    "end_pos": 150232,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 504
    }
  },
  "430": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_338",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "82 This dataset is platform for evaluating the mathematical problem-solving abilities of AI models. It con- tains diverse set of math problems. ranging from arithmetic to calculus. and is designed to test the model ability to under- stand and solve complex mathematical problems. Math23k 383 This one challenges model ability to un- derstand and solve mathematical word problems. It contains 23.000 Chinese arithmetic word problems that require models to perform reasoning and computation based on the problem description.",
    "chunk_index": 338,
    "start_pos": 150233,
    "end_pos": 150719,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 522
    }
  },
  "431": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_339",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "and computation based on the problem description. GSM8K 384 dataset of diverse grade school math word problems. testing model ability to perform multi-step math- ematical reasoning. 5.2.9. Problem Solving and Logical Reasoning ANLI 393 large-scale dataset designed to test the robust- ness of machine learning models in Natural Language Inference NLI is created through an iterative. adversarial process where humans try to generate examples that models cannot correctly classify.",
    "chunk_index": 339,
    "start_pos": 150720,
    "end_pos": 151166,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 480
    }
  },
  "432": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_340",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "te examples that models cannot correctly classify. HumanEval 141 dataset for evaluating the problem- solving ability of AI models. which includes diverse set of tasks that require various cognitive abilities. making it com- prehensive tool for assessing general intelligence in AI. StrategyQA 349 question-answering dataset that re- quires reasoning over multiple pieces of evidence to evaluate the strategic reasoning ability of AI models. pushing the bound- aries of what machines can understand and answer.",
    "chunk_index": 340,
    "start_pos": 151167,
    "end_pos": 151639,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 509
    }
  },
  "433": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_341",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "aries of what machines can understand and answer. 5.2.10. Cross-Lingual Understanding XNLI 398 cross-lingual benchmark. XNLI extends the MultiNLI 429 corpus to 15 languages. including low-resource ones like Urdu. It tests models on cross-lingual sentence under- standing. with 112.500 annotated pairs across three categories. entailment. contradiction. and neutral. PAWS-X 399 PAWS-X. or Cross-lingual Paraphrase Adver- saries from Word Scrambling. is multilingual version of the 31",
    "chunk_index": 341,
    "start_pos": 151640,
    "end_pos": 152084,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 482
    }
  },
  "434": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_342",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rd Scrambling. is multilingual version of the 31 --- Page 32 --- PAWS 430 dataset for paraphrase identification. It includes examples in seven languages and is designed to evaluate the performance of cross-lingual paraphrase identification models. 5.2.11. Truthfulness Truthful-QA 405 unique benchmark that measures language model truthfulness when generating answers. The dataset includes questions across various categories like health. law. and politics. some designed to test the model against com- mon human misconceptions.",
    "chunk_index": 342,
    "start_pos": 152086,
    "end_pos": 152576,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 528
    }
  },
  "435": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_343",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "the model against com- mon human misconceptions. 5.2.12. Biases and Ethics in AI ETHOS 408 ETHOS is hate speech detection dataset built from YouTube and Reddit comments. It is tool in the fight against online hate speech. ffering binary and multi-label variants for robust content moderation. StereoSet 409 StereoSet is comprehensive dataset de- signed to measure and evaluate the presence of stereotypical biases in language models. It focuses on four key domains. gender. profession. race. and religion. Contrasting stereotypi-",
    "chunk_index": 343,
    "start_pos": 152577,
    "end_pos": 153071,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 529
    }
  },
  "436": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_344",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ssion. race. and religion. Contrasting stereotypi- cal bias against language modeling ability provides valuable tool for understanding and mitigating biases in large language models. 6. Applications Applying Large Language Models LLMs to variety of downstream tasks has become popular trend in both AI- related research communities and industries. with many emerg- ing uses being discovered and explored daily. LLMs. which are capable of understanding and generating human-like text. have",
    "chunk_index": 344,
    "start_pos": 153072,
    "end_pos": 153517,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 488
    }
  },
  "437": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_345",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "understanding and generating human-like text. have found meaningful applications across variety of fields. This section provides an overview of LLM applications in medicine. education. science. mathematics. law. finance. robotics. and coding. While each of these domains pose di fferent challenges. LLMs open up opportunities to make significant contributions to these domains through their generalizability. General Purpose. LLMs are being widely considered as general-purpose tools for wide variety of tasks 431 This",
    "chunk_index": 345,
    "start_pos": 153518,
    "end_pos": 153992,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 518
    }
  },
  "438": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_346",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "pose tools for wide variety of tasks 431 This is due to their inherent ability to understand. generate. and manipulate human-like text in contextually relevant man- ner. This allows them to perform tasks ranging from simple language translation and question-answering to more complex tasks like summarization. text generation. and even program- ming help 432 The utility of LLMs is further enhanced by their ability to adapt to the specific style and tone of the text they are processing. making the outputs more user-friendly and",
    "chunk_index": 346,
    "start_pos": 153993,
    "end_pos": 154482,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 530
    }
  },
  "439": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_347",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "cessing. making the outputs more user-friendly and context-aware. In everyday applications. LLMs can be used as personal assistants. helping users draft emails or schedule appointments 433 they can also be deployed in customer ser- vice to handle common questions or applied to generate content for digital platforms like websites by creating human-like text based on given prompts 434 Moreover. LLMs play cru- cial role in data analysis. where they can filter large volumes of text data. summarize key points. and find patterns that would",
    "chunk_index": 347,
    "start_pos": 154483,
    "end_pos": 154979,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 539
    }
  },
  "440": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_348",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "summarize key points. and find patterns that would take humans much longer to identify 435 Despite their wide- ranging applications. it is essential to remember that LLMs.similar to any AI system. are only as good as the data they have been trained on. Medicine. The application of LLMs in the field of medicine is reshaping healthcare delivery and research. For example. LLMs are increasingly used in clinical decision support systems to provide physicians with evidence-based treatment recommen-",
    "chunk_index": 348,
    "start_pos": 154980,
    "end_pos": 155429,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 497
    }
  },
  "441": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_349",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "physicians with evidence-based treatment recommen- dations 436. 437. 438 By analyzing patient data and medical literature. they can help identify potential diagnoses. suggest appropriate tests. and recommend optimal treatment strategies. Moreover. LLMs can also enhance patient interactions with healthcare systems. e.g. they can be used in chatbot applica- tions 439. 440. 441 to answer patient queries about symptoms or medications. schedule appointments. and even provide es- sential health advice. For medical research. LLMs are used to",
    "chunk_index": 349,
    "start_pos": 155430,
    "end_pos": 155925,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 540
    }
  },
  "442": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_350",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "lth advice. For medical research. LLMs are used to extract and filter information from considerable amount of medical literature. identify relevant studies. summarize find- ings. and even predict future research trends 442. 443. 444 For medical education. LLMs can help create training mate- rials. generate exam questions. provide detailed explanations of complex medical topics. and ffer personalized feedback to students 445. 446. 447. 448 They can also simulate patient interactions. enabling students to practice and improve their",
    "chunk_index": 350,
    "start_pos": 155926,
    "end_pos": 156420,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 535
    }
  },
  "443": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_351",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "s. enabling students to practice and improve their clinical skills. At broader level. LLMs can assist in public health initiatives by analyzing media data to detect disease out- breaks. monitor public sentiment towards health policies. and disseminate health information in clear and understandable manner 449 LLMs can be employed to support public health initiatives. addressing related issues such as data privacy. the necessity for explainability. and the potential risk of propagat- ing biases 450. 451",
    "chunk_index": 351,
    "start_pos": 156421,
    "end_pos": 156886,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 506
    }
  },
  "444": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_352",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "potential risk of propagat- ing biases 450. 451 Education. The integration of LLMs into the educational sec- tor offers opportunities to enhance learning experiences. teacher support. and educational content development. For students. by analyzing their learning styles. performance. and preferences. LLMs can provide customized study materials and practice questions to develop personalized learning experiences 452 For teachers. LLMs can help to create lesson plans and grade assignments and generate diverse and inclusive educational",
    "chunk_index": 352,
    "start_pos": 156887,
    "end_pos": 157378,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 536
    }
  },
  "445": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_353",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nts and generate diverse and inclusive educational content. significantly saving more time for teaching and student interaction 453. 454 In language learning. LLMs serve as advanced conversational partners capable of simulating conver- sations in multiple languages. correcting grammar. enhancing vocabulary. and aiding pronunciation for the needs of fluency in practice 455 Furthermore. LLMs improve accessibility in education by providing support for students with disabili- ties. They can generate real-time transcriptions for the hear-",
    "chunk_index": 353,
    "start_pos": 157379,
    "end_pos": 157873,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 539
    }
  },
  "446": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_354",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "an generate real-time transcriptions for the hear- ing impaired. ffer reading assistance for the visually impaired. and simplify complex texts for those with learning disabili- ties 451 As LLMs continue to evolve. their applications in education can benefit more students and teachers from di fferent perspectives in practice. Science. Similar to medical applications. LLMs can expedite the research process by quickly analyzing and summarizing sci- entific literature. By briefing comprehensible and accessible re-",
    "chunk_index": 354,
    "start_pos": 157874,
    "end_pos": 158343,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 515
    }
  },
  "447": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_355",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ure. By briefing comprehensible and accessible re- search summaries. LLMs can assist researchers in staying up- to-date with the latest findings. even in fields outside their area of expertise 456. 457 In addition. LLMs can aid scientists 32",
    "chunk_index": 355,
    "start_pos": 158344,
    "end_pos": 158537,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 244,
      "word_count": 38,
      "optimized_for_embedding": true,
      "original_content_length": 244,
      "optimized_content_length": 241
    }
  },
  "448": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_356",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "456. 457 In addition. LLMs can aid scientists 32 --- Page 33 --- Table 12. Performance comparison of top performing LLMs across various NLU and NLG tasks. Here. N-Shots indicate the number of example prompts provided to the model during the evaluation. representing its capability in few-shot or zero-shot learning settings. represents the fine-tuned version. and represents the benchmark. Task Dataset BenchmarkTop-1 Top-2 Top-3 Model Size Score N-shots Model Size Score N-shots Model Size Score N-shots",
    "chunk_index": 356,
    "start_pos": 158539,
    "end_pos": 159017,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 504
    }
  },
  "449": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_357",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Size Score N-shots Model Size Score N-shots Multi-TaskBIG-bench Chinchilla 70B 65.1 5-shot Gopher 280B 53.97 5-shot PaLM 540B 53.7 5-shot MMLU GPT-4 86.4 5-shot Gemini Ultra 83.7 5-shot Flan-PaLM-2 Large 81.2 5-shot Language Understanding SuperGLUE ERNIE 3.0 12B 90.6 PaLM 540B 90.4 T5 11B 88.9 Story Comprehension and GenerationHellaSwag GPT-4 95.3 10-shot Gemini Ultra 87.8 10-shot PaLM-2 Large 86.8 one shot",
    "chunk_index": 357,
    "start_pos": 159018,
    "end_pos": 159460,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 410
    }
  },
  "450": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_358",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tra 87.8 10-shot PaLM-2 Large 86.8 one shot StoryCloze GPT3 175B 87.7 few shot PaLM-2 Large 87.4 one shot OPT 175B 79.82 Physical Knowledge and World UnderstandingPIQA PaLM-2 Large 85.0 one shot LLaMa 65B 82.8 zero shot MT-NLG 530B 81.99 zero shot TriviaQA PaLM-2 Large 86.1 one shot LLaMA-2 70B 85.0 one shot PaLM 540B 81.4 one shot Contextual Language UnderstandingLAMBADA PaLM 540B 89.7 few shot MT-NLG 530B 87.15 few shot PaLM-2 Large 86.9 one shot",
    "chunk_index": 358,
    "start_pos": 159461,
    "end_pos": 159919,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 452
    }
  },
  "451": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_359",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "87.15 few shot PaLM-2 Large 86.9 one shot Commonsense ReasoningWinoGrande GPT-4 87.5 5-shot PaLM-2 Large 83.0 one shot PaLM 540B 81.1 zero shot SIQA LLaMA 65B 52.3 zero shot Chinchilla 70B 51.3 zero shot Gopher 280B 50.6 zero shot Reading Comprehension BoolQ PaLM 540B 92.2 T5 11B 91.2 PaLM-2 Large 90.9 one shot Truthfulness Truthful-QA LLaMA 65B 57 Mathematical ReasoningMATH Gemini Ultra 53.2 4-shot PaLM-2 Large 34.3 4-shot LLaMa-2 65B 13.5 4-shot",
    "chunk_index": 359,
    "start_pos": 159920,
    "end_pos": 160392,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 451
    }
  },
  "452": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_360",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Large 34.3 4-shot LLaMa-2 65B 13.5 4-shot GSM8K GPT-4 92.0 5-shot PaLM-2 Large 80.7 8-shot U-PaLM 540B 58.5 Problem Solving and Logical ReasoningHumanEval Gemini Ultra 74.4 zero shot GPT-4 67.0 zero shot Code Llama 34B 48.8 zero shot in formulating new hypotheses and research questions since their ability to process large-scale datasets allows them to un- veil insights that might not be immediately apparent to human researchers 458 Moreover. for scientific writing. LLMs can",
    "chunk_index": 360,
    "start_pos": 160393,
    "end_pos": 160865,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 478
    }
  },
  "453": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_361",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "458 Moreover. for scientific writing. LLMs can help researchers draft documents. suggest improvements. and ensure adherence to specific formatting guidelines 459. 460 This not only saves time but also improves the clarity of scien- tific communication. enabling interdisciplinary teams to work together more ffectively. Maths. In addition to providing mathematical research and education support. LLMs can assist in solving mathematical problems by giving step-by-step explanations and guiding users",
    "chunk_index": 361,
    "start_pos": 160866,
    "end_pos": 161323,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 499
    }
  },
  "454": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_362",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "giving step-by-step explanations and guiding users through complex proofs and calculations. They can help iden- tify errors in reasoning or computation and suggest corrections. serving as an invaluable tool for both learning and verification purposes 461. 462 LLMs can be employed to check the valid- ity of mathematical proofs. ffering preliminary filter before human review. While they are not substitute for the meticu- lous work of mathematicians. they can help simplify the process",
    "chunk_index": 362,
    "start_pos": 161324,
    "end_pos": 161768,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 486
    }
  },
  "455": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_363",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "mathematicians. they can help simplify the process of proof verification 463. 464 Moreover. LLMs enhance ac- cessibility to mathematics by translating complex concepts and findings into understandable language for non-specialists 465 where the gap between theoretical mathematics and applied contexts such as physics. engineering. and economics can be bridged. Law. LLMs can assist with the thematic analysis of legal doc- uments. including generating initial coding for datasets. iden-",
    "chunk_index": 363,
    "start_pos": 161769,
    "end_pos": 162210,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 486
    }
  },
  "456": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_364",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ding generating initial coding for datasets. iden- tifying themes. and classifying data according to these themes. This collaborative ffort between legal experts and LLMs has proved to be ffective in analyzing legal texts such as court opinions on theft. improving both the fficiency and quality of the research 466 Additionally. LLMs have been evaluated for their ability to generate explanations of legal terms. focusing on improving factual accuracy and relevance by incorporating",
    "chunk_index": 364,
    "start_pos": 162211,
    "end_pos": 162652,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 483
    }
  },
  "457": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_365",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng factual accuracy and relevance by incorporating sentences from case law. By feeding relevant case law into the LLM. the augmented models can generate higher-quality expla- nations with less factually incorrect information 467 More- over. LLMs can be trained with specialized domain knowledgeto perform legal reasoning tasks 468 and answer legal ques- tions 469 Finance. LLMs like BloombergGPT 151 trained on exten- sive proprietary financial datasets. exhibit superior performance",
    "chunk_index": 365,
    "start_pos": 162653,
    "end_pos": 163096,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 483
    }
  },
  "458": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_366",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "financial datasets. exhibit superior performance on financial tasks. This indicates the value of domain-specific training in creating LLMs that can more accurately understand and process industry-specific language and concepts. The intro- duction of FinGPT 470 as an open-source model ffers trans- parent and accessible resources to develop novel applications such as robo-advising. algorithmic trading. and low-code so- lutions. ultimately expanding the capabilities of financial ser- vices. Both BloombergGPT and FinGPT show the adaptabil-",
    "chunk_index": 366,
    "start_pos": 163097,
    "end_pos": 163593,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 541
    }
  },
  "459": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_367",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Both BloombergGPT and FinGPT show the adaptabil- ity of LLMs to the financial domain. with the former showing the power of custom datasets and the latter emphasizing data- centric approach and low-rank adaptation techniques for cus- tomization. Moreover. LLMs demonstrate an ability to break down complex financial tasks into actionable plans. enabling end-to-end solutions that were previously unfeasible with sin- gle model 471 Robotics. In robotics research. LLMs have promising appli-",
    "chunk_index": 367,
    "start_pos": 163594,
    "end_pos": 164040,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 488
    }
  },
  "460": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_368",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "In robotics research. LLMs have promising appli- cations. such as enhancing human-robot interaction 28. 472. 473. 474 task planning 237 motion planning 246 nav- igation 246. 475 object manipulation 236 personalized robots 476 etc. LLMs enable robots to understand the en- vironment ffectively and generate plans to complete tasks col- laboratively 240. 26 They can facilitate continuous learning by allowing robots to access and integrate information from wide range of sources. helping robots acquire new skills. adapt",
    "chunk_index": 368,
    "start_pos": 164041,
    "end_pos": 164536,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 519
    }
  },
  "461": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_369",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "sources. helping robots acquire new skills. adapt to changes. and refine their paths 224. 233. 234 7. Challenges and Future Directions LLMs such as GPT-4 and its predecessors have significantly advanced natural language processing. Nevertheless. they also bring along set of challenges. The computational cost. ad- versarial robustness. and interpretability are among the tech- nical challenges that are intrinsic to these models. Further- more. as these models are scaled up to handle more complex 33",
    "chunk_index": 369,
    "start_pos": 164537,
    "end_pos": 164993,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 501
    }
  },
  "462": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_370",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ese models are scaled up to handle more complex 33 --- Page 34 --- tasks or to operate in more complex or dynamic environments. new challenges in scalability. privacy. and real-time processing emerge. On the frontier of foundational research. integrating multi-modality and the ffectiveness of transfer learning are be- ing keenly explored. Additionally. the continuous learning as- pect of these models. which aims to have models that can adapt to new information over time. presents fresh set of challenges.",
    "chunk_index": 370,
    "start_pos": 164995,
    "end_pos": 165457,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 509
    }
  },
  "463": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_371",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ion over time. presents fresh set of challenges. These challenges not only underscore the technical intricacies involved but also highlight the broader impact and the future trajectory of LLMs in real-world applications. The following sections delve into these challenges. shedding light on the on- going and potential fforts to address them. Computational Cost. Training LLMs require extensive compu- tational resources. which increases production costs and raises environmental concerns due to substantial energy consump-",
    "chunk_index": 371,
    "start_pos": 165458,
    "end_pos": 165934,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 523
    }
  },
  "464": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_372",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "mental concerns due to substantial energy consump- tion during large-scale training. Improved performance occurs as computational resources increase. but the rate of improve- ment gradually decreases when both the model and dataset size remain fixed. following the power law of diminishing re- turns 477 Bias and Fairness. LLMs can inherit and amplify societal bi- ases in their training data. These biases can manifest in the model outputs. leading to potential ethical and fairness is- sues 478",
    "chunk_index": 372,
    "start_pos": 165935,
    "end_pos": 166388,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 496
    }
  },
  "465": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_373",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "to potential ethical and fairness is- sues 478 Overfitting. Although LLMs possess substantial learning ca- pabilities. they are susceptible to overfitting noisy and peculiar patterns within their extensive training data. Consequently. this may cause them to generate illogical responses 479 The de- bate about Memorization vs. Generalization in LLMs is about finding the right balance. Memorization allows the model to remember specific details from its training data. ensuring it can",
    "chunk_index": 373,
    "start_pos": 166389,
    "end_pos": 166829,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 484
    }
  },
  "466": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_374",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ic details from its training data. ensuring it can provide accurate answers to precise questions. However. gen- eralization enables the model to make inferences and produce responses for inputs it has not seen before. which is essential for handling various real-world tasks. Striking the right bal- ance is the challenge. too much memorization can lead to over- fitting. making the model inflexible and struggling with new inputs 480 Economic and Research Inequality. The high cost of train-",
    "chunk_index": 374,
    "start_pos": 166830,
    "end_pos": 167274,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 492
    }
  },
  "467": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_375",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "and Research Inequality. The high cost of train- ing and deploying LLMs may make their development concen- trated within well-funded organizations. potentially worsening economic and research inequalities in AI 481 Reasoning and Planning. Some reasoning and planning tasks. even as seemingly simple as common-sense planning. which humans find easy. remain well beyond the current capabilities of LLMs evaluated using an assessment framework. This is not entirely unexpected. considering that LLMs primarily generate",
    "chunk_index": 375,
    "start_pos": 167275,
    "end_pos": 167744,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 515
    }
  },
  "468": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_376",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "expected. considering that LLMs primarily generate text completions based on likelihood and ffer no solid guaran- tees in terms of reasoning abilities 482 Hallucinations. LLMs exhibit hallucinations where they generate responses that. while sounding plausible. are incorrect or do not align with the provided information 483 Hallucina- tions can be categorized into three categories. Input-conflicting hallucination. wherein LLMs produce content that diverges from the input given by users.",
    "chunk_index": 376,
    "start_pos": 167745,
    "end_pos": 168196,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 490
    }
  },
  "469": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_377",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ntent that diverges from the input given by users. Context-conflicting hallucination. where LLMs generatecontent that contradicts information they have generated earlier. Fact-conflicting hallucination involves LLM generation of content that does not align with established world knowledge. Prompt Engineering. Prompts serve as inputs to LLMs. and their syntax and semantics play crucial role in determining the model output. The prompt variations. sometimes counter- intuitive to humans. can result in significant changes in model",
    "chunk_index": 377,
    "start_pos": 168197,
    "end_pos": 168685,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 531
    }
  },
  "470": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_378",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "humans. can result in significant changes in model output and are addressed through prompt engineering. which involves designing natural language queries to guide LLMs responses ffectively 484. 32 Limited Knowledge. Information acquired during pretraining is limited and may become obsolete after some time. Re- training the model using updated data is costly. To generate factually accurate responses. people use retrieval augmen- tation pipeline 198 However. pre-trained models are not",
    "chunk_index": 378,
    "start_pos": 168686,
    "end_pos": 169132,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 487
    }
  },
  "471": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_379",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ipeline 198 However. pre-trained models are not trained with retrieval augmentation generation RAG 6. 21 hence. adapting the training pipeline is necessary 193. 25 Safety and Controllability. Using LLMs comes with the risk of generating harmful. misleading. or inappropriate content. whether by accident or when given specific prompts. Ensuring these models are safely utilized is significant concern 485 Security and Privacy. LLMs are prone to leaking personal information and generating false. unethical. misaligned re-",
    "chunk_index": 379,
    "start_pos": 169133,
    "end_pos": 169619,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 521
    }
  },
  "472": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_380",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "on and generating false. unethical. misaligned re- sponses. Researchers have explored various security attacks. i.e. backdoor attacks. jailbreaking. prompt injection. and data poisoning. that lead to breaking LLMs security. Therefore. developing better defense mechanisms is essential to ensure LLMs are safe. reliable. and trustworthy for complex AI applications 486 Multi-Modality. Multi-modal learning. where LLMs are trained on diverse data like text. images. and videos. aims to create models with richer understanding but faces challenges",
    "chunk_index": 380,
    "start_pos": 169620,
    "end_pos": 170117,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 544
    }
  },
  "473": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_381",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "els with richer understanding but faces challenges in data alignment. fusion strategies. and higher computational demands. Catastrophic Forgetting. LLMs are often pre-trained on large datasets and then fine-tuned on domain-specific data. reducing training resources. However. they face issues like domain adaptation and catastrophic forgetting. which hinder the retention of original knowledge when learning new tasks. Adversarial Robustness. Large Language Models LLMs have shown great capabilities in various tasks but are vul-",
    "chunk_index": 381,
    "start_pos": 170118,
    "end_pos": 170598,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 529
    }
  },
  "474": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_382",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "great capabilities in various tasks but are vul- nerable to adversarial attacks. where slight. deliberate input alterations can mislead them. Especially with models like BERT. adversarial fine-tuning can enhance robustness. al- though it sometimes compromises generalization 487 As LLMs integrate more into complex systems. examining their security properties becomes crucial. given the emerging field of adversarial attacks on LLMs within trustworthy ML 488 This vulnerability is notable in safety-critical domains. ne-",
    "chunk_index": 382,
    "start_pos": 170599,
    "end_pos": 171076,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 520
    }
  },
  "475": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_383",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ability is notable in safety-critical domains. ne- cessitating robust adversarial evaluation tools to ensure LLM reliability 489 Interpretability and Explainability. The black-box nature of LLMs poses challenges in understanding their decision- making. which is crucial for broader acceptance and trust. 34",
    "chunk_index": 383,
    "start_pos": 171077,
    "end_pos": 171337,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 311,
      "word_count": 41,
      "optimized_for_embedding": true,
      "original_content_length": 311,
      "optimized_content_length": 306
    }
  },
  "476": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_384",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ch is crucial for broader acceptance and trust. 34 --- Page 35 --- especially in sensitive domains. Despite their advanced capabilities. the lack of insight into their operation limits their effectiveness and trustworthiness 490. 491 fforts are being made to make LLMs more explainable to promote user trust and to ensure responsible AI usage. Understanding the logic behind LLMs responses is essential for fostering trust and ensuring they align with human values and legal standards. Privacy Concerns. Privacy concerns in Large Language",
    "chunk_index": 384,
    "start_pos": 171339,
    "end_pos": 171832,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 538
    }
  },
  "477": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_385",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ivacy Concerns. Privacy concerns in Large Language Models LLMs have escalated with their growth in complexity and size. particularly around data sharing and potential misuse. There is risk of malicious content creation. filter bypass. and data privacy issues. especially in e-commerce. where protecting customer privacy is crucial. If models are trained on private data. additional concerns arise if such models are made publicly available. LLMs tend to memorize phrases from their training sets. which an adversary could exploit to extract",
    "chunk_index": 385,
    "start_pos": 171833,
    "end_pos": 172326,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 540
    }
  },
  "478": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_386",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "sets. which an adversary could exploit to extract sensitive data. posing threat to personal privacy 492. 493 Real-Time Processing. Real-time processing in Large Lan- guage Models LLMs is pivotal for various applications. especially with the rising popularity of mobile AI applications and concerns regarding information security and privacy. However. LLMs often have hundreds of layers and millions of parameters. which impede real-time processing due to the high computational demands and limited weight storage on",
    "chunk_index": 386,
    "start_pos": 172327,
    "end_pos": 172799,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 515
    }
  },
  "479": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_387",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "omputational demands and limited weight storage on hardware platforms. particularly in edge computing environ- ments 494 While certain fforts like MobileBERT aim to reduce memory requirements. they still face substantial execution overhead due to the large number of model layers. leading to high inference latency. Long-Term Dependencies. Large Language Models have shown considerable progress in understanding and generating text. yet they often struggle with preserving context and handling long-term dependencies. particularly in complex.",
    "chunk_index": 387,
    "start_pos": 172800,
    "end_pos": 173296,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 542
    }
  },
  "480": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_388",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "long-term dependencies. particularly in complex. multi-turn conversations or long documents. This limitation can lead to incoherent or irrelevant responses. Hardware Acceleration. The growth of LLMs presents signif- icant hardware challenges due to the increasing computational and memory demands associated with training and deploying these models. GPUs have played crucial role in meeting the hardware requirements for training LLMs. with the networking industry also evolving to optimize hardware for training",
    "chunk_index": 388,
    "start_pos": 173297,
    "end_pos": 173762,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 512
    }
  },
  "481": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_389",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ry also evolving to optimize hardware for training workloads. However. the growing size of LLMs. which has been outpacing hardware progress. makes model inference in- creasingly costly. Model quantization is promising approach to bridge the widening gap between LLM size and hardware capacity 495 Although specialized hardware acceleration like GPUs or TPUs can significantly reduce the computational cost. making real-time applications more feasible. they may not fully resolve all limitations. necessitating further advancements",
    "chunk_index": 389,
    "start_pos": 173763,
    "end_pos": 174247,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 530
    }
  },
  "482": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_390",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ll limitations. necessitating further advancements in hardware technology. Regulatory and Ethical Frameworks. The rapid advancements in artificial intelligence have given rise to sophisticated Large Language Models LLMs like OpenAI GPT-4 157 and Google Bard. These developments underscore the imperative for regulatory oversight to manage the ethical and social challenges accompanying LLMs widespread use 496 For instance. LLMs can generate content that can be used posi-tively or negatively. emphasizing the need for proactive ethical",
    "chunk_index": 390,
    "start_pos": 174248,
    "end_pos": 174745,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 536
    }
  },
  "483": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_391",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tively. emphasizing the need for proactive ethical frameworks and policy measures to guide their responsible use and assign accountability for their outputs 497 Auditing is identified as promising governance mechanism to ensure that AI systems. including LLMs. are designed and deployed ethically. legally. and technically robust 498 8. Conclusion This article has comprehensively reviewed the develop- ments in LLMs. It contributes to summarizing significant findings of LLMs in the existing literature and provides",
    "chunk_index": 391,
    "start_pos": 174746,
    "end_pos": 175221,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 516
    }
  },
  "484": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_392",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "of LLMs in the existing literature and provides detailed analysis of the design aspects. including architec- tures. datasets. and training pipelines. We identified crucial architectural components and training strategies employed by different LLMs. These aspects are presented as summaries and discussions throughout the article. Moreover. we have discussed the performance di fferences of LLMs in zero-shot and few-shot settings. explored the impact of fine-tuning. and compared supervised and generalized models and encoder vs.",
    "chunk_index": 392,
    "start_pos": 175222,
    "end_pos": 175703,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 529
    }
  },
  "485": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_393",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "supervised and generalized models and encoder vs. decoder vs. encoder-decoder architectures. comprehensive review of multi-modal LLMs. retrieval augmented LLMs. LLMs-powered agents. fficient LLMs. datasets. evaluation. applications. and challenges is also provided. This article is anticipated to serve as valuable resource for researchers. offering insights into the recent advancements in LLMs and providing fundamental concepts and details to develop better LLMs. Acknowledgement. The author would like to acknowl-",
    "chunk_index": 393,
    "start_pos": 175704,
    "end_pos": 176180,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 517
    }
  },
  "486": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_394",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nowledgement. The author would like to acknowl- edge the support received from Saudi Data and AI Authority SDAIA and King Fahd University of Petroleum and Miner- als KFUPM under SDAIA-KFUPM Joint Research Center for Artificial Intelligence Grant No. JRC-AI-RFP-11. References A. Chernyavskiy. D. Ilvovsky. P. Nakov. Transformers. the end of his- tory for natural language processing. in. Machine Learning and Knowledge Discovery in Databases. Research Track. European Con- ference. ECML PKDD 2021. Bilbao. Spain. September 13 17. 2021.",
    "chunk_index": 394,
    "start_pos": 176181,
    "end_pos": 176678,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 535
    }
  },
  "487": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_395",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "PKDD 2021. Bilbao. Spain. September 13 17. 2021. Proceedings. Part III 21. Springer. 2021. pp. 677 693. A. Wang. Pruksachatkun. N. Nangia. A. Singh. J. Michael. F. Hill. O. Levy. S. Bowman. Superglue. stickier benchmark for general- purpose language understanding systems. Advances in neural informa- tion processing systems 32 2019 1. 26. 29 D. Adiwardana. M.-T. Luong. D. R. So. J. Hall. N. Fiedel. R. Thoppilan. Z. Yang. A. Kulshreshtha. G. Nemade. Lu. et al. Towards human-",
    "chunk_index": 395,
    "start_pos": 176679,
    "end_pos": 177133,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 477
    }
  },
  "488": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_396",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "shtha. G. Nemade. Lu. et al. Towards human- like open-domain chatbot. arXiv preprint arXiv.2001.09977 2020 B. A. Arcas. Do large language models understand us. Daedalus 151 2022 183 197. A. Radford. J. Wu. R. Child. D. Luan. D. Amodei. I. Sutskever. et al. Language models are unsupervised multitask learners. OpenAI blog 2019 9. 2. T. Brown. B. Mann. N. Ryder. M. Subbiah. J. D. Kaplan. P. Dhariwal. A. Neelakantan. P. Shyam. G. Sastry. A. Askell. et al. Language models",
    "chunk_index": 396,
    "start_pos": 177134,
    "end_pos": 177601,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 471
    }
  },
  "489": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_397",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "yam. G. Sastry. A. Askell. et al. Language models are few-shot learners. Advances in neural information processing sys- tems 33 2020 1877 1901. 2. 6. 7. 8. 9. 16. 18. 23. 24. 25. 34 J. Devlin. M.-W. Chang. K. Lee. K. Toutanova. Bert. Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv.1810.04805 2018 2. 18. 24 35",
    "chunk_index": 397,
    "start_pos": 177602,
    "end_pos": 177920,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 369,
      "word_count": 59,
      "optimized_for_embedding": true,
      "original_content_length": 369,
      "optimized_content_length": 359
    }
  },
  "490": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_398",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Xiv preprint arXiv.1810.04805 2018 2. 18. 24 35 --- Page 36 --- M. E. Peters. M. Neumann. M. Iyyer. M. Gardner. C. Clark. K. Lee. L. Zettlemoyer. Deep contextualized word representations. in. NAACL- HLT. Association for Computational Linguistics. 2018. pp. 2227 2237. M. Lewis. Liu. N. Goyal. M. Ghazvininejad. A. Mohamed. O. Levy. Stoyanov. L. Zettlemoyer. Bart. Denoising sequence-to-sequence pre- training for natural language generation. translation. and comprehen- sion. arXiv preprint arXiv.1910.13461 2019",
    "chunk_index": 398,
    "start_pos": 177922,
    "end_pos": 178409,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 512
    }
  },
  "491": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_399",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "n- sion. arXiv preprint arXiv.1910.13461 2019 10 C. Ra ffel. N. Shazeer. A. Roberts. K. Lee. S. Narang. M. Matena. Zhou. W. Li. P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Re- search 21 2020 5485 5551. 2. 7. 8. 18. 19. 24. 25. 28. 30. 31 11 L. Xue. N. Constant. A. Roberts. M. Kale. R. Al-Rfou. A. Siddhant. A. Barua. C. Ra ffel. mt5. massively multilingual pre-trained text-to-",
    "chunk_index": 399,
    "start_pos": 178410,
    "end_pos": 178840,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 481,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 458
    }
  },
  "492": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_400",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "mt5. massively multilingual pre-trained text-to- text transformer. arXiv preprint arXiv.2010.11934 2020 2. 7. 8. 24. 25. 28. 30 12 Z. Zhang. Gu. X. Han. S. Chen. C. Xiao. Z. Sun. Yao. F. Qi. J. Guan. P. Ke. et al. Cpm-2. Large-scale cost-e ffective pre-trained lan- guage models. AI Open 2021 216 224. 2. 8. 25 13 T. L. Scao. A. Fan. C. Akiki. E. Pavlick. S. Ili c. D. Hesslow. R. Castagne A. S. Luccioni. F. Yvon. M. Galle et al. Bloom. 176b- parameter open-access multilingual language model. arXiv preprint",
    "chunk_index": 400,
    "start_pos": 178841,
    "end_pos": 179327,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 96,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 509
    }
  },
  "493": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_401",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "access multilingual language model. arXiv preprint arXiv.2211.05100 2022 2. 4. 9. 11. 23. 24. 25. 30 14 S. Zhang. S. Roller. N. Goyal. M. Artetxe. M. Chen. S. Chen. C. Dewan. M. Diab. X. Li. X. Lin. et al. Opt. Open pre-trained transformer language models. arXiv preprint arXiv.2205.01068 2022 2. 9. 11. 24. 25 15 A. Chowdhery. S. Narang. J. Devlin. M. Bosma. G. Mishra. A. Roberts. P. Barham. H. W. Chung. C. Sutton. S. Gehrmann. et al. Palm. Scal- ing language modeling with pathways. arXiv preprint arXiv.2204.02311",
    "chunk_index": 401,
    "start_pos": 179328,
    "end_pos": 179811,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 518
    }
  },
  "494": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_402",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ing with pathways. arXiv preprint arXiv.2204.02311 2022 2. 6. 9. 11. 23. 24. 25 16 H. W. Chung. L. Hou. S. Longpre. B. Zoph. Tay. W. Fedus. E. Li. X. Wang. M. Dehghani. S. Brahma. et al. Scaling instruction-finetuned language models. arXiv preprint arXiv.2210.11416 2022 2. 7. 11. 16. 17. 22. 24. 25. 28. 31 17 Sanh. A. Webson. C. Ra ffel. S. H. Bach. L. Sutawika. Z. Alyafeai. A. Cha ffin. A. Stiegler. T. L. Scao. A. Raja. et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint",
    "chunk_index": 402,
    "start_pos": 179812,
    "end_pos": 180294,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 513
    }
  },
  "495": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_403",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "bles zero-shot task generalization. arXiv preprint arXiv.2110.08207 2021 2. 11. 16. 25. 28. 31 18 Wang. S. Mishra. P. Alipoormolabashi. Kordi. A. Mirzaei. A. Naik. A. Ashok. A. S. Dhanasekaran. A. Arunkumar. D. Stap. et al. Super-naturalinstructions. Generalization via declarative instructions on 1600 nlp tasks. in. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022. pp. 5085 5109. 2. 7. 11. 16. 17. 24. 25. 28. 31 19 Wang. Kordi. S. Mishra. A. Liu. N. A. Smith. D. Khashabi. H. Ha-",
    "chunk_index": 403,
    "start_pos": 180295,
    "end_pos": 180795,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 527
    }
  },
  "496": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_404",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Mishra. A. Liu. N. A. Smith. D. Khashabi. H. Ha- jishirzi. Self-instruct. Aligning language model with self generated in- structions. arXiv preprint arXiv.2212.10560 2022 2. 16. 19. 22. 28 20 L. Ouyang. J. Wu. X. Jiang. D. Almeida. C. Wainwright. P. Mishkin. C. Zhang. S. Agarwal. K. Slama. A. Ray. et al. Training language mod- els to follow instructions with human feedback. Advances in Neural In- formation Processing Systems 35 2022 27730 27744. 2. 7. 11. 16. 22 21 H. Touvron. L. Martin. K. Stone. P. Albert. A. Almahairi. Babaei.",
    "chunk_index": 404,
    "start_pos": 180796,
    "end_pos": 181296,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 535
    }
  },
  "497": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_405",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "in. K. Stone. P. Albert. A. Almahairi. Babaei. N. Bashlykov. S. Batra. P. Bhargava. S. Bhosale. et al. Llama 2. Open foundation and fine-tuned chat models. arXiv preprint arXiv.2307.09288 2023 2. 7. 10. 16. 25. 34 22 J. Wei. Tay. R. Bommasani. C. Ra ffel. B. Zoph. S. Borgeaud. D. Yo- gatama. M. Bosma. D. Zhou. D. Metzler. et al. Emergent abilities of large language models. arXiv preprint arXiv.2206.07682 2022 23 T. Webb. K. J. Holyoak. H. Lu. Emergent analogical reasoning in large",
    "chunk_index": 405,
    "start_pos": 181297,
    "end_pos": 181753,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 485
    }
  },
  "498": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_406",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "oak. H. Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour 2023 1526 1541. 24 D. A. Boiko. R. MacKnight. G. Gomes. Emergent autonomous sci- entific research capabilities of large language models. arXiv preprint arXiv.2304.05332 2023 25 G. Izacard. P. Lewis. M. Lomeli. L. Hosseini. F. Petroni. T. Schick. J. Dwivedi-Yu. A. Joulin. S. Riedel. E. Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv.2208.03299 2022 2. 18. 19. 34",
    "chunk_index": 406,
    "start_pos": 181754,
    "end_pos": 182219,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 494
    }
  },
  "499": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_407",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "iv preprint arXiv.2208.03299 2022 2. 18. 19. 34 26 D. Driess. F. Xia. M. S. Sajjadi. C. Lynch. A. Chowdhery. B. Ichter. A. Wahid. J. Tompson. Q. Vuong. T. Yu. et al. Palm-e. An embodiedmultimodal language model. arXiv preprint arXiv.2303.03378 2023 2. 20. 22. 33 27 A. Parisi. Zhao. N. Fiedel. Talm. Tool augmented language models. arXiv preprint arXiv.2205.12255 2022 2. 19. 20 28 B. Zhang. H. Soh. Large language models as zero-shot human models for human-robot interaction. arXiv preprint arXiv.2303.03548 2023 2. 33",
    "chunk_index": 407,
    "start_pos": 182220,
    "end_pos": 182711,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 519
    }
  },
  "500": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_408",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ion. arXiv preprint arXiv.2303.03548 2023 2. 33 29 Q. Ye. H. Xu. G. Xu. J. Ye. M. Yan. Zhou. J. Wang. A. Hu. P. Shi. Shi. et al. mplug-owl. Modularization empowers large language models with multimodality. arXiv preprint arXiv.2304.14178 2023 2. 22 30 W. Wang. Z. Chen. X. Chen. J. Wu. X. Zhu. G. Zeng. P. Luo. T. Lu. J. Zhou. Qiao. et al. Visionllm. Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv.2305.11175 2023 2. 22",
    "chunk_index": 408,
    "start_pos": 182712,
    "end_pos": 183158,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 470
    }
  },
  "501": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_409",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "sks. arXiv preprint arXiv.2305.11175 2023 2. 22 31 R. Yang. L. Song. Li. S. Zhao. Ge. X. Li. Shan. Gpt4tools. Teaching large language model to use tools via self-instruction. arXiv preprint arXiv.2305.18752 2023 2. 19. 22. 23 32 E. Saravia. Prompt Engineering Guide. https. github.com dair- ai Prompt-Engineering-Guide 12 2022 2. 7. 18. 34 33 A. Zeng. X. Liu. Z. Du. Z. Wang. H. Lai. M. Ding. Z. Yang. Xu. W. Zheng. X. Xia. et al. Glm-130b. An open bilingual pre-trained",
    "chunk_index": 409,
    "start_pos": 183159,
    "end_pos": 183613,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 470
    }
  },
  "502": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_410",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "a. et al. Glm-130b. An open bilingual pre-trained model. arXiv preprint arXiv.2210.02414 2022 2. 10. 23. 24. 25 34 Wang. H. Le. A. D. Gotmare. N. D. Bui. J. Li. S. C. Hoi. Codet5 Open code large language models for code understanding and genera- tion. arXiv preprint arXiv.2305.07922 2023 2. 11. 24. 25 35 S. Wang. Sun. Xiang. Z. Wu. S. Ding. W. Gong. S. Feng. J. Shang. Zhao. C. Pang. et al. Ernie 3.0 titan. Exploring larger-scale knowl- edge enhanced pre-training for language understanding and generation.",
    "chunk_index": 410,
    "start_pos": 183614,
    "end_pos": 184103,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 96,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 509
    }
  },
  "503": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_411",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "raining for language understanding and generation. arXiv preprint arXiv.2112.12731 2021 2. 8. 24. 25 36 J. Rasley. S. Rajbhandari. O. Ruwase. He. Deepspeed. System op- timizations enable training deep learning models with over 100 billion parameters. in. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining. 2020. pp. 3505 3506. 2. 37 S. Rajbhandari. J. Rasley. O. Ruwase. He. Zero. Memory optimiza- tions toward training trillion parameter models. in. SC20. International",
    "chunk_index": 411,
    "start_pos": 184104,
    "end_pos": 184588,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 515
    }
  },
  "504": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_412",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "trillion parameter models. in. SC20. International Conference for High Performance Computing. Networking. Storage and Analysis. IEEE. 2020. pp. 16. 2. 4. 24 38 J. He. C. Zhou. X. Ma. T. Berg-Kirkpatrick. G. Neubig. Towards unified view of parameter-e fficient transfer learning. arXiv preprint arXiv.2110.04366 2021 2. 20. 21 39 Z. Hu. Lan. L. Wang. W. Xu. E.-P. Lim. R. K.-W. Lee. L. Bing. S. Po- ria. Llm-adapters. An adapter family for parameter-e fficient fine-tuning",
    "chunk_index": 412,
    "start_pos": 184589,
    "end_pos": 185024,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 486,
      "optimized_content_length": 471
    }
  },
  "505": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_413",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "dapter family for parameter-e fficient fine-tuning of large language models. arXiv preprint arXiv.2304.01933 2023 2. 20 40 B. Lester. R. Al-Rfou. N. Constant. The power of scale for parameter- efficient prompt tuning. arXiv preprint arXiv.2104.08691 2021 2. 8. 20. 21 41 X. L. Li. P. Liang. Prefix-tuning. Optimizing continuous prompts for generation. arXiv preprint arXiv.2101.00190 2021 2. 20. 21 42 X. Ma. G. Fang. X. Wang. Llm-pruner. On the structural pruning of large language models. arXiv preprint arXiv.2305.11627 2023 2. 22",
    "chunk_index": 413,
    "start_pos": 185025,
    "end_pos": 185525,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 533
    }
  },
  "506": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_414",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "els. arXiv preprint arXiv.2305.11627 2023 2. 22 43 R. Xu. F. Luo. C. Wang. B. Chang. J. Huang. S. Huang. F. Huang. From dense to sparse. Contrastive pruning for better pre-trained lan- guage model compression. in. Proceedings of the AAAI Conference on Artificial Intelligence. ol. 36. 2022. pp. 11547 11555. 2. 22 44 G. Xiao. J. Lin. M. Seznec. H. Wu. J. Demouth. S. Han. Smoothquant. Accurate and fficient post-training quantization for large language models. in. ICML. ol. 202 of Proceedings of Machine Learning Re-",
    "chunk_index": 414,
    "start_pos": 185526,
    "end_pos": 186005,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 517
    }
  },
  "507": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_415",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ol. 202 of Proceedings of Machine Learning Re- search. PMLR. 2023. pp. 38087 38099. 2. 21 45 C. Tao. L. Hou. W. Zhang. L. Shang. X. Jiang. Q. Liu. P. Luo. N. Wong. Compression of generative pre-trained language models via quantiza- tion. arXiv preprint arXiv.2203.10705 2022 2. 21 46 A. Pal. D. Karkhanis. M. Roberts. S. Dooley. A. Sundararajan. S. Naidu. Giraffe. Adventures in expanding context lengths in llms. arXiv preprint arXiv.2308.10882 2023 2. 17 47 B. Peng. J. Quesnelle. H. Fan. E. Shippole. Yarn. fficient con-",
    "chunk_index": 415,
    "start_pos": 186006,
    "end_pos": 186496,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 523
    }
  },
  "508": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_416",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "snelle. H. Fan. E. Shippole. Yarn. fficient con- text window extension of large language models. arXiv preprint arXiv.2309.00071 2023 2. 17 48 M. Guo. J. Ainslie. D. Uthus. S. Ontanon. J. Ni. .-H. Sung. Yang. 36",
    "chunk_index": 416,
    "start_pos": 186497,
    "end_pos": 186670,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 224,
      "word_count": 40,
      "optimized_for_embedding": true,
      "original_content_length": 224,
      "optimized_content_length": 211
    }
  },
  "509": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_417",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "thus. S. Ontanon. J. Ni. .-H. Sung. Yang. 36 --- Page 37 --- Longt5. fficient text-to-text transformer for long sequences. arXiv preprint arXiv.2112.07916 2021 2. 18 49 S. Chen. S. Wong. L. Chen. Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv.2306.15595 2023 2. 17 50 W. X. Zhao. K. Zhou. J. Li. T. Tang. X. Wang. Hou. Min. B. Zhang. J. Zhang. Z. Dong. et al. survey of large language models. arXiv preprint arXiv.2303.18223 2023 2. 3.",
    "chunk_index": 417,
    "start_pos": 186672,
    "end_pos": 187154,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 95,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 495
    }
  },
  "510": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_418",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "s. arXiv preprint arXiv.2303.18223 2023 2. 3. 51 U. Naseem. I. Razzak. S. K. Khan. M. Prasad. comprehensive sur- vey on word representation models. From classical to state-of-the-art word representation language models. Transactions on Asian and Low- Resource Language Information Processing 20 2021 35. 2. 52 B. Min. H. Ross. E. Sulem. A. P. B. Veyseh. T. H. Nguyen. O. Sainz. E. Agirre. I. Heinz. D. Roth. Recent advances in natural language pro- cessing via large pre-trained language models. survey. arXiv preprint",
    "chunk_index": 418,
    "start_pos": 187155,
    "end_pos": 187645,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 518
    }
  },
  "511": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_419",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "-trained language models. survey. arXiv preprint arXiv.2111.01243 2021 2. 53 C. Zhou. Q. Li. C. Li. J. Yu. Liu. G. Wang. K. Zhang. C. Ji. Q. Yan. L. He. et al. comprehensive survey on pretrained foundation models. history from bert to chatgpt. arXiv preprint arXiv.2302.09419 2023 2. 54 Q. Dong. L. Li. D. Dai. C. Zheng. Z. Wu. B. Chang. X. Sun. J. Xu. Z. Sui. survey for in-context learning. arXiv preprint arXiv.2301.00234 2022 2. 7. 18 55 J. Huang. K. C.-C. Chang. Towards reasoning in large language models.",
    "chunk_index": 419,
    "start_pos": 187646,
    "end_pos": 188138,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 97,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 511
    }
  },
  "512": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_420",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Chang. Towards reasoning in large language models. survey. arXiv preprint arXiv.2212.10403 2022 2. 7. 18 56 Wang. W. Zhong. L. Li. F. Mi. X. Zeng. W. Huang. L. Shang. X. Jiang. Q. Liu. Aligning large language models with human. survey. arXiv preprint arXiv.2307.12966 2023 57 X. Zhu. J. Li. Liu. C. Ma. W. Wang. survey on model compression for large language models. arXiv preprint arXiv.2308.07633 2023 58 S. Yin. C. Fu. S. Zhao. K. Li. X. Sun. T. Xu. E. Chen. survey on multi-",
    "chunk_index": 420,
    "start_pos": 188139,
    "end_pos": 188601,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 478
    }
  },
  "513": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_421",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "K. Li. X. Sun. T. Xu. E. Chen. survey on multi- modal large language models. arXiv preprint arXiv.2306.13549 2023 2. 22. 23 59 J. J. Webster. C. Kit. Tokenization as the initial phase in nlp. in. COL- ING 1992 volume 4. The 14th international conference on computa- tional linguistics. 1992. 60 T. Kudo. Subword regularization. Improving neural network translation models with multiple subword candidates. in. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ol-",
    "chunk_index": 421,
    "start_pos": 188602,
    "end_pos": 189067,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 501
    }
  },
  "514": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_422",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Association for Computational Linguistics ol- ume 1. Long Papers 2018. pp. 66 75. 61 R. Sennrich. B. Haddow. A. Birch. Neural machine translation of rare words with subword units. in. Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics olume 1. Long Papers 2016. pp. 1715 1725. 62 M. Schuster. K. Nakajima. Japanese and korean voice search. in. 2012 IEEE international conference on acoustics. speech and signal process- ing ICASSP IEEE. 2012. pp. 5149 5152.",
    "chunk_index": 422,
    "start_pos": 189068,
    "end_pos": 189538,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 496
    }
  },
  "515": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_423",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rocess- ing ICASSP IEEE. 2012. pp. 5149 5152. 63 S. J. Mielke. Z. Alyafeai. E. Salesky. C. Ra ffel. M. Dey. M. Galle A. Raja. C. Si. W. Lee. B. Sagot. et al. Between words and char- acters. brief history of open-vocabulary modeling and tokenization in nlp. arXiv preprint arXiv.2112.10508 2021 64 A. Vaswani. N. Shazeer. N. Parmar. J. Uszkoreit. L. Jones. A. N. Gomez. \u0141. Kaiser. I. Polosukhin. Attention is all you need. Advances in neural information processing systems 30 2017 4.",
    "chunk_index": 423,
    "start_pos": 189539,
    "end_pos": 189997,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 482
    }
  },
  "516": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_424",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ral information processing systems 30 2017 4. 65 O. Press. N. Smith. M. Lewis. Train short. test long. Attention with linear biases enables input length extrapolation. in. International Con- ference on Learning Representations. 2022. URL https. openreview.net forum.id R8sQPpGCv0 4. 17 66 J. Su. Lu. S. Pan. A. Murtadha. B. Wen. Liu. Roformer. En- hanced transformer with rotary position embedding. arXiv preprint arXiv.2104.09864 2021 4. 9. 17 67 R. Child. S. Gray. A. Radford. I. Sutskever. Generating long sequences",
    "chunk_index": 424,
    "start_pos": 189998,
    "end_pos": 190488,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 518
    }
  },
  "517": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_425",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Radford. I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv.1904.10509 2019 4. 7. 23 68 T. Dao. D. Fu. S. Ermon. A. Rudra. C. Re Flashattention. Fast and memory-e fficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 2022 16344 16359. 69 K. Hornik. M. Stinchcombe. H. White. Multilayer feedforward networks are universal approximators. Neural networks 1989 359 366. 70 Nair. G. E. Hinton. Rectified linear units improve restricted boltz-",
    "chunk_index": 425,
    "start_pos": 190489,
    "end_pos": 190984,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 516
    }
  },
  "518": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_426",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Rectified linear units improve restricted boltz- mann machines. in. Proceedings of the 27th international conference onmachine learning ICML-10 2010. pp. 807 814. 71 D. Hendrycks. K. Gimpel. Gaussian error linear units gelus arXiv preprint arXiv.1606.08415 2016 72 N. Srivastava. G. Hinton. A. Krizhevsky. I. Sutskever. R. Salakhutdinov. Dropout. simple way to prevent neural networks from overfitting. The journal of machine learning research 15 2014 1929 1958.",
    "chunk_index": 426,
    "start_pos": 190985,
    "end_pos": 191425,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 462
    }
  },
  "519": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_427",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "chine learning research 15 2014 1929 1958. 73 D. Krueger. T. Maharaj. J. Krama r. M. Pezeshki. N. Ballas. N. R. Ke. A. Goyal. Bengio. A. Courville. C. Pal. Zoneout. Regular- izing rnns by randomly preserving hidden activations. arXiv preprint arXiv.1606.01305 2016 74 N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv.2002.05202 2020 75 N. Dauphin. A. Fan. M. Auli. D. Grangier. Language modeling with gated convolutional networks. in. International conference on machine",
    "chunk_index": 427,
    "start_pos": 191426,
    "end_pos": 191894,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 488
    }
  },
  "520": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_428",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "networks. in. International conference on machine learning. PMLR. 2017. pp. 933 941. 76 J. L. Ba. J. R. Kiros. G. E. Hinton. Layer normalization. arXiv preprint arXiv.1607.06450 2016 77 B. Zhang. R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems 32 2019 78 A. Baevski. M. Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv.1809.10853 2018 79 H. Wang. S. Ma. L. Dong. S. Huang. D. Zhang. F. Wei. Deepnet. Scaling",
    "chunk_index": 428,
    "start_pos": 191895,
    "end_pos": 192369,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 499
    }
  },
  "521": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_429",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Dong. S. Huang. D. Zhang. F. Wei. Deepnet. Scaling transformers to 1.000 layers. arXiv preprint arXiv.2203.00555 2022 80 M. Shoeybi. M. Patwary. R. Puri. P. LeGresley. J. Casper. B. Catanzaro. Megatron-lm. Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv.1909.08053 2019 4. 81 bmtrain. fficient training for big models. URL https. github.com OpenBMB BMTrain 4. 82 T. Wolf. L. Debut. Sanh. J. Chaumond. C. Delangue. A. Moi. P. Cis-",
    "chunk_index": 429,
    "start_pos": 192370,
    "end_pos": 192823,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 476
    }
  },
  "522": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_430",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Sanh. J. Chaumond. C. Delangue. A. Moi. P. Cis- tac. T. Rault. R. Louf. M. Funtowicz. et al. Transformers. State-of-the- art natural language processing. in. Proceedings of the 2020 conference on empirical methods in natural language processing. system demon- strations. 2020. pp. 38 45. 83 J. Bradbury. R. Frostig. P. Hawkins. M. J. Johnson. C. Leary. D. Maclau- rin. G. Necula. A. Paszke. J. VanderPlas. S. Wanderman-Milne. et al. Jax. composable transformations of python numpy programs 2018",
    "chunk_index": 430,
    "start_pos": 192824,
    "end_pos": 193282,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 494
    }
  },
  "523": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_431",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ransformations of python numpy programs 2018 84 S. Li. J. Fang. Z. Bian. H. Liu. Liu. H. Huang. B. Wang. You. Colossal-ai. unified deep learning system for large-scale parallel train- ing. arXiv preprint arXiv.2110.14883 2021 85 J. He. J. Qiu. A. Zeng. Z. Yang. J. Zhai. J. Tang. Fastmoe. fast mixture-of-expert training system. arXiv preprint arXiv.2103.13262 2021 86 L. Huawei Technologies Co. Huawei mindspore ai development frame- work. in. Artificial Intelligence Technology. Springer. 2022. pp. 137 162.",
    "chunk_index": 431,
    "start_pos": 193283,
    "end_pos": 193779,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 509
    }
  },
  "524": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_432",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "igence Technology. Springer. 2022. pp. 137 162. 87 A. Paszke. S. Gross. F. Massa. A. Lerer. J. Bradbury. G. Chanan. T. Killeen. Z. Lin. N. Gimelshein. L. Antiga. et al. Pytorch. An imper- ative style. high-performance deep learning library. Advances in neural information processing systems 32 2019 88 M. Abadi. P. Barham. J. Chen. Z. Chen. A. Davis. J. Dean. M. Devin. S. Ghemawat. G. Irving. M. Isard. et al. Tensorflow. system for large- scale machine learning. in. Osdi. ol. 16. Savannah. GA. USA. 2016. pp. 265 283.",
    "chunk_index": 432,
    "start_pos": 193780,
    "end_pos": 194270,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 520
    }
  },
  "525": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_433",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ol. 16. Savannah. GA. USA. 2016. pp. 265 283. 89 T. Chen. M. Li. Li. M. Lin. N. Wang. M. Wang. T. Xiao. B. Xu. C. Zhang. Z. Zhang. Mxnet. flexible and fficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv.1512.01274 2015 90 W. Fedus. B. Zoph. N. Shazeer. Switch transformers. Scaling to tril- lion parameter models with simple and fficient sparsity. The Journal of Machine Learning Research 23 2022 5232 5270. 5.",
    "chunk_index": 433,
    "start_pos": 194271,
    "end_pos": 194704,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 484,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 483,
      "optimized_content_length": 452
    }
  },
  "526": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_434",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ne Learning Research 23 2022 5232 5270. 5. 91 N. Du. Huang. A. M. Dai. S. Tong. D. Lepikhin. Xu. M. Krikun. Zhou. A. W. Yu. O. Firat. et al. Glam. fficient scaling of language models with mixture-of-experts. in. International Conference on Ma- chine Learning. PMLR. 2022. pp. 5547 5569. 5. 9. 23. 24. 25 92 X. Ren. P. Zhou. X. Meng. X. Huang. Wang. W. Wang. P. Li. X. Zhang. A. Podolskiy. G. Arshinov. et al. Pangu-P. Towards trillion parameter language model with sparse heterogeneous computing. arXiv",
    "chunk_index": 434,
    "start_pos": 194705,
    "end_pos": 195188,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 97,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 502
    }
  },
  "527": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_435",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "model with sparse heterogeneous computing. arXiv preprint arXiv.2303.10845 2023 5. 10. 16. 23. 24. 25 93 T. Wang. A. Roberts. D. Hesslow. T. Le Scao. H. W. Chung. I. Beltagy. J. Launay. C. Ra ffel. What language model architecture and pretrain- 37",
    "chunk_index": 435,
    "start_pos": 195189,
    "end_pos": 195392,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 254,
      "word_count": 43,
      "optimized_for_embedding": true,
      "original_content_length": 254,
      "optimized_content_length": 247
    }
  },
  "528": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_436",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "What language model architecture and pretrain- 37 --- Page 38 --- ing objective works best for zero-shot generalization. in. International Conference on Machine Learning. PMLR. 2022. pp. 22964 22984. 94 L. Dong. N. Yang. W. Wang. F. Wei. X. Liu. Wang. J. Gao. M. Zhou. H.-W. Hon. Unified language model pre-training for natural language understanding and generation. Advances in neural information process- ing systems 32 2019 95 J. Kaplan. S. McCandlish. T. Henighan. T. B. Brown. B. Chess. R. Child.",
    "chunk_index": 436,
    "start_pos": 195394,
    "end_pos": 195861,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 501
    }
  },
  "529": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_437",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ish. T. Henighan. T. B. Brown. B. Chess. R. Child. S. Gray. A. Radford. J. Wu. D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv.2001.08361 2020 96 J. Ho ffmann. S. Borgeaud. A. Mensch. E. Buchatskaya. T. Cai. E. Rutherford. D. d. L. Casas. L. A. Hendricks. J. Welbl. A. Clark. et al. Training compute-optimal large language models. arXiv preprint arXiv.2203.15556 2022 6. 9. 25. 29 97 S. Iyer. X. Lin. R. Pasunuru. T. Mihaylov. D. Simig. P. Yu. K. Shuster.",
    "chunk_index": 437,
    "start_pos": 195862,
    "end_pos": 196308,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 480
    }
  },
  "530": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_438",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "asunuru. T. Mihaylov. D. Simig. P. Yu. K. Shuster. T. Wang. Q. Liu. P. S. Koura. et al. Opt-iml. Scaling language model in- struction meta learning through the lens of generalization. arXiv preprint arXiv.2212.12017 2022 7. 11. 16. 17. 22. 25. 28 98 Z. Sun. Shen. Q. Zhou. H. Zhang. Z. Chen. D. Cox. Yang. C. Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv.2305.03047 2023 7. 17 99 A. Askell. Bai. A. Chen. D. Drain. D. Ganguli. T. Henighan. A. Jones.",
    "chunk_index": 438,
    "start_pos": 196309,
    "end_pos": 196806,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 525
    }
  },
  "531": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_439",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Chen. D. Drain. D. Ganguli. T. Henighan. A. Jones. N. Joseph. B. Mann. N. DasSarma. et al. general language assistant as laboratory for alignment. arXiv preprint arXiv.2112.00861 2021 100 D. M. Ziegler. N. Stiennon. J. Wu. T. B. Brown. A. Radford. D. Amodei. P. Christiano. G. Irving. Fine-tuning language models from human pref- erences. arXiv preprint arXiv.1909.08593 2019 101 S. Kim. S. J. Joo. D. Kim. J. Jang. S. Ye. J. Shin. M. Seo. The cot collec- tion. Improving zero-shot and few-shot learning of language models via",
    "chunk_index": 439,
    "start_pos": 196807,
    "end_pos": 197301,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 526
    }
  },
  "532": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_440",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "-shot and few-shot learning of language models via chain-of-thought fine-tuning. arXiv preprint arXiv.2305.14045 2023 7. 16 102 Q. Liu. F. Zhou. Z. Jiang. L. Dou. M. Lin. From zero to hero. Exam- ining the power of symbolic tasks in instruction tuning. arXiv preprint arXiv.2304.07995 2023 7. 16 103 J. Wei. X. Wang. D. Schuurmans. M. Bosma. F. Xia. E. Chi. Q. Le. D. Zhou. et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 2022 24824 24837. 7. 20. 23",
    "chunk_index": 440,
    "start_pos": 197302,
    "end_pos": 197800,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 532
    }
  },
  "533": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_441",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rocessing Systems 35 2022 24824 24837. 7. 20. 23 104 X. Wang. J. Wei. D. Schuurmans. Q. Le. E. Chi. S. Narang. A. Chowd- hery. D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv.2203.11171 2022 7. 20 105 S. Yao. D. Yu. J. Zhao. I. Shafran. T. L. Gri ffiths. Cao. K. Narasimhan. Tree of thoughts. Deliberate problem solving with large language mod- els. arXiv preprint arXiv.2305.10601 2023 7. 20 106 N. Houlsby. A. Giurgiu. S. Jastrzebski. B. Morrone. Q. De Laroussilhe.",
    "chunk_index": 441,
    "start_pos": 197801,
    "end_pos": 198290,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 522
    }
  },
  "534": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_442",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "iu. S. Jastrzebski. B. Morrone. Q. De Laroussilhe. A. Gesmundo. M. Attariyan. S. Gelly. Parameter-e fficient transfer learn- ing for nlp. in. International Conference on Machine Learning. PMLR. 2019. pp. 2790 2799. 7. 20 107 S. McCandlish. J. Kaplan. D. Amodei. O. D. Team. An empirical model of large-batch training. arXiv preprint arXiv.1812.06162 2018 108 W. Zeng. X. Ren. T. Su. H. Wang. Liao. Z. Wang. X. Jiang. Z. Yang. K. Wang. X. Zhang. et al. Pangu- \u03b1. Large-scale autoregressive pre-",
    "chunk_index": 442,
    "start_pos": 198291,
    "end_pos": 198747,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 493
    }
  },
  "535": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_443",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "et al. Pangu- \u03b1. Large-scale autoregressive pre- trained chinese language models with auto-parallel computation. arXiv preprint arXiv.2104.12369 2021 8. 23. 24. 25 109 S. Yuan. H. Zhao. Z. Du. M. Ding. X. Liu. Cen. X. Zou. Z. Yang. J. Tang. Wudaocorpora. super large-scale chinese corpora for pre- training language models. AI Open 2021 65 68. 8. 30 110 Sun. S. Wang. S. Feng. S. Ding. C. Pang. J. Shang. J. Liu. X. Chen. Zhao. Lu. et al. Ernie 3.0. Large-scale knowledge enhanced",
    "chunk_index": 443,
    "start_pos": 198748,
    "end_pos": 199209,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 480
    }
  },
  "536": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_444",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "et al. Ernie 3.0. Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv.2107.02137 2021 8. 25 111 Z. Dai. Z. Yang. Yang. J. Carbonell. Q. Le. R. Salakhutdinov. Transformer-xl. Attentive language models beyond fixed-length con- text. arXiv preprint arXiv.1901.02860 2019 112 O. Lieber. O. Sharir. B. Lenz. Shoham. Jurassic-1. Technical details and evaluation. White Paper. AI21 Labs 2021 8. 24. 25",
    "chunk_index": 444,
    "start_pos": 199210,
    "end_pos": 199642,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 483,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 482,
      "optimized_content_length": 450
    }
  },
  "537": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_445",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "uation. White Paper. AI21 Labs 2021 8. 24. 25 113 Levine. N. Wies. O. Sharir. H. Bata. A. Shashua. Limits to depth ef- ficiencies of self-attention. Advances in Neural Information Processing Systems 33 2020 22640 22651. 8. 11 114 B. Kim. H. Kim. S.-W. Lee. G. Lee. D. Kwak. D. H. Jeon. S. Park.S. Kim. S. Kim. D. Seo. et al. What changes can large-scale language models bring. intensive study on hyperclova. Billions-scale korean generative pretrained transformers. arXiv preprint arXiv.2109.04650 2021 8. 25",
    "chunk_index": 445,
    "start_pos": 199643,
    "end_pos": 200119,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 508
    }
  },
  "538": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_446",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ers. arXiv preprint arXiv.2109.04650 2021 8. 25 115 S. Wu. X. Zhao. T. Yu. R. Zhang. C. Shen. H. Liu. F. Li. H. Zhu. J. Luo. L. Xu. et al. Yuan 1.0. Large-scale pre-trained language model in zero- shot and few-shot learning. arXiv preprint arXiv.2110.04725 2021 8. 24. 25 116 J. W. Rae. S. Borgeaud. T. Cai. K. Millican. J. Ho ffmann. F. Song. J. Aslanides. S. Henderson. R. Ring. S. Young. et al. Scaling lan- guage models. Methods. analysis insights from training gopher. arXiv preprint arXiv.2112.11446 2021 8. 9. 25. 28",
    "chunk_index": 446,
    "start_pos": 200120,
    "end_pos": 200609,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 523
    }
  },
  "539": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_447",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Xiv preprint arXiv.2112.11446 2021 8. 9. 25. 28 117 S. Smith. M. Patwary. B. Norick. P. LeGresley. S. Rajbhandari. J. Casper. Z. Liu. S. Prabhumoye. G. Zerveas. Korthikanti. et al. Using deepspeed and megatron to train megatron-turing nlg 530b. large-scale generative language model. arXiv preprint arXiv.2201.11990 2022 8. 9. 24. 25 118 S. Black. S. Biderman. E. Hallahan. Q. Anthony. L. Gao. L. Golding. H. He. C. Leahy. K. McDonell. J. Phang. et al. Gpt-neox-20b. An open-",
    "chunk_index": 447,
    "start_pos": 200610,
    "end_pos": 201052,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 475
    }
  },
  "540": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_448",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "McDonell. J. Phang. et al. Gpt-neox-20b. An open- source autoregressive language model. arXiv preprint arXiv.2204.06745 2022 9. 23. 24. 25 119 W. Ben. K. Aran. Gpt-j-6b. billion parameter autoregressive lan- guage model 2021 120 P. Micikevicius. S. Narang. J. Alben. G. Diamos. E. Elsen. D. Garcia. B. Ginsburg. M. Houston. O. Kuchaiev. G. Venkatesh. et al. Mixed pre- cision training. arXiv preprint arXiv.1710.03740 2017 9. 23 121 N. Shazeer. A. Mirhoseini. K. Maziarz. A. Davis. Q. Le. G. Hin-",
    "chunk_index": 448,
    "start_pos": 201053,
    "end_pos": 201521,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 496
    }
  },
  "541": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_449",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Mirhoseini. K. Maziarz. A. Davis. Q. Le. G. Hin- ton. J. Dean. Outrageously large neural networks. The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv.1701.06538 2017 9. 23 122 S. Soltan. S. Ananthakrishnan. J. FitzGerald. R. Gupta. W. Hamza. H. Khan. C. Peris. S. Rawls. A. Rosenbaum. A. Rumshisky. et al. Alex- atm 20b. Few-shot learning using large-scale multilingual seq2seq model. arXiv preprint arXiv.2208.01448 2022 9. 23. 24. 25 123 R. Anil. A. M. Dai. O. Firat. M. Johnson. D. Lepikhin. A. Passos.",
    "chunk_index": 449,
    "start_pos": 201522,
    "end_pos": 202006,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 520
    }
  },
  "542": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_450",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Dai. O. Firat. M. Johnson. D. Lepikhin. A. Passos. S. Shakeri. E. Taropa. P. Bailey. Z. Chen. et al. Palm technical report. arXiv preprint arXiv.2305.10403 2023 9. 25 124 Tay. J. Wei. H. W. Chung. Q. Tran. D. R. So. S. Shakeri. X. Garcia. H. S. Zheng. J. Rao. A. Chowdhery. et al. Transcending scaling laws with 0.1 extra compute. arXiv preprint arXiv.2210.11399 2022 9. 24. 25 125 Tay. M. Dehghani. Q. Tran. X. Garcia. J. Wei. X. Wang. H. W. Chung. D. Bahri. T. Schuster. S. Zheng. et al. Ul2. Unifying lan-",
    "chunk_index": 450,
    "start_pos": 202007,
    "end_pos": 202496,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 102,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 508
    }
  },
  "543": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_451",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "T. Schuster. S. Zheng. et al. Ul2. Unifying lan- guage learning paradigms. in. The Eleventh International Conference on Learning Representations. 2022. 9. 10. 24. 25 126 Z. Du. Qian. X. Liu. M. Ding. J. Qiu. Z. Yang. J. Tang. Glm. Gen- eral language model pretraining with autoregressive blank infilling. in. Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics olume 1. Long Papers 2022. pp. 320 335. 10 127 H. Touvron. T. Lavril. G. Izacard. X. Martinet. M.-A. Lachaux.",
    "chunk_index": 451,
    "start_pos": 202497,
    "end_pos": 202969,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 508
    }
  },
  "544": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_452",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "T. Lavril. G. Izacard. X. Martinet. M.-A. Lachaux. T. Lacroix. B. Rozie re. N. Goyal. E. Hambro. F. Azhar. et al. Llama. Open and fficient foundation language models. arXiv preprint arXiv.2302.13971 2023 10. 23. 25 128 M. N. Rabe. C. Staats. Self-attention does not need n2 memory. arXiv preprint arXiv.2112.05682 2021 10 129 A. Korthikanti. J. Casper. S. Lym. L. McAfee. M. Andersch. M. Shoeybi. B. Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems 2023 10",
    "chunk_index": 452,
    "start_pos": 202970,
    "end_pos": 203468,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 525
    }
  },
  "545": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_453",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "dings of Machine Learning and Systems 2023 10 130 A. Dubey. A. Jauhri. A. Pandey. A. Kadian. A. Al-Dahle. A. Letman. A. Mathur. A. Schelten. A. Yang. A. Fan. et al. The llama herd of models. arXiv preprint arXiv.2407.21783 2024 10. 25 131 https. mistral.ai news mixtral-8x22b 10. 25 132 https. github.com Snowflake-Labs snowflake-arctic 10. 25 133 https. github.com xai-org grok-1 10 134 https. x.ai blog grok-1.5 10 135 G. Team. R. Anil. S. Borgeaud. Wu. J.-B. Alayrac. J. Yu. R. Soricut.",
    "chunk_index": 453,
    "start_pos": 203469,
    "end_pos": 203947,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 489
    }
  },
  "546": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_454",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "orgeaud. Wu. J.-B. Alayrac. J. Yu. R. Soricut. J. Schalkwyk. A. M. Dai. A. Hauth. et al. Gemini. family of highly capable multimodal models. arXiv preprint arXiv.2312.11805 2023 10 136 M. Reid. N. Savinov. D. Teplyashin. D. Lepikhin. T. Lillicrap. J.-b. 38",
    "chunk_index": 454,
    "start_pos": 203948,
    "end_pos": 204165,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 268,
      "word_count": 45,
      "optimized_for_embedding": true,
      "original_content_length": 268,
      "optimized_content_length": 256
    }
  },
  "547": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_455",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "D. Teplyashin. D. Lepikhin. T. Lillicrap. J.-b. 38 --- Page 39 --- Alayrac. R. Soricut. A. Lazaridou. O. Firat. J. Schrittwieser. et al. Gem- ini 1.5. Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv.2403.05530 2024 10 137 B. Adler. N. Agarwal. A. Aithal. D. H. Anh. P. Bhattacharya. A. Brun- dyn. J. Casper. B. Catanzaro. S. Clay. J. Cohen. et al. Nemotron-4 340b technical report. arXiv preprint arXiv.2406.11704 2024 10. 25",
    "chunk_index": 455,
    "start_pos": 204167,
    "end_pos": 204597,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 481,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 471
    }
  },
  "548": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_456",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rt. arXiv preprint arXiv.2406.11704 2024 10. 25 138 X. Bi. D. Chen. G. Chen. S. Chen. D. Dai. C. Deng. H. Ding. K. Dong. Q. Du. Z. Fu. et al. Deepseek llm. Scaling open-source language models with longtermism. arXiv preprint arXiv.2401.02954 2024 10. 25 139 DeepSeek-AI. A. Liu. B. Feng. B. Wang. B. Wang. B. Liu. C. Zhao. C. Deng. C. Ruan. D. Dai. D. Guo. D. Yang. D. Chen. D. Ji. E. Li. F. Lin. F. Luo. G. Hao. G. Chen. G. Li. H. Zhang. H. Xu. H. Yang. H. Zhang. H. Ding. H. Xin. H. Gao. H. Li. H. Qu. J. L. Cai. J. Liang.",
    "chunk_index": 456,
    "start_pos": 204598,
    "end_pos": 205082,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 107,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 524
    }
  },
  "549": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_457",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "H. Xin. H. Gao. H. Li. H. Qu. J. L. Cai. J. Liang. J. Guo. J. Ni. J. Li. J. Chen. J. Yuan. J. Qiu. J. Song. K. Dong. K. Gao. K. Guan. L. Wang. L. Zhang. L. Xu. L. Xia. L. Zhao. L. Zhang. M. Li. M. Wang. M. Zhang. M. Zhang. M. Tang. M. Li. N. Tian. P. Huang. P. Wang. P. Zhang. Q. Zhu. Q. Chen. Q. Du. R. J. Chen. R. L. Jin. R. Ge. R. Pan. R. Xu. R. Chen. S. S. Li. S. Lu. S. Zhou. S. Chen. S. Wu. S. Ye. S. Ma. S. Wang. S. Zhou. S. Yu. S. Zhou. S. Zheng. T. Wang. T. Pei. T. Yuan. T. Sun. W. L. Xiao. W. Zeng. W. An. W. Liu. W. Liang. W. Gao.",
    "chunk_index": 457,
    "start_pos": 205083,
    "end_pos": 205574,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 131,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 542
    }
  },
  "550": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_458",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "L. Xiao. W. Zeng. W. An. W. Liu. W. Liang. W. Gao. W. Zhang. X. Q. Li. X. Jin. X. Wang. X. Bi. X. Liu. X. Wang. X. Shen. X. Chen. X. Chen. X. Nie. X. Sun. Deepseek-v2. strong. economical. and fficient mixture-of-experts language model. CoRR abs 2405.04434 2024 10. 25 140 E. Nijkamp. B. Pang. H. Hayashi. L. Tu. H. Wang. Zhou. S. Savarese. C. Xiong. Codegen. An open large language model for code with multi- turn program synthesis. arXiv preprint arXiv.2203.13474 2022 11. 23. 25. 28",
    "chunk_index": 458,
    "start_pos": 205575,
    "end_pos": 206025,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 484
    }
  },
  "551": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_459",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "preprint arXiv.2203.13474 2022 11. 23. 25. 28 141 M. Chen. J. Tworek. H. Jun. Q. Yuan. H. P. d. O. Pinto. J. Kaplan. H. Ed- wards. Burda. N. Joseph. G. Brockman. et al. Evaluating large lan- guage models trained on code. arXiv preprint arXiv.2107.03374 2021 11. 25. 29. 31 142 Li. D. Choi. J. Chung. N. Kushman. J. Schrittwieser. R. Leblond. T. Eccles. J. Keeling. F. Gimeno. A. Dal Lago. et al. Competition-level code generation with alphacode. Science 378 6624 2022 1092 1097. 11. 23. 25. 29",
    "chunk_index": 459,
    "start_pos": 206026,
    "end_pos": 206494,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 493
    }
  },
  "552": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_460",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "cience 378 6624 2022 1092 1097. 11. 23. 25. 29 143 N. Shazeer. Fast transformer decoding. One write-head is all you need. arXiv preprint arXiv.1911.02150 2019 11 144 R. Pang. H. He. Text generation by learning from demonstrations. arXiv preprint arXiv.2009.07839 2020 11 145 R. Dabre. A. Fujita. Softmax tempering for training neural machine translation models. arXiv preprint arXiv.2009.09372 2020 11 146 Wang. W. Wang. S. Joty. S. C. Hoi. Codet5. Identifier-aware unified",
    "chunk_index": 460,
    "start_pos": 206495,
    "end_pos": 206946,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 473
    }
  },
  "553": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_461",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Joty. S. C. Hoi. Codet5. Identifier-aware unified pre-trained encoder-decoder models for code understanding and genera- tion. arXiv preprint arXiv.2109.00859 2021 11 147 R. Li. L. B. Allal. Zi. N. Muennigho ff. D. Kocetkov. C. Mou. M. Marone. C. Akiki. J. Li. J. Chim. et al. Starcoder. may the source be with you. arXiv preprint arXiv.2305.06161 2023 11. 25 148 R. Taylor. M. Kardas. G. Cucurull. T. Scialom. A. Hartshorn. E. Saravia. A. Poulton. Kerkez. R. Stojnic. Galactica. large language model for",
    "chunk_index": 461,
    "start_pos": 206947,
    "end_pos": 207422,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 503
    }
  },
  "554": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_462",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "R. Stojnic. Galactica. large language model for science. arXiv preprint arXiv.2211.09085 2022 11. 24. 25. 29 149 FairScale authors. Fairscale. general purpose modular pytorch library for high performance and large scale training. https. github.com facebookresearch fairscale 2021 11 150 R. Thoppilan. D. De Freitas. J. Hall. N. Shazeer. A. Kulshreshtha. H.-T. Cheng. A. Jin. T. Bos. L. Baker. Du. et al. Lamda. Language models for dialog applications. arXiv preprint arXiv.2201.08239 2022 11. 25",
    "chunk_index": 462,
    "start_pos": 207423,
    "end_pos": 207892,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 495
    }
  },
  "555": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_463",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ns. arXiv preprint arXiv.2201.08239 2022 11. 25 151 S. Wu. O. Irsoy. S. Lu. Dabravolski. M. Dredze. S. Gehrmann. P. Kambadur. D. Rosenberg. G. Mann. Bloomberggpt. large language model for finance. arXiv preprint arXiv.2303.17564 2023 11. 25. 33 152 X. Zhang. Q. Yang. D. Xu. Xuanyuan 2.0. large chinese finan- cial chat model with hundreds of billions parameters. arXiv preprint arXiv.2305.12002 2023 11. 17. 25 153 W. Ben. Mesh-transformer-jax. Model-parallel implementation of trans-",
    "chunk_index": 463,
    "start_pos": 207893,
    "end_pos": 208350,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 485
    }
  },
  "556": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_464",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ormer-jax. Model-parallel implementation of trans- former language model with jax 2021 12. 24 154 N. Muennigho ff. T. Wang. L. Sutawika. A. Roberts. S. Biderman. T. L. Scao. M. S. Bari. S. Shen. Z.-X. Yong. H. Schoelkopf. et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv.2211.01786 2022 16. 25. 28. 31 155 D. Yin. X. Liu. F. Yin. M. Zhong. H. Bansal. J. Han. K.-W. Chang. Dynosaur. dynamic growth paradigm for instruction-tuning data cu-ration. arXiv preprint arXiv.2305.14327 2023 16",
    "chunk_index": 464,
    "start_pos": 208351,
    "end_pos": 208837,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 521
    }
  },
  "557": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_465",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ration. arXiv preprint arXiv.2305.14327 2023 16 156 P. Gao. J. Han. R. Zhang. Z. Lin. S. Geng. A. Zhou. W. Zhang. P. Lu. C. He. X. Yue. et al. Llama-adapter v2. Parameter-e fficient visual in- struction model. arXiv preprint arXiv.2304.15010 2023 16. 24 157 Openai. gpt-4 technical report 2023 16. 35 158 R. Taori. I. Gulrajani. T. Zhang. Dubois. X. Li. C. Guestrin. P. Liang. T. B. Hashimoto. Stanford alpaca. An instruction-following llama model. https. github.com tatsu-lab stanford_alpaca 2023 16. 25. 28",
    "chunk_index": 465,
    "start_pos": 208838,
    "end_pos": 209319,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 508
    }
  },
  "558": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_466",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "b.com tatsu-lab stanford_alpaca 2023 16. 25. 28 159 W.-L. Chiang. Z. Li. Z. Lin. Sheng. Z. Wu. H. Zhang. L. Zheng. S. Zhuang. Zhuang. J. E. Gonzalez. I. Stoica. E. P. Xing. Vicuna. An open-source chatbot impressing gpt-4 with 90 chatgpt quality March 2023 URL https. lmsys.org blog 2023-03-30-vicuna 16. 22. 25. 28 160 B. Peng. C. Li. P. He. M. Galley. J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv.2304.03277 2023 16. 28 161 T. Liu. B. K. H. Low. Goat. Fine-tuned llama outperforms gpt-4 on",
    "chunk_index": 466,
    "start_pos": 209320,
    "end_pos": 209801,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 505
    }
  },
  "559": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_467",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Low. Goat. Fine-tuned llama outperforms gpt-4 on arithmetic tasks. arXiv preprint arXiv.2305.14201 2023 16 162 H. Wang. C. Liu. N. Xi. Z. Qiang. S. Zhao. B. Qin. T. Liu. Huatuo. Tuning llama model with chinese medical knowledge. arXiv preprint arXiv.2304.06975 2023 16 163 C. Xu. Q. Sun. K. Zheng. X. Geng. P. Zhao. J. Feng. C. Tao. D. Jiang. Wizardlm. Empowering large language models to follow complex in- structions. arXiv preprint arXiv.2304.12244 2023 16 164 Z. Luo. C. Xu. P. Zhao. Q. Sun. X. Geng. W. Hu. C. Tao. J. Ma. Q. Lin.",
    "chunk_index": 467,
    "start_pos": 209802,
    "end_pos": 210302,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 534
    }
  },
  "560": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_468",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ao. Q. Sun. X. Geng. W. Hu. C. Tao. J. Ma. Q. Lin. D. Jiang. Wizardcoder. Empowering code large language models with evol-instruct. arXiv preprint arXiv.2306.08568 2023 16. 25 165 J. Menick. M. Trebacz. Mikulik. J. Aslanides. F. Song. M. Chadwick. M. Glaese. S. Young. L. Campbell-Gillingham. G. Irving. et al. Teach- ing language models to support answers with verified quotes. arXiv preprint arXiv.2203.11147 2022 17 166 R. Nakano. J. Hilton. S. Balaji. J. Wu. L. Ouyang. C. Kim.",
    "chunk_index": 468,
    "start_pos": 210303,
    "end_pos": 210748,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 481
    }
  },
  "561": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_469",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "o. J. Hilton. S. Balaji. J. Wu. L. Ouyang. C. Kim. C. Hesse. S. Jain. Kosaraju. W. Saunders. et al. Webgpt. Browser- assisted question-answering with human feedback. arXiv preprint arXiv.2112.09332 2021 17. 19. 20. 25. 31 167 A. Glaese. N. McAleese. M. Tr ebacz. J. Aslanides. Firoiu. T. Ewalds. M. Rauh. L. Weidinger. M. Chadwick. P. Thacker. et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv.2209.14375 2022 17. 20. 25",
    "chunk_index": 469,
    "start_pos": 210749,
    "end_pos": 211185,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 487,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 487,
      "optimized_content_length": 468
    }
  },
  "562": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_470",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "arXiv preprint arXiv.2209.14375 2022 17. 20. 25 168 R. Rafailov. A. Sharma. E. Mitchell. S. Ermon. C. D. Manning. C. Finn. Direct preference optimization. Your language model is secretly re- ward model. arXiv preprint arXiv.2305.18290 2023 17 169 H. Dong. W. Xiong. D. Goyal. R. Pan. S. Diao. J. Zhang. K. Shum. T. Zhang. Raft. Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv.2304.06767 2023 17 170 Z. Yuan. H. Yuan. C. Tan. W. Wang. S. Huang. F. Huang. Rrhf. Rank",
    "chunk_index": 470,
    "start_pos": 211186,
    "end_pos": 211657,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 505
    }
  },
  "563": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_471",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "n. C. Tan. W. Wang. S. Huang. F. Huang. Rrhf. Rank responses to align language models with human feedback without tears. arXiv preprint arXiv.2304.05302 2023 17 171 F. Song. B. Yu. M. Li. H. Yu. F. Huang. Li. H. Wang. Preference rank- ing optimization for human alignment. arXiv preprint arXiv.2306.17492 2023 17 172 H. Liu. C. Sferrazza. P. Abbeel. Languages are rewards. Hindsight fine- tuning using human feedback. arXiv preprint arXiv.2302.02676 2023 17",
    "chunk_index": 471,
    "start_pos": 211658,
    "end_pos": 212081,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 474,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 474,
      "optimized_content_length": 457
    }
  },
  "564": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_472",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "edback. arXiv preprint arXiv.2302.02676 2023 17 173 Bai. S. Kadavath. S. Kundu. A. Askell. J. Kernion. A. Jones. A. Chen. A. Goldie. A. Mirhoseini. C. McKinnon. et al. Constitutional ai. Harm- lessness from ai feedback. arXiv preprint arXiv.2212.08073 2022 17 174 Dubois. X. Li. R. Taori. T. Zhang. I. Gulrajani. J. Ba. C. Guestrin. P. Liang. T. B. Hashimoto. Alpacafarm. simulation frame- work for methods that learn from human feedback. arXiv preprint arXiv.2305.14387 2023 17",
    "chunk_index": 472,
    "start_pos": 212082,
    "end_pos": 212533,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 478
    }
  },
  "565": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_473",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "edback. arXiv preprint arXiv.2305.14387 2023 17 175 C. Si. Z. Gan. Z. Yang. S. Wang. J. Wang. J. Boyd-Graber. L. Wang. Prompting gpt-3 to be reliable. arXiv preprint arXiv.2210.09150 2022 17 176 D. Ganguli. A. Askell. N. Schiefer. T. Liao. K. Lukos ut e. A. Chen. A. Goldie. A. Mirhoseini. C. Olsson. D. Hernandez. et al. The capac- ity for moral self-correction in large language models. arXiv preprint arXiv.2302.07459 2023 17 177 A. Wei. N. Haghtalab. J. Steinhardt. Jailbroken. How does llm safety",
    "chunk_index": 473,
    "start_pos": 212534,
    "end_pos": 213002,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 501
    }
  },
  "566": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_474",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ab. J. Steinhardt. Jailbroken. How does llm safety training fail. arXiv preprint arXiv.2307.02483 2023 17 39",
    "chunk_index": 474,
    "start_pos": 213003,
    "end_pos": 213064,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 112,
      "word_count": 16,
      "optimized_for_embedding": true,
      "original_content_length": 112,
      "optimized_content_length": 108
    }
  },
  "567": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_475",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "il. arXiv preprint arXiv.2307.02483 2023 17 39 --- Page 40 --- 178 D. Ganguli. L. Lovitt. J. Kernion. A. Askell. Bai. S. Kadavath. B. Mann. E. Perez. N. Schiefer. K. Ndousse. et al. Red teaming lan- guage models to reduce harms. Methods. scaling behaviors. and lessons learned. arXiv preprint arXiv.2209.07858 2022 17. 28 179 S. Casper. J. Lin. J. Kwon. G. Culp. D. Hadfield-Menell. Explore. estab- lish. exploit. Red teaming language models from scratch. arXiv preprint arXiv.2306.09442 2023 17",
    "chunk_index": 475,
    "start_pos": 213066,
    "end_pos": 213529,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 495
    }
  },
  "568": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_476",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "cratch. arXiv preprint arXiv.2306.09442 2023 17 180 E. Perez. S. Huang. F. Song. T. Cai. R. Ring. J. Aslanides. A. Glaese. N. McAleese. G. Irving. Red teaming language models with language models. arXiv preprint arXiv.2202.03286 2022 17 181 T. Scialom. T. Chakrabarty. S. Muresan. Fine-tuned language models are continual learners. in. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022. pp. 6107 6122. 17 182 Z. Shi. A. Lipani. Don stop pretraining. make prompt-based fine-",
    "chunk_index": 476,
    "start_pos": 213530,
    "end_pos": 214009,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 516
    }
  },
  "569": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_477",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "i. Don stop pretraining. make prompt-based fine- tuning powerful learner. arXiv preprint arXiv.2305.01711 2023 17 183 H. Gupta. S. A. Sawant. S. Mishra. M. Nakamura. A. Mitra. S. Mashetty. C. Baral. Instruction tuned models are quick learners. arXiv preprint arXiv.2306.05539 2023 17 184 H. Chen. Zhang. Q. Zhang. H. Yang. X. Hu. X. Ma. Yanggong. J. Zhao. Maybe only 0.5 data is needed. preliminary exploration of low training data instruction tuning. arXiv preprint arXiv.2305.09246 2023 17",
    "chunk_index": 477,
    "start_pos": 214010,
    "end_pos": 214476,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 491
    }
  },
  "570": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_478",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tuning. arXiv preprint arXiv.2305.09246 2023 17 185 C. Zhou. P. Liu. P. Xu. S. Iyer. J. Sun. Mao. X. Ma. A. Efrat. P. Yu. L. Yu. et al. Lima. Less is more for alignment. arXiv preprint arXiv.2305.11206 2023 17. 25. 28 186 C. Han. Q. Wang. W. Xiong. Chen. H. Ji. S. Wang. Lm-infinite. Sim- ple on-the-fly length generalization for large language models. arXiv preprint arXiv.2308.16137 2023 17. 18 187 J. Ainslie. T. Lei. M. de Jong. S. Ontan n. S. Brahma. Zemlyan-",
    "chunk_index": 478,
    "start_pos": 214477,
    "end_pos": 214918,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 464
    }
  },
  "571": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_479",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "i. M. de Jong. S. Ontan n. S. Brahma. Zemlyan- skiy. D. Uthus. M. Guo. J. Lee-Thorp. Tay. et al. Colt5. Faster long-range transformers with conditional computation. arXiv preprint arXiv.2303.09752 2023 18 188 J. Ding. S. Ma. L. Dong. X. Zhang. S. Huang. W. Wang. F. Wei. Longnet. Scaling transformers to 1.000.000.000 tokens. arXiv preprint arXiv.2307.02486 2023 18 189 Chen. S. Qian. H. Tang. X. Lai. Z. Liu. S. Han. J. Jia. Longlora. ffi- cient fine-tuning of long-context large language models. arXiv preprint",
    "chunk_index": 479,
    "start_pos": 214919,
    "end_pos": 215405,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 512
    }
  },
  "572": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_480",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "long-context large language models. arXiv preprint arXiv.2309.12307 2023 18 190 N. Ratner. Levine. Belinkov. O. Ram. I. Magar. O. Abend. E. Karpas. A. Shashua. K. Leyton-Brown. Shoham. Parallel context windows for large language models. in. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics olume 1. Long Papers 2023. pp. 6383 6402. 18 191 W. Wang. L. Dong. H. Cheng. X. Liu. X. Yan. J. Gao. F. Wei. Augmenting language models with long-term memory. arXiv preprint",
    "chunk_index": 480,
    "start_pos": 215406,
    "end_pos": 215881,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 502
    }
  },
  "573": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_481",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "guage models with long-term memory. arXiv preprint arXiv.2306.07174 2023 18 192 X. Xu. Z. Gou. W. Wu. Z.-Y Niu. H. Wu. H. Wang. S. Wang. Long time no see. open-domain conversation with long-term persona memory. arXiv preprint arXiv.2203.05797 2022 18 193 S. Borgeaud. A. Mensch. J. Ho ffmann. T. Cai. E. Rutherford. K. Milli- can. G. B. Van Den Driessche. J.-B. Lespiau. B. Damoc. A. Clark. et al. Improving language models by retrieving from trillions of tokens. in. International conference on machine learning. PMLR. 2022. pp. 2206",
    "chunk_index": 481,
    "start_pos": 215882,
    "end_pos": 216379,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 534
    }
  },
  "574": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_482",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ference on machine learning. PMLR. 2022. pp. 2206 2240. 18. 19. 34 194 W. Zhong. L. Guo. Q. Gao. Wang. Memorybank. Enhanc- ing large language models with long-term memory. arXiv preprint arXiv.2305.10250 2023 18 195 N. Shinn. F. Cassano. B. Labash. A. Gopinath. K. Narasimhan. S. Yao. Reflexion. Language agents with verbal reinforcement learning. arXiv preprint arXiv.2303.11366 14 2023 18. 20 196 C. Hu. J. Fu. C. Du. S. Luo. J. Zhao. H. Zhao. Chatdb. Augment- ing llms with databases as their symbolic memory. arXiv preprint",
    "chunk_index": 482,
    "start_pos": 216380,
    "end_pos": 216873,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 527
    }
  },
  "575": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_483",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "databases as their symbolic memory. arXiv preprint arXiv.2306.03901 2023 18 197 Z. Jiang. F. F. Xu. L. Gao. Z. Sun. Q. Liu. J. Dwivedi-Yu. Yang. J. Callan. G. Neubig. Active retrieval augmented generation. arXiv preprint arXiv.2305.06983 2023 18 198 O. Ram. Levine. I. Dalmedigos. D. Muhlgay. A. Shashua. K. Leyton- Brown. Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv.2302.00083 2023 18. 34 199 X. Li. X. Qiu. Mot. Pre-thinking and recalling enable chatgpt to self-",
    "chunk_index": 483,
    "start_pos": 216874,
    "end_pos": 217347,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 497
    }
  },
  "576": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_484",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Pre-thinking and recalling enable chatgpt to self- improve with memory-of-thoughts. arXiv preprint arXiv.2305.05181 2023 18 200 D. Schuurmans. Memory augmented large language models are compu- tationally universal. arXiv preprint arXiv.2301.04589 2023 18 201 A. Modarressi. A. Imani. M. Fayyaz. H. Schu tze. Ret-llm. Towards general read-write memory for large language models. arXiv preprint arXiv.2305.14322 2023 18 202 S. Robertson. H. Zaragoza. et al. The probabilistic relevance frame-",
    "chunk_index": 484,
    "start_pos": 217348,
    "end_pos": 217803,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 490
    }
  },
  "577": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_485",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ragoza. et al. The probabilistic relevance frame- work. Bm25 and beyond. Foundations and Trends in Information Re- trieval 2009 333 389. 18 203 X. Wang. J. Wei. D. Schuurmans. Q. Le. E. Chi. D. Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv.2207.00747 2022 18 204 F. Zhang. B. Chen. Zhang. J. Liu. D. Zan. Mao. J.-G. Lou. W. Chen. Repocoder. Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv.2303.12570 2023 18",
    "chunk_index": 485,
    "start_pos": 217804,
    "end_pos": 218269,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 488
    }
  },
  "578": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_486",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ration. arXiv preprint arXiv.2303.12570 2023 18 205 B. Wang. W. Ping. P. Xu. L. McAfee. Z. Liu. M. Shoeybi. Dong. O. Kuchaiev. B. Li. C. Xiao. et al. Shall we pretrain autoregressive language models with retrieval. comprehensive study. arXiv preprint arXiv.2304.06762 2023 19 206 L. Wang. N. Yang. F. Wei. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv.2307.07164 2023 19 207 J. Liu. D. Shen. Zhang. B. Dolan. L. Carin. W. Chen. What makes",
    "chunk_index": 486,
    "start_pos": 218270,
    "end_pos": 218726,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 481
    }
  },
  "579": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_487",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Zhang. B. Dolan. L. Carin. W. Chen. What makes good in-context examples for gpt-3. arXiv preprint arXiv.2101.06804 2021 19 208 O. Rubin. J. Herzig. J. Berant. Learning to retrieve prompts for in- context learning. arXiv preprint arXiv.2112.08633 2021 19 209 W. Shi. S. Min. M. Yasunaga. M. Seo. R. James. M. Lewis. L. Zettle- moyer. W.-t. Yih. Replug. Retrieval-augmented black-box language models. arXiv preprint arXiv.2301.12652 2023 19 210 O. Rubin. J. Berant. Long-range language modeling with self-retrieval.",
    "chunk_index": 487,
    "start_pos": 218727,
    "end_pos": 219209,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 513
    }
  },
  "580": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_488",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Long-range language modeling with self-retrieval. arXiv preprint arXiv.2306.13421 2023 19 211 K. Guu. K. Lee. Z. Tung. P. Pasupat. M. Chang. Retrieval augmented language model pre-training. in. International conference on machine learning. PMLR. 2020. pp. 3929 3938. 19 212 S. Hofsta tter. J. Chen. K. Raman. H. Zamani. Fid-light. fficient and ef- fective retrieval-augmented text generation. in. Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2023. pp. 1437 1447. 19",
    "chunk_index": 488,
    "start_pos": 219210,
    "end_pos": 219701,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 533
    }
  },
  "581": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_489",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "in Information Retrieval. 2023. pp. 1437 1447. 19 213 M. Komeili. K. Shuster. J. Weston. Internet-augmented dialogue gener- ation. arXiv preprint arXiv.2107.07566 2021 19 214 A. Lazaridou. E. Gribovskaya. W. Stokowiec. N. Grigorev. Internet- augmented language models through few-shot prompting for open- domain question answering. arXiv preprint arXiv.2203.05115 2022 19 215 D. Gao. L. Ji. L. Zhou. K. Q. Lin. J. Chen. Z. Fan. M. Z. Shou. Assist- gpt. general multi-modal assistant that can plan. execute. inspect. and",
    "chunk_index": 489,
    "start_pos": 219702,
    "end_pos": 220185,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 519
    }
  },
  "582": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_490",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "dal assistant that can plan. execute. inspect. and learn. arXiv preprint arXiv.2306.08640 2023 19 216 P. Lu. B. Peng. H. Cheng. M. Galley. K.-W. Chang. N. Wu. S.-C. Zhu. J. Gao. Chameleon. Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv.2304.09842 2023 19. 20. 23 217 B. Paranjape. S. Lundberg. S. Singh. H. Hajishirzi. L. Zettlemoyer. M. T. Ribeiro. Art. Automatic multi-step reasoning and tool-use for large lan- guage models. arXiv preprint arXiv.2303.09014 2023 19",
    "chunk_index": 490,
    "start_pos": 220186,
    "end_pos": 220659,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 507
    }
  },
  "583": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_491",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2303.09014 2023 19 218 C.-Y Hsieh. S.-A. Chen. C.-L. Li. Fujii. A. Ratner. C.-Y Lee. R. Kr- ishna. T. Pfister. Tool documentation enables zero-shot tool-usage with large language models. arXiv preprint arXiv.2308.00675 2023 19 219 Song. W. Xiong. D. Zhu. C. Li. K. Wang. Tian. S. Li. Restgpt. Connecting large language models with real-world applications via rest- ful apis. arXiv preprint arXiv.2306.06624 2023 19 220 S. Hao. T. Liu. Z. Wang. Z. Hu. Toolkengpt. Augmenting frozen lan-",
    "chunk_index": 491,
    "start_pos": 220660,
    "end_pos": 221154,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 514
    }
  },
  "584": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_492",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Z. Wang. Z. Hu. Toolkengpt. Augmenting frozen lan- guage models with massive tools via tool embeddings. arXiv preprint arXiv.2305.11554 2023 19 221 S. G. Patil. T. Zhang. X. Wang. J. E. Gonzalez. Gorilla. Large language model connected with massive apis. arXiv preprint arXiv.2305.15334 2023 19 222 Q. Xu. F. Hong. B. Li. C. Hu. Z. Chen. J. Zhang. On the tool manipu- lation capability of open-source large language models. arXiv preprint arXiv.2305.16504 2023 19",
    "chunk_index": 492,
    "start_pos": 221155,
    "end_pos": 221580,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 476,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 476,
      "optimized_content_length": 463
    }
  },
  "585": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_493",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2305.16504 2023 19 223 Qin. S. Liang. Ye. K. Zhu. L. Yan. Lu. Lin. X. Cong. X. Tang. B. Qian. et al. Toolllm. Facilitating large language models to master 16000 real-world apis. arXiv preprint arXiv.2307.16789 2023 19. 40",
    "chunk_index": 493,
    "start_pos": 221581,
    "end_pos": 221806,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 276,
      "word_count": 49,
      "optimized_for_embedding": true,
      "original_content_length": 276,
      "optimized_content_length": 250
    }
  },
  "586": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_494",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "is. arXiv preprint arXiv.2307.16789 2023 19. 40 --- Page 41 --- 20 224 Shen. K. Song. X. Tan. D. Li. W. Lu. Zhuang. Hugginggpt. Solv- ing ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv.2303.17580 2023 19. 20. 33 225 Liang. C. Wu. T. Song. W. Wu. Xia. Liu. Ou. S. Lu. L. Ji. S. Mao. et al. Taskmatrix. ai. Completing tasks by connecting foun- dation models with millions of apis. arXiv preprint arXiv.2303.16434 2023 19",
    "chunk_index": 494,
    "start_pos": 221808,
    "end_pos": 222241,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 484,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 484,
      "optimized_content_length": 446
    }
  },
  "587": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_495",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "apis. arXiv preprint arXiv.2303.16434 2023 19 226 D. Suri s. S. Menon. C. ondrick. Vipergpt. Visual inference via python execution for reasoning. arXiv preprint arXiv.2303.08128 2023 20 227 A. Maedche. S. Morana. S. Schacht. D. Werth. J. Krumeich. Advanced user assistance systems. Business Information Systems Engineering 58 2016 367 370. 20 228 M. Campbell. A. J. Hoane Jr. F.-h. Hsu. Deep blue. Artificial intelligence 134 1-2 2002 57 83. 20 229 S. Hong. X. Zheng. J. Chen. Cheng. J. Wang. C. Zhang. Z. Wang.",
    "chunk_index": 495,
    "start_pos": 222242,
    "end_pos": 222731,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 511
    }
  },
  "588": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_496",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "g. J. Chen. Cheng. J. Wang. C. Zhang. Z. Wang. S. K. S. Yau. Z. Lin. L. Zhou. et al. Metagpt. Meta programming for multi-agent collaborative framework. arXiv preprint arXiv.2308.00352 2023 20 230 Z. Xi. W. Chen. X. Guo. W. He. Ding. B. Hong. M. Zhang. J. Wang. S. Jin. E. Zhou. et al. The rise and potential of large language model based agents. survey. arXiv preprint arXiv.2309.07864 2023 20 231 L. Wang. C. Ma. X. Feng. Z. Zhang. H. Yang. J. Zhang. Z. Chen. J. Tang.",
    "chunk_index": 496,
    "start_pos": 222732,
    "end_pos": 223172,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 469
    }
  },
  "589": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_497",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng. Z. Zhang. H. Yang. J. Zhang. Z. Chen. J. Tang. X. Chen. Lin. et al. survey on large language model based au- tonomous agents. arXiv preprint arXiv.2308.11432 2023 20 232 W. Huang. P. Abbeel. D. Pathak. I. Mordatch. Language models as zero- shot planners. Extracting actionable knowledge for embodied agents. in. International Conference on Machine Learning. PMLR. 2022. pp. 9118 9147. 20 233 S. Hao. Gu. H. Ma. J. J. Hong. Z. Wang. D. Z. Wang. Z. Hu. Reason- ing with language model is planning with world model. arXiv preprint",
    "chunk_index": 497,
    "start_pos": 223173,
    "end_pos": 223671,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 95,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 531
    }
  },
  "590": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_498",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "model is planning with world model. arXiv preprint arXiv.2305.14992 2023 20. 33 234 W. Yao. S. Heinecke. J. C. Niebles. Z. Liu. Feng. L. Xue. R. Murthy. Z. Chen. J. Zhang. D. Arpit. et al. Retroformer. Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv.2308.02151 2023 20. 33 235 W. Huang. F. Xia. T. Xiao. H. Chan. J. Liang. P. Florence. A. Zeng. J. Tompson. I. Mordatch. Chebotar. P. Sermanet. T. Jackson. N. Brown. L. Luu. S. Levine. K. Hausman. brian ichter. Inner mono-",
    "chunk_index": 498,
    "start_pos": 223672,
    "end_pos": 224155,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 515
    }
  },
  "591": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_499",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "S. Levine. K. Hausman. brian ichter. Inner mono- logue. Embodied reasoning through planning with language models. in. 6th Annual Conference on Robot Learning. 2022. URL https. openreview.net forum.id 3R3Pz5i0tye 20 236 C. Jin. W. Tan. J. Yang. B. Liu. R. Song. L. Wang. J. Fu. Alphablock. Embodied finetuning for vision-language reasoning in robot manipula- tion. arXiv preprint arXiv.2305.18898 2023 20. 33 237 I. Singh. Blukis. A. Mousavian. A. Goyal. D. Xu. J. Tremblay. D. Fox.",
    "chunk_index": 499,
    "start_pos": 224156,
    "end_pos": 224600,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 481
    }
  },
  "592": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_500",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Mousavian. A. Goyal. D. Xu. J. Tremblay. D. Fox. J. Thomason. A. Garg. Progprompt. Generating situated robot task plans using large language models. in. 2023 IEEE International Conference on Robotics and Automation ICRA IEEE. 2023. pp. 11523 11530. 20. 33 238 W. Yu. N. Gileadi. C. Fu. S. Kirmani. K.-H. Lee. M. G. Arenas. H.-T. L. Chiang. T. Erez. L. Hasenclever. J. Humplik. et al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv.2306.08647 2023 20",
    "chunk_index": 500,
    "start_pos": 224601,
    "end_pos": 225033,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 483,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 483,
      "optimized_content_length": 472
    }
  },
  "593": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_501",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "thesis. arXiv preprint arXiv.2306.08647 2023 20 239 X. Tang. A. Zou. Z. Zhang. Zhao. X. Zhang. A. Cohan. M. Gerstein. Medagents. Large language models as collaborators for zero-shot med- ical reasoning. arXiv preprint arXiv.2311.10537 2023 20 240 A. Brohan. Chebotar. C. Finn. K. Hausman. A. Herzog. D. Ho. J. Ibarz. A. Irpan. E. Jang. R. Julian. et al. Do as can. not as say. Grounding language in robotic ffordances. in. Conference on Robot Learning. PMLR. 2023. pp. 287 318. 20. 33",
    "chunk_index": 501,
    "start_pos": 225034,
    "end_pos": 225492,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 484
    }
  },
  "594": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_502",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "on Robot Learning. PMLR. 2023. pp. 287 318. 20. 33 241 H. Ha. P. Florence. S. Song. Scaling up and distilling down. Language- guided robot skill acquisition. arXiv preprint arXiv.2307.14535 2023 20 242 A. Rajvanshi. K. Sikka. X. Lin. B. Lee. H.-P. Chiu. A. Velasquez. Say- nav. Grounding large language models for dynamic planning to navi- gation in new environments. arXiv preprint arXiv.2309.04077 2023 20 243 C. H. Song. J. Wu. C. Washington. B. M. Sadler. W.-L. Chao. Su.",
    "chunk_index": 502,
    "start_pos": 225493,
    "end_pos": 225933,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 475
    }
  },
  "595": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_503",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "C. Washington. B. M. Sadler. W.-L. Chao. Su. Llm-planner. Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv.2212.04088 2022 20 244 S. Dorbala. J. F. Mullen Jr. D. Manocha. Can an embodied agent findyour cat-shaped mug llm-based zero-shot object navigation. arXiv preprint arXiv.2303.03480 2023 20 245 C. Huang. O. Mees. A. Zeng. W. Burgard. Visual language maps for robot navigation. in. 2023 IEEE International Conference on Robotics",
    "chunk_index": 503,
    "start_pos": 225934,
    "end_pos": 226386,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 503,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 503,
      "optimized_content_length": 480
    }
  },
  "596": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_504",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "in. 2023 IEEE International Conference on Robotics and Automation ICRA IEEE. 2023. pp. 10608 10615. 20 246 Ding. X. Zhang. C. Paxton. S. Zhang. Task and motion planning with large language models for object rearrangement. arXiv preprint arXiv.2303.06247 2023 20. 33 247 X. Liu. Zheng. Z. Du. M. Ding. Qian. Z. Yang. J. Tang. Gpt under- stands. too. arXiv preprint arXiv.2103.10385 2021 20. 21 248 G. Chen. F. Liu. Z. Meng. S. Liang. Revisiting parameter-e fficient tun-",
    "chunk_index": 504,
    "start_pos": 226387,
    "end_pos": 226832,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 469
    }
  },
  "597": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_505",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng. S. Liang. Revisiting parameter-e fficient tun- ing. Are we really there yet. arXiv preprint arXiv.2202.07962 2022 20 249 Wang. S. Mukherjee. X. Liu. J. Gao. A. H. Awadallah. J. Gao. Adamix. Mixture-of-adapter for parameter-e fficient tuning of large lan- guage models. arXiv preprint arXiv.2205.12410 2022 4. 20 250 E. J. Hu. Shen. P. Wallis. Z. Allen-Zhu. Li. S. Wang. L. Wang. W. Chen. Lora. Low-rank adaptation of large language models. arXiv preprint arXiv.2106.09685 2021 21. 22. 23",
    "chunk_index": 505,
    "start_pos": 226833,
    "end_pos": 227304,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 491
    }
  },
  "598": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_506",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "arXiv preprint arXiv.2106.09685 2021 21. 22. 23 251 X. Liu. K. Ji. Fu. W. Tam. Z. Du. Z. Yang. J. Tang. P-tuning. Prompt tuning can be comparable to fine-tuning across scales and tasks. in. Pro- ceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics olume 2. Short Papers 2022. pp. 61 68. 21 252 A. Razdaibiedina. Mao. R. Hou. M. Khabsa. M. Lewis. A. Almahairi. Progressive prompts. Continual learning for language models. arXiv preprint arXiv.2301.12314 2023 21",
    "chunk_index": 506,
    "start_pos": 227305,
    "end_pos": 227771,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 494
    }
  },
  "599": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_507",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2301.12314 2023 21 253 Z.-R. Zhang. C. Tan. H. Xu. C. Wang. J. Huang. S. Huang. To- wards adaptive prefix tuning for parameter-e fficient language model fine-tuning. arXiv preprint arXiv.2305.15212 2023 21 254 E. B. Zaken. S. Ravfogel. Goldberg. Bitfit. Simple parameter- efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv.2106.10199 2021 21 255 T. Dettmers. M. Lewis. Belkada. L. Zettlemoyer. Llm. int8",
    "chunk_index": 507,
    "start_pos": 227772,
    "end_pos": 228218,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 470
    }
  },
  "600": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_508",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Lewis. Belkada. L. Zettlemoyer. Llm. int8 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv.2208.07339 2022 21. 22 256 E. Frantar. S. Ashkboos. T. Hoefler. D. Alistarh. Gptq. Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv.2210.17323 2022 21 257 X. Wei. Zhang. Li. X. Zhang. R. Gong. J. Guo. X. Liu. Outlier sup- pression Accurate quantization of large language models by equiva-",
    "chunk_index": 508,
    "start_pos": 228219,
    "end_pos": 228653,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 484,
      "optimized_content_length": 455
    }
  },
  "601": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_509",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "quantization of large language models by equiva- lent and optimal shifting and scaling. arXiv preprint arXiv.2304.09145 2023 21 258 E. Frantar. D. Alistarh. Optimal brain compression. framework for accurate post-training quantization and pruning. Advances in Neural In- formation Processing Systems 35 2022 4475 4488. 21 259 C. Lee. J. Jin. T. Kim. H. Kim. E. Park. Owq. Lessons learned from ac- tivation outliers for weight quantization in large language models. arXiv preprint arXiv.2306.02272 2023 21",
    "chunk_index": 509,
    "start_pos": 228654,
    "end_pos": 229122,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 503
    }
  },
  "602": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_510",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2306.02272 2023 21 260 S. J. Kwon. J. Kim. J. Bae. K. M. Yoo. J.-H. Kim. B. Park. B. Kim. J.- W. Ha. N. Sung. D. Lee. Alphatuning. Quantization-aware parameter- efficient adaptation of large-scale pre-trained language models. arXiv preprint arXiv.2210.03858 2022 21 261 T. Dettmers. A. Pagnoni. A. Holtzman. L. Zettlemoyer. Qlora. fficient finetuning of quantized llms. arXiv preprint arXiv.2305.14314 2023 21. 22 262 Z. Liu. B. Oguz. C. Zhao. E. Chang. P. Stock. Mehdad. Shi. R. Kr-",
    "chunk_index": 510,
    "start_pos": 229123,
    "end_pos": 229609,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 512
    }
  },
  "603": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_511",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "o. E. Chang. P. Stock. Mehdad. Shi. R. Kr- ishnamoorthi. Chandra. Llm-qat. Data-free quantization aware train- ing for large language models. arXiv preprint arXiv.2305.17888 2023 21. 22 263 Guo. A. Yao. H. Zhao. Chen. Network sketching. Exploiting bi- nary structure in deep cnns. in. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. pp. 5955 5963. 21 264 J. Kim. J. H. Lee. S. Kim. J. Park. K. M. Yoo. S. J. Kwon. D. Lee.",
    "chunk_index": 511,
    "start_pos": 229610,
    "end_pos": 230044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 458
    }
  },
  "604": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_512",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "e. S. Kim. J. Park. K. M. Yoo. S. J. Kwon. D. Lee. Memory-e fficient fine-tuning of compressed large language models via sub-4-bit integer quantization. arXiv preprint arXiv.2305.14152 2023 22 265 M. Sun. Z. Liu. A. Bair. J. Z. Kolter. simple and ffective pruning approach for large language models. arXiv preprint arXiv.2306.11695 2023 22 266 Z. Wang. J. Wohlwend. T. Lei. Structured pruning of large language models. arXiv preprint arXiv.1910.04732 2019 22 41",
    "chunk_index": 512,
    "start_pos": 230045,
    "end_pos": 230472,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 478,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 478,
      "optimized_content_length": 461
    }
  },
  "605": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_513",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "els. arXiv preprint arXiv.1910.04732 2019 22 41 --- Page 42 --- 267 L. Yin. Wu. Z. Zhang. C.-Y Hsieh. Wang. Jia. M. Pechenizkiy. Liang. Z. Wang. S. Liu. Outlier weighed layerwise sparsity owl missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv.2310.05175 2023 22 268 C. Tao. L. Hou. H. Bai. J. Wei. X. Jiang. Q. Liu. P. Luo. N. Wong. Structured pruning for fficient generative pre-trained language models. in. Findings of the Association for Computational Linguistics. ACL",
    "chunk_index": 513,
    "start_pos": 230474,
    "end_pos": 230955,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 497
    }
  },
  "606": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_514",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "the Association for Computational Linguistics. ACL 2023. 2023. pp. 10880 10895. 22 269 J.-B. Alayrac. J. Donahue. P. Luc. A. Miech. I. Barr. Hasson. K. Lenc. A. Mensch. K. Millican. M. Reynolds. et al. Flamingo. visual lan- guage model for few-shot learning. Advances in Neural Information Pro- cessing Systems 35 2022 23716 23736. 22 270 J. Li. D. Li. S. Savarese. S. Hoi. Blip-2. Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv.2301.12597 2023 22",
    "chunk_index": 514,
    "start_pos": 230956,
    "end_pos": 231438,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 517
    }
  },
  "607": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_515",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2301.12597 2023 22 271 H. Liu. C. Li. Q. Wu. J. Lee. Visual instruction tuning. arXiv preprint arXiv.2304.08485 2023 22 272 K. Li. He. Wang. Li. W. Wang. P. Luo. Wang. L. Wang. Qiao. Videochat. Chat-centric video understanding. arXiv preprint arXiv.2305.06355 2023 22 273 M. Maaz. H. Rasheed. S. Khan. F. S. Khan. Video-chatgpt. Towards de- tailed video understanding via large vision and language models. arXiv preprint arXiv.2306.05424 2023 22",
    "chunk_index": 515,
    "start_pos": 231439,
    "end_pos": 231904,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 474
    }
  },
  "608": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_516",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2306.05424 2023 22 274 H. Zhang. X. Li. L. Bing. Video-llama. An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv.2306.02858 2023 22 275 X. Mei. C. Meng. H. Liu. Q. Kong. T. Ko. C. Zhao. M. D. Plumbley. Zou. W. Wang. Wavcaps. chatgpt-assisted weakly-labelled au- dio captioning dataset for audio-language multimodal research. arXiv preprint arXiv.2303.17395 2023 22 276 C. Lyu. M. Wu. L. Wang. X. Huang. B. Liu. Z. Du. S. Shi. Z. Tu. Macaw-",
    "chunk_index": 516,
    "start_pos": 231905,
    "end_pos": 232387,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 512
    }
  },
  "609": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_517",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ng. X. Huang. B. Liu. Z. Du. S. Shi. Z. Tu. Macaw- llm. Multi-modal language modeling with image. audio. video. and text integration. arXiv preprint arXiv.2306.09093 2023 22 277 D. Zhu. J. Chen. X. Shen. X. Li. M. Elhoseiny. Minigpt-4. Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv.2304.10592 2023 22 278 A. Dosovitskiy. L. Beyer. A. Kolesnikov. D. Weissenborn. X. Zhai. T. Unterthiner. M. Dehghani. M. Minderer. G. Heigold. S. Gelly. et al.",
    "chunk_index": 517,
    "start_pos": 232388,
    "end_pos": 232841,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 493
    }
  },
  "610": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_518",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "hghani. M. Minderer. G. Heigold. S. Gelly. et al. An image is worth 16x16 words. Transformers for image recognition at scale. arXiv preprint arXiv.2010.11929 2020 22 279 W. Dai. J. Li. D. Li. A. M. H. Tiong. J. Zhao. W. Wang. B. Li. P. Fung. S. Hoi. Instructblip. Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv.2305.06500 2023 22 280 Z. Xu. Shen. L. Huang. Multiinstruct. Improving multi-modal zero- shot learning via instruction tuning. arXiv preprint arXiv.2212.10773 2022 22",
    "chunk_index": 518,
    "start_pos": 232842,
    "end_pos": 233332,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 523
    }
  },
  "611": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_519",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tuning. arXiv preprint arXiv.2212.10773 2022 22 281 Z. Zhao. L. Guo. T. Yue. S. Chen. S. Shao. X. Zhu. Z. Yuan. J. Liu. Chatbridge. Bridging modalities with large language model as lan- guage catalyst. arXiv preprint arXiv.2305.16103 2023 22 282 L. Li. Yin. S. Li. L. Chen. P. Wang. S. Ren. M. Li. Yang. J. Xu. X. Sun. et al. M3 it. large-scale dataset towards multi-modal multi- lingual instruction tuning. arXiv preprint arXiv.2306.04387 2023 22 283 R. Pi. J. Gao. S. Diao. R. Pan. H. Dong. J. Zhang. L. Yao. J. Han.",
    "chunk_index": 519,
    "start_pos": 233333,
    "end_pos": 233828,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 98,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 518
    }
  },
  "612": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_520",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Diao. R. Pan. H. Dong. J. Zhang. L. Yao. J. Han. H. Xu. L. K. T. Zhang. Detgpt. Detect what you need via reasoning. arXiv preprint arXiv.2305.14167 2023 22 284 G. Luo. Zhou. T. Ren. S. Chen. X. Sun. R. Ji. Cheap and quick. Efficient vision-language instruction tuning for large language models. arXiv preprint arXiv.2305.15023 2023 22 285 R. Zhang. J. Han. A. Zhou. X. Hu. S. Yan. P. Lu. H. Li. P. Gao. Qiao. Llama-adapter. fficient fine-tuning of language models with zero-init",
    "chunk_index": 520,
    "start_pos": 233829,
    "end_pos": 234278,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 478
    }
  },
  "613": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_521",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ient fine-tuning of language models with zero-init attention. arXiv preprint arXiv.2303.16199 2023 22 286 A. Radford. J. W. Kim. T. Xu. G. Brockman. C. McLeavey. I. Sutskever. Robust speech recognition via large-scale weak supervision. in. Inter- national Conference on Machine Learning. PMLR. 2023. pp. 28492 28518. 22 287 Z. Zhang. A. Zhang. M. Li. H. Zhao. G. Karypis. A. Smola. Multi- modal chain-of-thought reasoning in language models. arXiv preprint arXiv.2302.00923 2023 23",
    "chunk_index": 521,
    "start_pos": 234279,
    "end_pos": 234720,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 481
    }
  },
  "614": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_522",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2302.00923 2023 23 288 J. Ge. H. Luo. S. Qian. Gan. J. Fu. S. Zhan. Chain of thought prompt tuning in vision language models. arXiv preprint arXiv.2304.07919 2023 23 289 C. Wu. S. Yin. W. Qi. X. Wang. Z. Tang. N. Duan. Visual chatgpt. Talk- ing. drawing and editing with visual foundation models. arXiv preprint arXiv.2303.04671 2023 23 290 Z. Yang. L. Li. J. Wang. K. Lin. E. Azarnasab. F. Ahmed. Z. Liu. C. Liu. M. Zeng. L. Wang. Mm-react. Prompting chatgpt for multimodal rea-",
    "chunk_index": 522,
    "start_pos": 234721,
    "end_pos": 235196,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 508
    }
  },
  "615": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_523",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "g. Mm-react. Prompting chatgpt for multimodal rea- soning and action. arXiv preprint arXiv.2303.11381 2023 23 291 T. Wang. J. Zhang. J. Fei. Ge. H. Zheng. Tang. Z. Li. M. Gao. S. Zhao. Shan. et al. Caption anything. Interactive image descrip- tion with diverse multimodal controls. arXiv preprint arXiv.2305.02677 2023 23 292 X. Zhu. R. Zhang. B. He. Z. Zeng. S. Zhang. P. Gao. Pointclip v2. Adapting clip for powerful 3d open-world learning. arXiv preprint arXiv.2211.11682 2022 23",
    "chunk_index": 523,
    "start_pos": 235197,
    "end_pos": 235654,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 482
    }
  },
  "616": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_524",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "arning. arXiv preprint arXiv.2211.11682 2022 23 293 T. Gupta. A. Kembhavi. Visual programming. Compositional visual rea- soning without training. in. Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. 2023. pp. 14953 14962. 23 294 P. Gao. Z. Jiang. H. You. P. Lu. S. C. Hoi. X. Wang. H. Li. Dynamic fusion with intra-and inter-modality attention flow for visual question answering. in. Proceedings of the IEEE CVF conference on computer vision and pattern recognition. 2019. pp. 6639 6648. 23",
    "chunk_index": 524,
    "start_pos": 235655,
    "end_pos": 236137,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 524
    }
  },
  "617": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_525",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "and pattern recognition. 2019. pp. 6639 6648. 23 295 Z. Yu. J. Yu. Cui. D. Tao. Q. Tian. Deep modular co-attention net- works for visual question answering. in. Proceedings of the IEEE CVF conference on computer vision and pattern recognition. 2019. pp. 6281 6290. 23 296 H. You. R. Sun. Z. Wang. L. Chen. G. Wang. H. A. Ayyubi. K.- W. Chang. S.-F. Chang. Idealgpt. Iteratively decomposing vision and language reasoning via large language models. arXiv preprint arXiv.2305.14985 2023 23",
    "chunk_index": 525,
    "start_pos": 236138,
    "end_pos": 236588,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 486
    }
  },
  "618": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_526",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2305.14985 2023 23 297 R. Zhang. X. Hu. B. Li. S. Huang. H. Deng. Qiao. P. Gao. H. Li. Prompt. generate. then cache. Cascade of foundation models makes strong few-shot learners. in. Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. 2023. pp. 15211 15222. 23 298 T. Q. Nguyen. J. Salazar. Transformers without tears. Improving the normalization of self-attention. CoRR abs 1910.05895 2019 24 299 Liu. M. Ott. N. Goyal. J. Du. M. Joshi. D. Chen. O. Levy. M. Lewis.",
    "chunk_index": 526,
    "start_pos": 236589,
    "end_pos": 237084,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 524
    }
  },
  "619": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_527",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "oyal. J. Du. M. Joshi. D. Chen. O. Levy. M. Lewis. L. Zettlemoyer. Stoyanov. Roberta. robustly optimized bert pre- training approach. arXiv preprint arXiv.1907.11692 2019 24. 30 300 X. Geng. A. Gudibande. H. Liu. E. Wallace. P. Abbeel. S. Levine. D. Song. Koala. dialogue model for academic research. Blog post April 2023 URL https. bair.berkeley.edu blog 2023 04 03 koala 25 301 L. Gao. S. Biderman. S. Black. L. Golding. T. Hoppe. C. Foster. J. Phang. H. He. A. Thite. N. Nabeshima. et al. The pile. An",
    "chunk_index": 527,
    "start_pos": 237085,
    "end_pos": 237559,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 504
    }
  },
  "620": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_528",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "He. A. Thite. N. Nabeshima. et al. The pile. An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv.2101.00027 2020 28. 30 302 H. Laurenc on. L. Saulnier. T. Wang. C. Akiki. A. Villanova del Moral. T. Le Scao. L. on Werra. C. Mou. E. Gonza lez Ponferrada. H. Nguyen. et al. The bigscience roots corpus. 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems 35 2022 31809 31826. 28 303 Wikipedia. URL https. en.wikipedia.org wiki Main_Page 28",
    "chunk_index": 528,
    "start_pos": 237560,
    "end_pos": 238023,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 498
    }
  },
  "621": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_529",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ia. URL https. en.wikipedia.org wiki Main_Page 28 304 Together Computer. Redpajama. An open source recipe to reproduce llama training dataset Apr. 2023 URL https. github.com togethercomputer RedPajama-Data 28 305 O. Honovich. T. Scialom. O. Levy. T. Schick. Unnatural instructions. Tuning language models with almost no human labor. arXiv preprint arXiv.2212.09689 2022 28 306 Bai. A. Jones. K. Ndousse. A. Askell. A. Chen. N. DasSarma. D. Drain. S. Fort. D. Ganguli. T. Henighan. et al. Training helpful and",
    "chunk_index": 529,
    "start_pos": 238024,
    "end_pos": 238505,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 508
    }
  },
  "622": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_530",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nguli. T. Henighan. et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv.2204.05862 2022 28 307 D. Hendrycks. C. Burns. S. Basart. A. Zou. M. Mazeika. D. Song. J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv.2009.03300 2020 26. 29 308 A. Srivastava. A. Rastogi. A. Rao. A. A. M. Shoeb. A. Abid. A. Fisch. A. R. Brown. A. Santoro. A. Gupta. A. Garriga-Alonso. et al. Beyond 42",
    "chunk_index": 530,
    "start_pos": 238506,
    "end_pos": 238948,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 479
    }
  },
  "623": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_531",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ro. A. Gupta. A. Garriga-Alonso. et al. Beyond 42 --- Page 43 --- the imitation game. Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv.2206.04615 2022 26. 29 309 A. Wang. A. Singh. J. Michael. F. Hill. O. Levy. S. R. Bowman. Glue. multi-task benchmark and analysis platform for natural language un- derstanding. arXiv preprint arXiv.1804.07461 2018 26. 29 310 Yao. Q. Dong. J. Guan. B. Cao. Z. Zhang. C. Xiao. X. Wang. F. Qi.",
    "chunk_index": 531,
    "start_pos": 238950,
    "end_pos": 239380,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 481,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 464
    }
  },
  "624": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_532",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Guan. B. Cao. Z. Zhang. C. Xiao. X. Wang. F. Qi. J. Bao. J. Nie. et al. Cuge. chinese language understanding and gen- eration evaluation benchmark. arXiv preprint arXiv.2112.13610 2021 29 311 L. Xu. H. Hu. X. Zhang. L. Li. C. Cao. Li. Xu. K. Sun. D. Yu. C. Yu. et al. Clue. chinese language understanding evaluation bench- mark. arXiv preprint arXiv.2004.05986 2020 29 312 L. Xu. X. Lu. C. Yuan. X. Zhang. H. Xu. H. Yuan. G. Wei. X. Pan. X. Tian. L. Qin. et al. Fewclue. chinese few-shot learning evaluation",
    "chunk_index": 532,
    "start_pos": 239381,
    "end_pos": 239866,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 100,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 507
    }
  },
  "625": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_533",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Fewclue. chinese few-shot learning evaluation benchmark. arXiv preprint arXiv.2107.07498 2021 29 313 E. M. Smith. M. Williamson. K. Shuster. J. Weston. .-L. Boureau. Can you put it all together. Evaluating conversational agents ability to blend skills. arXiv preprint arXiv.2004.08449 2020 29 314 P. Liang. R. Bommasani. T. Lee. D. Tsipras. D. Soylu. M. Yasunaga. Zhang. D. Narayanan. Wu. A. Kumar. et al. Holistic evaluation of language models. arXiv preprint arXiv.2211.09110 2022 29",
    "chunk_index": 533,
    "start_pos": 239867,
    "end_pos": 240331,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 515,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 485
    }
  },
  "626": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_534",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2211.09110 2022 29 315 S. Park. J. Moon. S. Kim. W. I. Cho. J. Han. J. Park. C. Song. J. Kim. Song. T. Oh. et al. Klue. Korean language understanding evaluation. arXiv preprint arXiv.2105.09680 2021 29 316 S. Reddy. D. Chen. C. D. Manning. Coqa. conversational question answering challenge. Transactions of the Association for Computational Linguistics 2019 249 266. 27. 29 317 M. T. Pilehvar. J. Camacho-Collados. Wic. 10.000 example",
    "chunk_index": 534,
    "start_pos": 240332,
    "end_pos": 240767,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 486,
      "optimized_content_length": 463
    }
  },
  "627": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_535",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Pilehvar. J. Camacho-Collados. Wic. 10.000 example pairs for evaluating context-sensitive representations. arXiv preprint arXiv.1808.09121 2018 27. 29 318 S. Merity. C. Xiong. J. Bradbury. R. Socher. Pointer sentinel mixture models. arXiv preprint arXiv.1609.07843 2016 28. 29 319 J. W. Rae. A. Potapenko. S. M. Jayakumar. T. P. Lillicrap. Compres- sive transformers for long-range sequence modelling. arXiv preprint arXiv.1911.05507 2019 28. 29 320 X. Liu. Q. Chen. C. Deng. H. Zeng. J. Chen. D. Li. B. Tang. Lcqmc.",
    "chunk_index": 535,
    "start_pos": 240768,
    "end_pos": 241252,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 516
    }
  },
  "628": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_536",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Deng. H. Zeng. J. Chen. D. Li. B. Tang. Lcqmc. large-scale chinese question matching corpus. in. Proceedings of the 27th international conference on computational linguistics. 2018. pp. 1952 1962. 28. 29 321 S. Iyer. N. Dandekar. K. Csernai. First quora dataset re- lease. Question pairs. https. quoradata.quora.com First-Quora-Dataset-Release-Question-Pairs 29 322 R. Rudinger. J. Naradowsky. B. Leonard. B. Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv.1804.09301 2018 29",
    "chunk_index": 536,
    "start_pos": 241253,
    "end_pos": 241715,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 498
    }
  },
  "629": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_537",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "lution. arXiv preprint arXiv.1804.09301 2018 29 323 M.-C. De Marne ffe. M. Simons. J. Tonhauser. The commitmentbank. In- vestigating projection in naturally occurring discourse. in. proceedings of Sinn und Bedeutung. ol. 23. 2019. pp. 107 124. 29 324 Z. Li. N. Ding. Z. Liu. H. Zheng. Shen. Chinese relation extraction with multi-grained information and external linguistic knowledge. in. Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics. 2019. pp. 4377 4386. 29",
    "chunk_index": 537,
    "start_pos": 241716,
    "end_pos": 242181,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 503
    }
  },
  "630": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_538",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "mpu- tational Linguistics. 2019. pp. 4377 4386. 29 325 J. Xu. J. Wen. X. Sun. Q. Su. discourse-level named entity recognition and relation extraction dataset for chinese literature text. arXiv preprint arXiv.1711.07010 2017 29 326 J. Chen. Q. Chen. X. Liu. H. Yang. D. Lu. B. Tang. The bq corpus. large-scale domain-specific chinese corpus for sentence semantic equiv- alence identification. in. Proceedings of the 2018 conference on empiri- cal methods in natural language processing. 2018. pp. 4946 4951. 29",
    "chunk_index": 538,
    "start_pos": 242182,
    "end_pos": 242651,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 509
    }
  },
  "631": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_539",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tural language processing. 2018. pp. 4946 4951. 29 327 B. Liu. D. Niu. H. Wei. J. Lin. He. K. Lai. Xu. Matching arti- cle pairs with graphical decomposition and convolutions. arXiv preprint arXiv.1802.07459 2018 29 328 P. Li. W. Li. Z. He. X. Wang. Cao. J. Zhou. W. Xu. Dataset and neu- ral recurrent sequence labeling model for open-domain factoid question answering. arXiv preprint arXiv.1607.06275 2016 29 329 N. Peng. M. Dredze. Named entity recognition for chinese social media",
    "chunk_index": 539,
    "start_pos": 242652,
    "end_pos": 243107,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 482
    }
  },
  "632": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_540",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Named entity recognition for chinese social media with jointly trained embeddings. in. Proceedings of the 2015 conference on empirical methods in natural language processing. 2015. pp. 548 554. 29 330 W. Ling. D. Yogatama. C. Dyer. P. Blunsom. Program induction by ratio- nale generation. Learning to solve and explain algebraic word problems. arXiv preprint arXiv.1705.04146 2017 29 331 R. Weischedel. S. Pradhan. L. Ramshaw. M. Palmer. N. Xue. M. Mar- cus. A. Taylor. C. Greenberg. E. Hovy. R. Belvin. et al. Ontonotes re-",
    "chunk_index": 540,
    "start_pos": 243108,
    "end_pos": 243590,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 524
    }
  },
  "633": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_541",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "eenberg. E. Hovy. R. Belvin. et al. Ontonotes re- lease 4.0. LDC2011T03. Philadelphia. Penn. Linguistic Data Consor- tium 2011 29 332 D. Vilares. C. Go mez-Rodri guez. Head-qa. healthcare dataset for complex reasoning. arXiv preprint arXiv.1906.04701 2019 29 333 S. L. Blodgett. L. Green. B. Connor. Demographic dialectal variation in social media. case study of african-american english. arXiv preprint arXiv.1608.08868 2016 29 334 N. Mostafazadeh. N. Chambers. X. He. D. Parikh. D. Batra. L. Van-",
    "chunk_index": 541,
    "start_pos": 243591,
    "end_pos": 244059,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 498
    }
  },
  "634": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_542",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "N. Chambers. X. He. D. Parikh. D. Batra. L. Van- derwende. P. Kohli. J. Allen. corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv.1604.01696 2016 28. 29 335 D. Paperno. G. Kruszewski. A. Lazaridou. Q. N. Pham. R. Bernardi. S. Pezzelle. M. Baroni. G. Boleda. R. Ferna ndez. The lambada dataset. Word prediction requiring broad discourse context. arXiv preprint arXiv.1606.06031 2016 28. 29 336 B. Hu. Q. Chen. F. Zhu. Lcsts. large scale chinese short text summa-",
    "chunk_index": 542,
    "start_pos": 244060,
    "end_pos": 244538,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 512
    }
  },
  "635": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_543",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "hu. Lcsts. large scale chinese short text summa- rization dataset. arXiv preprint arXiv.1506.05865 2015 29 337 Z. Shao. M. Huang. J. Wen. W. Xu. X. Zhu. Long and diverse text gener- ation with planning-based hierarchical variational model. arXiv preprint arXiv.1908.06605 2019 29 338 J. Novikova. O. Dus ek. Rieser. The e2e dataset. New challenges for end-to-end generation. arXiv preprint arXiv.1706.09254 2017 29 339 C. Zheng. M. Huang. A. Sun. Chid. large-scale chinese idiom dataset",
    "chunk_index": 543,
    "start_pos": 244539,
    "end_pos": 244996,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 486
    }
  },
  "636": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_544",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "A. Sun. Chid. large-scale chinese idiom dataset for cloze test. arXiv preprint arXiv.1906.01265 2019 29 340 Bisk. R. Zellers. J. Gao. Choi. et al. Piqa. Reasoning about phys- ical commonsense in natural language. in. Proceedings of the AAAI conference on artificial intelligence. ol. 34. 2020. pp. 7432 7439. 28. 29 341 M. Joshi. E. Choi. D. S. Weld. L. Zettlemoyer. Triviaqa. large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv.1705.03551 2017 28. 29. 31",
    "chunk_index": 544,
    "start_pos": 244997,
    "end_pos": 245474,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 502
    }
  },
  "637": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_545",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "arXiv preprint arXiv.1705.03551 2017 28. 29. 31 342 P. Clark. I. Cowhey. O. Etzioni. T. Khot. A. Sabharwal. C. Schoenick. O. Tafjord. Think you have solved question answering. try arc. the ai2 reasoning challenge. arXiv preprint arXiv.1803.05457 2018 28. 29. 31 343 S. Aroca-Ouellette. C. Paik. A. Roncone. K. Kann. Prost. Phys- ical reasoning of objects through space and time. arXiv preprint arXiv.2106.03634 2021 29 344 T. Mihaylov. P. Clark. T. Khot. A. Sabharwal. Can suit of armor con-",
    "chunk_index": 545,
    "start_pos": 245475,
    "end_pos": 245932,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 491
    }
  },
  "638": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_546",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "k. T. Khot. A. Sabharwal. Can suit of armor con- duct electricity. new dataset for open book question answering. arXiv preprint arXiv.1809.02789 2018 29 345 T. C. Ferreira. C. Gardent. N. Ilinykh. C. Van Der Lee. S. Mille. D. Moussallem. A. Shimorina. The 2020 bilingual. bi-directional webnlg shared task overview and evaluation results webnlg 2020 in. Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web WebNLG 2020. 29",
    "chunk_index": 546,
    "start_pos": 245933,
    "end_pos": 246369,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 487,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 487,
      "optimized_content_length": 468
    }
  },
  "639": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_547",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "eration from the Semantic Web WebNLG 2020. 29 346 C. Xu. W. Zhou. T. Ge. K. Xu. J. McAuley. F. Wei. Blow the dog whistle. chinese dataset for cant understanding with common sense and world knowledge. arXiv preprint arXiv.2104.02704 2021 29 347 G. Lai. Q. Xie. H. Liu. Yang. E. Hovy. Race. Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv.1704.04683 2017 29 348 E. Choi. H. He. M. Iyyer. M. Yatskar. W.-t. Yih. Choi. P. Liang.",
    "chunk_index": 547,
    "start_pos": 246370,
    "end_pos": 246804,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 458
    }
  },
  "640": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_548",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Iyyer. M. Yatskar. W.-t. Yih. Choi. P. Liang. L. Zettlemoyer. Quac. Question answering in context. arXiv preprint arXiv.1808.07036 2018 29 349 M. Geva. D. Khashabi. E. Segal. T. Khot. D. Roth. J. Berant. Did aristo- tle use laptop. question answering benchmark with implicit reason- ing strategies. Transactions of the Association for Computational Lin- guistics 2021 346 361. 29. 31 350 J. Boyd-Graber. B. Satino ff. H. He. H. Daume III. Besting the quiz mas-",
    "chunk_index": 548,
    "start_pos": 246805,
    "end_pos": 247234,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 479,
      "optimized_content_length": 460
    }
  },
  "641": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_549",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ino ff. H. He. H. Daume III. Besting the quiz mas- ter. Crowdsourcing incremental classification games. in. Proceedings of the 2012 joint conference on empirical methods in natural language pro- cessing and computational natural language learning. 2012. pp. 1290 1301. 29 351 S. Zhang. X. Zhang. H. Wang. J. Cheng. P. Li. Z. Ding. Chinese medical question answer matching using end-to-end character-level multi-scale cnns. Applied Sciences 2017 767. 29 352 S. Zhang. X. Zhang. H. Wang. L. Guo. S. Liu. Multi-scale attentive in-",
    "chunk_index": 549,
    "start_pos": 247235,
    "end_pos": 247724,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 527
    }
  },
  "642": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_550",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "H. Wang. L. Guo. S. Liu. Multi-scale attentive in- teraction networks for chinese medical question answer selection. IEEE 43",
    "chunk_index": 550,
    "start_pos": 247725,
    "end_pos": 247798,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 124,
      "word_count": 19,
      "optimized_for_embedding": true,
      "original_content_length": 124,
      "optimized_content_length": 124
    }
  },
  "643": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_551",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "chinese medical question answer selection. IEEE 43 --- Page 44 --- Access 2018 74061 74071. 29 353 C. Xu. J. Pei. H. Wu. Liu. C. Li. Matinf. jointly labeled large-scale dataset for classification. question answering and summarization. arXiv preprint arXiv.2004.12302 2020 29 354 K. Sakaguchi. R. L. Bras. C. Bhagavatula. Choi. Winogrande. An adversarial winograd schema challenge at scale. Communications of the ACM 64 2021 99 106. 27. 29 355 R. Zellers. A. Holtzman. Bisk. A. Farhadi. Choi. Hellaswag. Can",
    "chunk_index": 551,
    "start_pos": 247800,
    "end_pos": 248294,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 506
    }
  },
  "644": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_552",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Bisk. A. Farhadi. Choi. Hellaswag. Can machine really finish your sentence. arXiv preprint arXiv.1905.07830 2019 29 356 M. Roemmele. C. A. Bejan. A. S. Gordon. Choice of plausible alter- natives. An evaluation of commonsense causal reasoning. in. AAAI spring symposium. logical formalizations of commonsense reasoning. 2011. pp. 90 95. 29 357 H. Levesque. E. Davis. L. Morgenstern. The winograd schema chal- lenge. in. Thirteenth international conference on the principles of knowl- edge representation and reasoning. 2012. 27. 29",
    "chunk_index": 552,
    "start_pos": 248295,
    "end_pos": 248795,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 530
    }
  },
  "645": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_553",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "l- edge representation and reasoning. 2012. 27. 29 358 A. Talmor. J. Herzig. N. Lourie. J. Berant. Commonsenseqa. question answering challenge targeting commonsense knowledge. arXiv preprint arXiv.1811.00937 2018 29. 31 359 M. Sap. H. Rashkin. D. Chen. R. LeBras. Choi. Socialiqa. Commonsense reasoning about social interactions. arXiv preprint arXiv.1904.09728 2019 29 360 K. Sun. D. Yu. D. Yu. C. Cardie. Investigating prior knowledge for chal- lenging chinese machine reading comprehension. Transactions of the",
    "chunk_index": 553,
    "start_pos": 248796,
    "end_pos": 249276,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 513
    }
  },
  "646": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_554",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "machine reading comprehension. Transactions of the Association for Computational Linguistics 2020 141 155. 29 361 S. Zhang. X. Liu. J. Liu. J. Gao. K. Duh. B. Van Durme. Record. Bridg- ing the gap between human and machine commonsense reading compre- hension. arXiv preprint arXiv.1810.12885 2018 29 362 P. Rajpurkar. J. Zhang. K. Lopyrev. P. Liang. Squad. 100.000 questions for machine comprehension of text. arXiv preprint arXiv.1606.05250 2016 29. 31 363 C. Clark. K. Lee. M.-W. Chang. T. Kwiatkowski. M. Collins.",
    "chunk_index": 554,
    "start_pos": 249277,
    "end_pos": 249759,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 516
    }
  },
  "647": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_555",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "K. Lee. M.-W. Chang. T. Kwiatkowski. M. Collins. K. Toutanova. Boolq. Exploring the surprising di fficulty of natural yes no questions. arXiv preprint arXiv.1905.10044 2019 29. 31 364 P. Rajpurkar. R. Jia. P. Liang. Know what you don know. Unanswer- able questions for squad. arXiv preprint arXiv.1806.03822 2018 29. 31 365 D. Dua. Wang. P. Dasigi. G. Stanovsky. S. Singh. M. Gardner. Drop. reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv.1903.00161 2019 29. 31",
    "chunk_index": 555,
    "start_pos": 249760,
    "end_pos": 250244,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 512
    }
  },
  "648": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_556",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "hs. arXiv preprint arXiv.1903.00161 2019 29. 31 366 I. Dagan. O. Glickman. B. Magnini. The pascal recognising textual en- tailment challenge. in. Machine learning challenges workshop. Springer. 2005. pp. 177 190. 29. 31 367 Chang. M. Narang. H. Suzuki. G. Cao. J. Gao. Bisk. Webqa. Mul- tihop and multimodal qa. in. Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. 2022. pp. 16495 16504. 29. 31 368 Cui. T. Liu. Z. Chen. W. Ma. S. Wang. G. Hu. Dataset for the first",
    "chunk_index": 556,
    "start_pos": 250245,
    "end_pos": 250715,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 499
    }
  },
  "649": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_557",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Chen. W. Ma. S. Wang. G. Hu. Dataset for the first evaluation on chinese machine reading comprehension. arXiv preprint arXiv.1709.08299 2017 29 369 Cui. T. Liu. W. Che. L. Xiao. Z. Chen. W. Ma. S. Wang. G. Hu. span-extraction dataset for chinese machine reading comprehension. arXiv preprint arXiv.1810.07366 2018 29. 31 370 Cui. T. Liu. Z. Yang. Z. Chen. W. Ma. W. Che. S. Wang. G. Hu. sentence cloze dataset for chinese machine reading comprehension. arXiv preprint arXiv.2004.03116 2020 29",
    "chunk_index": 557,
    "start_pos": 250716,
    "end_pos": 251182,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 492
    }
  },
  "650": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_558",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ension. arXiv preprint arXiv.2004.03116 2020 29 371 Li. T. Liu. D. Li. Q. Li. J. Shi. Wang. Character-based bilstm-crf incorporating pos and dictionaries for chinese opinion target extraction. in. Asian Conference on Machine Learning. PMLR. 2018. pp. 518 533. 29 372 D. Khashabi. S. Chaturvedi. M. Roth. S. Upadhyay. D. Roth. Look- ing beyond the surface. challenge set for reading comprehension over multiple sentences. in. Proceedings of the 2018 Conference of the",
    "chunk_index": 558,
    "start_pos": 251183,
    "end_pos": 251615,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 483,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 483,
      "optimized_content_length": 466
    }
  },
  "651": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_559",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ces. in. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguis- tics. Human Language Technologies. olume Long Papers 2018. pp. 252 262. 29 373 T. Kwiatkowski. J. Palomaki. O. Redfield. M. Collins. A. Parikh. C. Al- berti. D. Epstein. I. Polosukhin. J. Devlin. K. Lee. et al. Natural ques- tions. benchmark for question answering research. Transactions of the",
    "chunk_index": 559,
    "start_pos": 251616,
    "end_pos": 251991,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 426,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 426,
      "optimized_content_length": 414
    }
  },
  "652": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_560",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "question answering research. Transactions of the Association for Computational Linguistics 2019 453 466. 29 374 C. C. Shao. T. Liu. Lai. Tseng. S. Tsai. Drcd. chinese ma- chine reading comprehension dataset. arXiv preprint arXiv.1806.00920 2018 29 375 W. He. K. Liu. J. Liu. Lyu. S. Zhao. X. Xiao. Liu. Wang. H. Wu. Q. She. et al. Dureader. chinese machine reading comprehension dataset from real-world applications. arXiv preprint arXiv.1711.05073 2017 29",
    "chunk_index": 560,
    "start_pos": 251992,
    "end_pos": 252437,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 456
    }
  },
  "653": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_561",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ations. arXiv preprint arXiv.1711.05073 2017 29 376 H. Tang. J. Liu. H. Li. Hong. H. Wu. H. Wang. Dureaderrobust. chinese dataset towards evaluating the robustness of machine reading comprehension models. arXiv preprint arXiv.2004.11142 2020 29 377 J. Welbl. N. F. Liu. M. Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv.1707.06209 2017 29 378 C. Xiong. Z. Dai. J. Callan. Z. Liu. R. Power. End-to-end neural ad-hoc ranking with kernel pooling. in. Proceedings of the 40th International",
    "chunk_index": 561,
    "start_pos": 252438,
    "end_pos": 252925,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 517
    }
  },
  "654": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_562",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "pooling. in. Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval. 2017. pp. 55 64. 29 379 A. Pen as. E. Hovy. P. Forner. Rodrigo. R. Sutcli ffe. R. Morante. Qa4mre 2011-2013. Overview of question answering for machine read- ing evaluation. in. Information Access Evaluation. Multilinguality. Mul- timodality. and Visualization. 4th International Conference of the CLEF Initiative. CLEF 2013. Valencia. Spain. September 23-26. 2013. Pro- ceedings 4. Springer. 2013. pp. 303 320. 29",
    "chunk_index": 562,
    "start_pos": 252926,
    "end_pos": 253419,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 540
    }
  },
  "655": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_563",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Pro- ceedings 4. Springer. 2013. pp. 303 320. 29 380 S. Lim. M. Kim. J. Lee. Korquad1. 0. Korean qa dataset for machine reading comprehension. arXiv preprint arXiv.1909.07005 2019 29 381 C. Xiao. H. Zhong. Z. Guo. C. Tu. Z. Liu. M. Sun. Feng. X. Han. Z. Hu. H. Wang. et al. Cail2018. large-scale legal dataset for judg- ment prediction. arXiv preprint arXiv.1807.02478 2018 29 382 D. Hendrycks. S. Basart. S. Kadavath. M. Mazeika. A. Arora. E. Guo. C. Burns. S. Puranik. H. He. D. Song. et al. Measuring coding challenge",
    "chunk_index": 563,
    "start_pos": 253420,
    "end_pos": 253911,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 520
    }
  },
  "656": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_564",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "H. He. D. Song. et al. Measuring coding challenge competence with apps. arXiv preprint arXiv.2105.09938 2021 29. 31 383 Wang. X. Liu. S. Shi. Deep neural solver for math word problems. in. Proceedings of the 2017 conference on empirical methods in natural language processing. 2017. pp. 845 854. 29. 31 384 K. Cobbe. Kosaraju. M. Bavarian. M. Chen. H. Jun. L. Kaiser. M. Plappert. J. Tworek. J. Hilton. R. Nakano. et al. Training verifiers to solve math word problems. arXiv preprint arXiv.2110.14168 2021 29. 31",
    "chunk_index": 564,
    "start_pos": 253912,
    "end_pos": 254393,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 512
    }
  },
  "657": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_565",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ms. arXiv preprint arXiv.2110.14168 2021 29. 31 385 J. Austin. A. Odena. M. I. Nye. M. Bosma. H. Michalewski. D. Dohan. E. Jiang. C. J. Cai. M. Terry. Q. Le. C. Sutton. Program synthesis with large language models. CoRR abs 2108.07732 2021 29 386 F. Shi. M. Suzgun. M. Freitag. X. Wang. S. Srivats. S. osoughi. H. W. Chung. Tay. S. Ruder. D. Zhou. et al. Language models are mul- tilingual chain-of-thought reasoners. arXiv preprint arXiv.2210.03057 2022 29",
    "chunk_index": 565,
    "start_pos": 254394,
    "end_pos": 254825,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 482,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 482,
      "optimized_content_length": 457
    }
  },
  "658": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_566",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "soners. arXiv preprint arXiv.2210.03057 2022 29 387 S. Roy. D. Roth. Solving general arithmetic word problems. arXiv preprint arXiv.1608.01413 2016 29 388 S.-Y Miao. C.-C. Liang. K.-Y Su. diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv.2106.15772 2021 29 389 R. Koncel-Kedziorski. S. Roy. A. Amini. N. Kushman. H. Hajishirzi. Mawps. math word problem repository. in. Proceedings of the 2016 conference of the north american chapter of the association for computa-",
    "chunk_index": 566,
    "start_pos": 254826,
    "end_pos": 255314,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 516
    }
  },
  "659": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_567",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "american chapter of the association for computa- tional linguistics. human language technologies. 2016. pp. 1152 1157. 29 390 A. Patel. S. Bhattamishra. N. Goyal. Are nlp models really able to solve simple math word problems. arXiv preprint arXiv.2103.07191 2021 29 391 Lai. C. Li. Wang. T. Zhang. R. Zhong. L. Zettlemoyer. W.-t. Yih. D. Fried. S. Wang. T. Yu. Ds-1000. natural and reliable benchmark for data science code generation. in. International Conference on Machine Learning. PMLR. 2023. pp. 18319 18345. 29",
    "chunk_index": 567,
    "start_pos": 255315,
    "end_pos": 255800,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 516
    }
  },
  "660": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_568",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Machine Learning. PMLR. 2023. pp. 18319 18345. 29 392 J. Austin. A. Odena. M. Nye. M. Bosma. H. Michalewski. D. Dohan. E. Jiang. C. Cai. M. Terry. Q. Le. et al. Program synthesis with large language models. arXiv preprint arXiv.2108.07732 2021 29 393 Nie. A. Williams. E. Dinan. M. Bansal. J. Weston. D. Kiela. Adver- sarial nli. new benchmark for natural language understanding. arXiv preprint arXiv.1910.14599 2019 29. 31 394 A. Williams. N. Nangia. S. R. Bowman. broad-coverage challenge",
    "chunk_index": 568,
    "start_pos": 255801,
    "end_pos": 256262,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 490
    }
  },
  "661": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_569",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Nangia. S. R. Bowman. broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv.1704.05426 2017 29 395 R. T. McCoy. E. Pavlick. T. Linzen. Right for the wrong reasons. Diag- 44",
    "chunk_index": 569,
    "start_pos": 256263,
    "end_pos": 256438,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 226,
      "word_count": 34,
      "optimized_for_embedding": true,
      "original_content_length": 226,
      "optimized_content_length": 217
    }
  },
  "662": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_570",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "T. Linzen. Right for the wrong reasons. Diag- 44 --- Page 45 --- nosing syntactic heuristics in natural language inference. arXiv preprint arXiv.1902.01007 2019 29 396 J. Liu. L. Cui. H. Liu. D. Huang. Wang. Zhang. Logiqa. chal- lenge dataset for machine reading comprehension with logical reason- ing. arXiv preprint arXiv.2007.08124 2020 29 397 P. Lewis. B. guz. R. Rinott. S. Riedel. H. Schwenk. Mlqa. Eval- uating cross-lingual extractive question answering. arXiv preprint arXiv.1910.07475 2019 29",
    "chunk_index": 570,
    "start_pos": 256440,
    "end_pos": 256919,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 502
    }
  },
  "663": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_571",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "wering. arXiv preprint arXiv.1910.07475 2019 29 398 A. Conneau. G. Lample. R. Rinott. A. Williams. S. R. Bowman. H. Schwenk. Stoyanov. Xnli. Evaluating cross-lingual sentence rep- resentations. arXiv preprint arXiv.1809.05053 2018 29. 31 399 Yang. Zhang. C. Tar. J. Baldridge. Paws-x. cross- lingual adversarial dataset for paraphrase identification. arXiv preprint arXiv.1908.11828 2019 29. 31 400 S. Narayan. S. B. Cohen. M. Lapata. Don give me the details. just the",
    "chunk_index": 571,
    "start_pos": 256920,
    "end_pos": 257368,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 468
    }
  },
  "664": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_572",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "en. M. Lapata. Don give me the details. just the summary. Topic-Aware Convolutional Neural Networks for Extreme Summarization. ArXiv. abs 1808 29 401 E. M. Ponti. G. Glavas O. Majewska. Q. Liu. I. Vuli c. A. Korhonen. Xcopa. multilingual dataset for causal commonsense reasoning. arXiv preprint arXiv.2005.00333 2020 29 402 A. Tikhonov. M. Ryabinin. It all in the heads. Using attention heads as baseline for cross-lingual transfer in commonsense reasoning. arXiv preprint arXiv.2106.12066 2021 29",
    "chunk_index": 572,
    "start_pos": 257369,
    "end_pos": 257839,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 497
    }
  },
  "665": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_573",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "soning. arXiv preprint arXiv.2106.12066 2021 29 403 J. H. Clark. E. Choi. M. Collins. D. Garrette. T. Kwiatkowski. Niko- laev. J. Palomaki. Tydi qa. benchmark for information-seeking ques- tion answering in typologically diverse languages. Transactions of the Association for Computational Linguistics 2020 454 470. 29 404 T. Scialom. P.-A. Dray. S. Lamprier. B. Piwowarski. J. Staiano. Mlsum. The multilingual summarization corpus. arXiv preprint arXiv.2004.14900 2020 29",
    "chunk_index": 573,
    "start_pos": 257840,
    "end_pos": 258281,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 472
    }
  },
  "666": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_574",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "corpus. arXiv preprint arXiv.2004.14900 2020 29 405 S. Lin. J. Hilton. O. Evans. Truthfulqa. Measuring how models mimic human falsehoods. arXiv preprint arXiv.2109.07958 2021 29. 32 406 I. Augenstein. C. Lioma. D. Wang. L. C. Lima. C. Hansen. C. Hansen. J. G. Simonsen. Multifc. real-world multi-domain dataset for evidence-based fact checking of claims. arXiv preprint arXiv.1909.03242 2019 29 407 J. Thorne. A. Vlachos. C. Christodoulopoulos. A. Mittal. Fever.",
    "chunk_index": 574,
    "start_pos": 258282,
    "end_pos": 258712,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 481,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 462
    }
  },
  "667": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_575",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "lachos. C. Christodoulopoulos. A. Mittal. Fever. large-scale dataset for fact extraction and verification. arXiv preprint arXiv.1803.05355 2018 29 408 I. Mollas. Z. Chrysopoulou. S. Karlos. G. Tsoumakas. Ethos. an online hate speech detection dataset. arXiv preprint arXiv.2006.08328 2020 29. 32 409 M. Nadeem. A. Bethke. S. Reddy. Stereoset. Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv.2004.09456 2020 29. 32 410 A. Parrish. A. Chen. N. Nangia. Padmakumar. J. Phang. J. Thomp-",
    "chunk_index": 575,
    "start_pos": 258713,
    "end_pos": 259197,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 514
    }
  },
  "668": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_576",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "en. N. Nangia. Padmakumar. J. Phang. J. Thomp- son. P. M. Htut. S. R. Bowman. Bbq. hand-built bias benchmark for question answering. arXiv preprint arXiv.2110.08193 2021 29 411 J. Zhao. T. Wang. M. Yatskar. Ordonez. K.-W. Chang. Gender bias in coreference resolution. Evaluation and debiasing methods. arXiv preprint arXiv.1804.06876 2018 29 412 N. Nangia. C. Vania. R. Bhalerao. S. R. Bowman. Crows-pairs. chal- lenge dataset for measuring social biases in masked language models. arXiv preprint arXiv.2010.00133 2020 29",
    "chunk_index": 576,
    "start_pos": 259198,
    "end_pos": 259693,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 521
    }
  },
  "669": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_577",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2010.00133 2020 29 413 S. Gehman. S. Gururangan. M. Sap. Choi. N. A. Smith. Realtoxic- ityprompts. Evaluating neural toxic degeneration in language models. arXiv preprint arXiv.2009.11462 2020 29 414 D. Borkan. L. Dixon. J. Sorensen. N. Thain. L. Vasserman. Nuanced metrics for measuring unintended bias with real data for text classifica- tion. in. Companion proceedings of the 2019 world wide web confer- ence. 2019. pp. 491 500. 29 415 O. Bojar. R. Chatterjee. C. Federmann. Graham. B. Haddow.",
    "chunk_index": 577,
    "start_pos": 259694,
    "end_pos": 260188,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 525
    }
  },
  "670": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_578",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Chatterjee. C. Federmann. Graham. B. Haddow. M. Huck. A. J. Yepes. P. Koehn. Logacheva. C. Monz. et al. Find- ings of the 2016 conference on machine translation. in. Proceedings of the First Conference on Machine Translation. olume 2. Shared Task Papers. 2016. pp. 131 198. 29 416 B. Loi c. B. Magdalena. B. Ond \u02c7rej. F. Christian. G. Yvette. G. Ro- man. H. Barry. H. Matthias. J. Eric. K. Tom. et al. Findings of the 2020 conference on machine translation wmt20 in. Proceedings of",
    "chunk_index": 578,
    "start_pos": 260189,
    "end_pos": 260637,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 481
    }
  },
  "671": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_579",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "on machine translation wmt20 in. Proceedings of the Fifth Conference on Machine Translation. Association for Compu- tational Linguistics 2020. pp. 55. 29 417 W. Li. F. Qi. M. Sun. X. Yi. J. Zhang. Ccpm. chinese classical poetry matching dataset. arXiv preprint arXiv.2106.01979 2021 29 418 E. Dinan. S. Roller. K. Shuster. A. Fan. M. Auli. J. Weston. Wizard of wikipedia. Knowledge-powered conversational agents. arXiv preprint arXiv.1811.01241 2018 29 419 H. Rashkin. E. M. Smith. M. Li. .-L. Boureau. Towards empathetic",
    "chunk_index": 579,
    "start_pos": 260638,
    "end_pos": 261129,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 521
    }
  },
  "672": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_580",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Smith. M. Li. .-L. Boureau. Towards empathetic open-domain conversation models. new benchmark and dataset. arXiv preprint arXiv.1811.00207 2018 29 420 E. Dinan. Logacheva. Malykh. A. Miller. K. Shuster. J. Urbanek. D. Kiela. A. Szlam. I. Serban. R. Lowe. et al. The second conversa- tional intelligence challenge convai2 in. The NeurIPS 18 Competi- tion. From Machine Learning to Intelligent Conversations. Springer. 2020. pp. 187 208. 29 421 H. Zhou. C. Zheng. K. Huang. M. Huang. X. Zhu. Kdconv. chinese",
    "chunk_index": 580,
    "start_pos": 261130,
    "end_pos": 261611,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 505
    }
  },
  "673": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_581",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "eng. K. Huang. M. Huang. X. Zhu. Kdconv. chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation. arXiv preprint arXiv.2004.04100 2020 29 422 L. CO. Iflytek. multiple categories chinese text classifier. competition official website 2019 29 423 J. Baumgartner. S. Zannettou. B. Keegan. M. Squire. J. Blackburn. The pushshift reddit dataset. in. Proceedings of the international AAAI con- ference on web and social media. ol. 14. 2020. pp. 830 839. 30",
    "chunk_index": 581,
    "start_pos": 261612,
    "end_pos": 262057,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 480
    }
  },
  "674": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_582",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "and social media. ol. 14. 2020. pp. 830 839. 30 424 A. Fan. Jernite. E. Perez. D. Grangier. J. Weston. M. Auli. Eli5. Long form question answering. arXiv preprint arXiv.1907.09190 2019 31 425 Wang. S. Mishra. P. Alipoormolabashi. Kordi. A. Mirzaei. A. Arunkumar. A. Ashok. A. S. Dhanasekaran. A. Naik. D. Stap. et al. Benchmarking generalization via in-context instructions on 1.600 lan- guage tasks. arXiv preprint arXiv.2204.07705 2022 31 426 T. Xie. C. H. Wu. P. Shi. R. Zhong. T. Scholak. M. Yasunaga. C.-S. Wu.",
    "chunk_index": 582,
    "start_pos": 262058,
    "end_pos": 262551,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 515
    }
  },
  "675": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_583",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Shi. R. Zhong. T. Scholak. M. Yasunaga. C.-S. Wu. M. Zhong. P. Yin. S. I. Wang. et al. Unifiedskg. Unifying and multi- tasking structured knowledge grounding with text-to-text language mod- els. arXiv preprint arXiv.2201.05966 2022 31 427 Q. Ye. B. Lin. X. Ren. Crossfit. few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv.2104.08835 2021 31 428 Aribandi. Tay. T. Schuster. J. Rao. H. S. Zheng. S. Mehta. H. Zhuang. Q. Tran. D. Bahri. J. Ni. et al. Ext5. Towards extreme",
    "chunk_index": 583,
    "start_pos": 262552,
    "end_pos": 263042,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 506
    }
  },
  "676": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_584",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "an. D. Bahri. J. Ni. et al. Ext5. Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv.2111.10952 2021 31 429 A. Williams. N. Nangia. S. Bowman. broad-coverage challenge cor- pus for sentence understanding through inference. in. Proceedings of the 2018 Conference of the North American Chapter of the Associ- ation for Computational Linguistics. Human Language Technologies. olume Long Papers Association for Computational Linguistics. New Orleans. Louisiana. 2018. pp. 1112 1122. doi.10.18653 v1 N18-1101",
    "chunk_index": 584,
    "start_pos": 263043,
    "end_pos": 263541,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 531
    }
  },
  "677": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_585",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "2018. pp. 1112 1122. doi.10.18653 v1 N18-1101 URL https. aclanthology.org N18-1101 31 430 Zhang. J. Baldridge. L. He. PAWS. Paraphrase adversaries from word scrambling. in. Proceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics. Human Language Technologies. olume Long and Short Papers Associa- tion for Computational Linguistics. Minneapolis. Minnesota. 2019. pp. 1298 1308. doi.10.18653 v1 N19-1131 URL https. aclanthology.org N19-1131 32",
    "chunk_index": 585,
    "start_pos": 263542,
    "end_pos": 264014,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 501
    }
  },
  "678": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_586",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "19-1131 URL https. aclanthology.org N19-1131 32 431 C. Qin. A. Zhang. Z. Zhang. J. Chen. M. Yasunaga. D. Yang. Is chat- GPT general-purpose natural language processing task solver. in. The 2023 Conference on Empirical Methods in Natural Language Process- ing. 2023. URL https. openreview.net forum.id u03xn1COsO 32 432 M. U. Hadi. R. Qureshi. A. Shah. M. Irfan. A. Zafar. M. B. Shaikh. N. Akhtar. J. Wu. S. Mirjalili. et al. Large language models. com- prehensive survey of its applications. challenges. limitations. and future",
    "chunk_index": 586,
    "start_pos": 264015,
    "end_pos": 264505,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 527
    }
  },
  "679": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_587",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "applications. challenges. limitations. and future prospects. TechRxiv 2023 32 433 X. L. Dong. S. Moon. E. Xu. K. Malik. Z. Yu. Towards next- generation intelligent assistants leveraging llm techniques. in. Proceed- ings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023. pp. 5792 5793. 32 434 K. Pandya. M. Holia. Automating customer service using langchain. Building custom open-source gpt chatbot for organizations. arXiv preprint arXiv.2310.05421 2023 32",
    "chunk_index": 587,
    "start_pos": 264506,
    "end_pos": 264956,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 486
    }
  },
  "680": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_588",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ations. arXiv preprint arXiv.2310.05421 2023 32 435 J. Li. B. Hui. G. Qu. B. Li. J. Yang. B. Li. B. Wang. B. Qin. R. Cao. R. Geng. et al. Can llm already serve as database interface. 45",
    "chunk_index": 588,
    "start_pos": 264957,
    "end_pos": 265101,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 195,
      "word_count": 39,
      "optimized_for_embedding": true,
      "original_content_length": 195,
      "optimized_content_length": 185
    }
  },
  "681": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_589",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "an llm already serve as database interface. 45 --- Page 46 --- big bench for large-scale database grounded text-to-sqls. arXiv preprint arXiv.2305.03111 2023 32 436 A. Rao. J. Kim. M. Kamineni. M. Pang. W. Lie. M. D. Succi. Evaluating chatgpt as an adjunct for radiologic decision-making. medRxiv 2023 2023 02. 32 437 M. Benary. X. D. Wang. M. Schmidt. D. Soll. G. Hilfenhaus. M. Nas- sir. C. Sigler. M. Kno dler. U. Keller. D. Beule. et al. Leveraging large language models for decision support in personalized oncology. JAMA",
    "chunk_index": 589,
    "start_pos": 265103,
    "end_pos": 265591,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 526
    }
  },
  "682": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_590",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "or decision support in personalized oncology. JAMA Network Open 11 2023 e2343689 e2343689. 32 438 C. M. Chiesa-Estomba. J. R. Lechien. L. A. Vaira. A. Brunet. G. Cam- maroto. M. Mayo-Yanez. A. Sanchez-Barrueco. C. Saga-Gutierrez. Ex- ploring the potential of chat-gpt as supportive tool for sialendoscopy clinical decision making and patient information support. European Archives of Oto-Rhino-Laryngology 2023 6. 32 439 S. Montagna. S. Ferretti. L. C. Klopfenstein. A. Florio. M. F. Pengo.",
    "chunk_index": 590,
    "start_pos": 265592,
    "end_pos": 266047,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 490
    }
  },
  "683": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_591",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "retti. L. C. Klopfenstein. A. Florio. M. F. Pengo. Data decentralisation of llm-based chatbot systems in chronic disease self-management. in. Proceedings of the 2023 ACM Conference on In- formation Technology for Social Good. 2023. pp. 205 212. 32 440 D. Bill. T. Eriksson. Fine-tuning llm using reinforcement learning from human feedback for therapy chatbot application 2023 32 441 M. Abbasian. I. Azimi. A. M. Rahmani. R. Jain. Conversational health agents. personalized llm-powered agent framework. arXiv preprint",
    "chunk_index": 591,
    "start_pos": 266048,
    "end_pos": 266526,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 516
    }
  },
  "684": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_592",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "alized llm-powered agent framework. arXiv preprint arXiv.2310.02374 2023 32 442 K. Lemley. Does chatgpt help us understand the medical literature. Journal of the American Society of Nephrology 2023 10 1681. 32 443 S. Pal. M. Bhattacharya. S.-S. Lee. C. Chakraborty. domain-specific next-generation large language model llm or chatgpt is required for biomedical engineering and research. Annals of Biomedical Engineering 2023 4. 32 444 Du. S. Zhao. Chen. R. Bai. J. Liu. H. Wu. H. Wang. B. Qin. The",
    "chunk_index": 592,
    "start_pos": 266527,
    "end_pos": 267005,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 497
    }
  },
  "685": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_593",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Chen. R. Bai. J. Liu. H. Wu. H. Wang. B. Qin. The calla dataset. Probing llms interactive knowledge acquisition from chi- nese medical literature. arXiv preprint arXiv.2309.04198 2023 32 445 A. Abd-Alrazaq. R. AlSaad. D. Alhuwail. A. Ahmed. P. M. Healy. S. Latifi. S. Aziz. R. Damseh. S. A. Alrazak. J. Sheikh. et al. Large language models in medical education. Opportunities. challenges. and future directions. JMIR Medical Education 2023 e48291. 32 446 A. B. Mbakwe. I. Lourentzou. L. A. Celi. O. J. Mechanic. A. Dagan.",
    "chunk_index": 593,
    "start_pos": 267006,
    "end_pos": 267494,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 521
    }
  },
  "686": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_594",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Lourentzou. L. A. Celi. O. J. Mechanic. A. Dagan. Chatgpt passing usmle shines spotlight on the flaws of medical educa- tion 2023 32 447 S. Ahn. The impending impacts of large language models on medical education. Korean Journal of Medical Education 35 2023 103. 32 448 E. Waisberg. J. Ong. M. Masalkhi. A. G. Lee. Large language model llm -driven chatbots for neuro-ophthalmic medical education. Eye 2023 3. 32 449 G. Deiana. M. Dettori. A. Arghittu. A. Azara. G. Gabutti. P. Castiglia.",
    "chunk_index": 594,
    "start_pos": 267495,
    "end_pos": 267954,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 487
    }
  },
  "687": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_595",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "A. Arghittu. A. Azara. G. Gabutti. P. Castiglia. Artificial intelligence and public health. Evaluating chatgpt responses to vaccination myths and misconceptions. Vaccines 11 2023 1217. 32 450 L. De Angelis. F. Baglivo. G. Arzilli. G. P. Privitera. P. Ferragina. A. E. Tozzi. C. Rizzo. Chatgpt and the rise of large language models. the new ai-driven infodemic threat in public health. Frontiers in Public Health 11 2023 1166120. 32 451 N. L. Rane. A. Tawde. S. P. Choudhary. J. Rane. Contribution and per-",
    "chunk_index": 595,
    "start_pos": 267955,
    "end_pos": 268423,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 505
    }
  },
  "688": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_596",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "e. S. P. Choudhary. J. Rane. Contribution and per- formance of chatgpt and other large language models llm for scientific and research advancements. double-edged sword. International Re- search Journal of Modernization in Engineering Technology and Science 10 2023 875 899. 32 452 W. Dai. J. Lin. H. Jin. T. Li. .-S. Tsai. D. Gas evi c. G. Chen. Can large language models provide feedback to students. case study on chatgpt. in. 2023 IEEE International Conference on Advanced Learning Tech-",
    "chunk_index": 596,
    "start_pos": 268424,
    "end_pos": 268879,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 490
    }
  },
  "689": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_597",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "nternational Conference on Advanced Learning Tech- nologies ICALT IEEE. 2023. pp. 323 325. 32 453 E. Kasneci. K. Se\u00dfler. S. Ku chemann. M. Bannert. D. Dementieva. F. Fischer. U. Gasser. G. Groh. S. Gu nnemann. E. Hu llermeier. et al. Chatgpt for good. on opportunities and challenges of large language models for education. Learning and individual di fferences 103 2023 102274. 32 454 N. Rane. Enhancing the quality of teaching and learning through chat- gpt and similar large language models. Challenges. future prospects.",
    "chunk_index": 597,
    "start_pos": 268880,
    "end_pos": 269359,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 523
    }
  },
  "690": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_598",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "rge language models. Challenges. future prospects. and ethical considerations in education. Future Prospects. and Ethical Considerations in Education September 15. 2023 2023 32 455 J. C. Young. M. Shishido. Investigating openai chatgpt potentials in generating chatbot dialogue for english as foreign language learning.International Journal of Advanced Computer Science and Applications 14 2023 32 456 J. Irons. C. Mason. P. Cooper. S. Sidra. A. Reeson. C. Paris. Exploring",
    "chunk_index": 598,
    "start_pos": 269360,
    "end_pos": 269804,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 473
    }
  },
  "691": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_599",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Cooper. S. Sidra. A. Reeson. C. Paris. Exploring the impacts of chatgpt on future scientific work. SocArXiv 2023 32 457 P. G. Schmidt. A. J. Meir. Using generative ai for literature searches and scholarly writing. Is the integrity of the scientific discourse in jeopardy. arXiv preprint arXiv.2311.06981 2023 32 458 Zheng. H. Koh. J. Ju. A. T. Nguyen. L. T. May. G. I. Webb. S. Pan. Large language models for scientific synthesis. inference and explana- tion. arXiv preprint arXiv.2310.07984 2023 33",
    "chunk_index": 599,
    "start_pos": 269805,
    "end_pos": 270277,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 499
    }
  },
  "692": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_600",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tion. arXiv preprint arXiv.2310.07984 2023 33 459 B. Aczel. E.-J. Wagenmakers. Transparency guidance for chatgpt usage in scientific writing. PsyArXiv 2023 33 460 S. Altma e. A. Sola-Leyva. A. Salumets. Artificial intelligence in sci- entific writing. friend or foe. Reproductive BioMedicine Online 2023 33 461 S. Imani. L. Du. H. Shrivastava. Mathprompter. Mathematical reasoning using large language models. arXiv preprint arXiv.2303.05398 2023 33 462 Z. Yuan. H. Yuan. C. Li. G. Dong. C. Tan. C. Zhou. Scaling relationship",
    "chunk_index": 600,
    "start_pos": 270278,
    "end_pos": 270778,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 525
    }
  },
  "693": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_601",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Li. G. Dong. C. Tan. C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv.2308.01825 2023 33 463 K. Yang. A. M. Swope. A. Gu. R. Chalamala. P. Song. S. Yu. S. Godil. R. Prenger. A. Anandkumar. Leandojo. Theorem proving with retrieval- augmented language models. arXiv preprint arXiv.2306.15626 2023 33 464 K. M. Collins. A. Q. Jiang. S. Frieder. L. Wong. M. Zilka. U. Bhatt. T. Lukasiewicz. Wu. J. B. Tenenbaum. W. Hart. et al. Evaluating",
    "chunk_index": 601,
    "start_pos": 270779,
    "end_pos": 271242,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 499
    }
  },
  "694": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_602",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Wu. J. B. Tenenbaum. W. Hart. et al. Evaluating language models for mathematics through interactions. arXiv preprint arXiv.2306.01694 2023 33 465 Liu. T. Han. S. Ma. J. Zhang. Yang. J. Tian. H. He. A. Li. M. He. Z. Liu. et al. Summary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology 2023 100017. 33 466 J. Dra pal. H. Westermann. J. Savelka. Using large language models to support thematic analysis in empirical legal studies. arXiv preprint arXiv.2310.18729 2023 33",
    "chunk_index": 602,
    "start_pos": 271243,
    "end_pos": 271737,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 522
    }
  },
  "695": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_603",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tudies. arXiv preprint arXiv.2310.18729 2023 33 467 J. Savelka. K. D. Ashley. M. A. Gray. H. Westermann. H. Xu. Explain- ing legal concepts with augmented large language models gpt-4 arXiv preprint arXiv.2306.09525 2023 33 468 N. Guha. J. Nyarko. D. E. Ho. C. Re A. Chilton. A. Narayana. A. Chohlas-Wood. A. Peters. B. Waldon. D. N. Rockmore. et al. Legal- bench. collaboratively built benchmark for measuring legal reasoning in large language models. arXiv preprint arXiv.2308.11462 2023 33",
    "chunk_index": 603,
    "start_pos": 271738,
    "end_pos": 272198,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 491
    }
  },
  "696": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_604",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2308.11462 2023 33 469 J. Cui. Z. Li. Yan. B. Chen. L. Yuan. Chatlaw. Open-source legal large language model with integrated external knowledge bases. arXiv preprint arXiv.2306.16092 2023 33 470 H. Yang. X.-Y Liu. C. D. Wang. Fingpt. Open-source financial large language models. arXiv preprint arXiv.2306.06031 2023 33 471 Li. S. Wang. H. Ding. H. Chen. Large language models in finance. survey. in. Proceedings of the Fourth ACM International Conference on AI in Finance. 2023. pp. 374 382. 33",
    "chunk_index": 604,
    "start_pos": 272199,
    "end_pos": 272698,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 523
    }
  },
  "697": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_605",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Conference on AI in Finance. 2023. pp. 374 382. 33 472 A. Lykov. D. Tsetserukou. Llm-brain. Ai-driven fast generation of robot behaviour tree based on large language model. arXiv preprint arXiv.2305.19352 2023 33 473 E. Billing. J. Rose n. M. Lamb. Language models for human-robot inter- action. in. ACM IEEE International Conference on Human-Robot Inter- action. March 13 16. 2023. Stockholm. Sweden. ACM Digital Library. 2023. pp. 905 906. 33 474 Ye. H. You. J. Du. Improved trust in human-robot collaboration with",
    "chunk_index": 605,
    "start_pos": 272699,
    "end_pos": 273177,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 516
    }
  },
  "698": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_606",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "Improved trust in human-robot collaboration with chatgpt. IEEE Access 2023 33 475 Ding. X. Zhang. C. Paxton. S. Zhang. Leveraging commonsense knowledge from large language models for task and motion planning. in. RSS 2023 Workshop on Learning for Task and Motion Planning. 2023. 33 476 J. Wu. R. Antonova. A. Kan. M. Lepert. A. Zeng. S. Song. J. Bohg. S. Rusinkiewicz. T. Funkhouser. Tidybot. Personalized robot assistance with large language models. arXiv preprint arXiv.2305.05658 2023 33",
    "chunk_index": 606,
    "start_pos": 273178,
    "end_pos": 273633,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 490
    }
  },
  "699": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_607",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "models. arXiv preprint arXiv.2305.05658 2023 33 477 E. Strubell. A. Ganesh. A. McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv.1906.02243 2019 34 478 E. M. Bender. T. Gebru. A. McMillan-Major. S. Shmitchell. On the dan- 46",
    "chunk_index": 607,
    "start_pos": 273634,
    "end_pos": 273857,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 274,
      "word_count": 41,
      "optimized_for_embedding": true,
      "original_content_length": 274,
      "optimized_content_length": 264
    }
  },
  "700": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_608",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "A. McMillan-Major. S. Shmitchell. On the dan- 46 --- Page 47 --- gers of stochastic parrots. Can language models be too big. in. Pro- ceedings of the 2021 ACM conference on fairness. accountability. and transparency. 2021. pp. 610 623. 34 479 C. Zhang. S. Bengio. M. Hardt. B. Recht. O. Vinyals. Understanding deep learning still requires rethinking generalization. Communications of the ACM 64 2021 107 115. 34 480 M. Ta nzer. S. Ruder. M. Rei. Memorisation versus generalisation in pre-",
    "chunk_index": 608,
    "start_pos": 273859,
    "end_pos": 274310,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 488
    }
  },
  "701": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_609",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "M. Rei. Memorisation versus generalisation in pre- trained language models. arXiv preprint arXiv.2105.00828 2021 34 481 S. M. West. M. Whittaker. K. Crawford. Discriminating systems. AI Now 2019 33. 34 482 K. Valmeekam. A. Olmo. S. Sreedharan. S. Kambhampati. Large lan- guage models still can plan benchmark for llms on planning and reasoning about change arXiv preprint arXiv.2206.10498 2022 34 483 Zhang. Li. L. Cui. D. Cai. L. Liu. T. Fu. X. Huang. E. Zhao.",
    "chunk_index": 609,
    "start_pos": 274311,
    "end_pos": 274752,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 461
    }
  },
  "702": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_610",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "L. Cui. D. Cai. L. Liu. T. Fu. X. Huang. E. Zhao. Zhang. Chen. et al. Siren song in the ai ocean. survey on hal- lucination in large language models. arXiv preprint arXiv.2309.01219 2023 34 484 A. Webson. E. Pavlick. Do prompt-based models really understand the meaning of their prompts. arXiv preprint arXiv.2109.01247 2021 34 485 O. Shaikh. H. Zhang. W. Held. M. Bernstein. D. Yang. On second thought. let not think step by step. bias and toxicity in zero-shot rea- soning. arXiv preprint arXiv.2212.08061 2022 34",
    "chunk_index": 610,
    "start_pos": 274753,
    "end_pos": 275247,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 515
    }
  },
  "703": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_611",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "soning. arXiv preprint arXiv.2212.08061 2022 34 486 B. C. Das. M. H. Amini. Wu. Security and privacy challenges of large language models. survey. arXiv preprint arXiv.2402.00888 2024 34 487 X. Liu. H. Cheng. P. He. W. Chen. Wang. H. Poon. J. Gao. Adversar- ial training for large neural language models. ArXiv April 2020 URL https. www.microsoft.com en-us research publication adversarial-training-for-large-neural-language-models 34 488 E. Shayegani. M. A. A. Mamun. Fu. P. Zaree. Dong. N. Abu-",
    "chunk_index": 611,
    "start_pos": 275248,
    "end_pos": 275728,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 495
    }
  },
  "704": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_612",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "A. A. Mamun. Fu. P. Zaree. Dong. N. Abu- Ghazaleh. Survey of vulnerabilities in large language models revealed by adversarial attacks 2023 arXiv.2310.10844 34 489 X. Xu. K. Kong. N. Liu. L. Cui. D. Wang. J. Zhang. M. Kankanhalli. An llm can fool itself. prompt-based adversarial attack 2023 arXiv. 2310.13345 34 490 H. Zhao. H. Chen. F. Yang. N. Liu. H. Deng. H. Cai. S. Wang. D. Yin. M. Du. Explainability for large language models. survey 2023 arXiv.2309.01029 35",
    "chunk_index": 612,
    "start_pos": 275729,
    "end_pos": 276176,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 465
    }
  },
  "705": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_613",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "age models. survey 2023 arXiv.2309.01029 35 491 S. Huang. S. Mamidanna. S. Jangam. Zhou. L. H. Gilpin. Can large language models explain themselves. study of llm-generated self- explanations 2023 arXiv.2310.11207 35 492 H. Brown. K. Lee. F. Mireshghallah. R. Shokri. F. Trame r. What does it mean for language model to preserve privacy. in. Proceedings of the 2022 ACM Conference on Fairness. Accountability. and Transparency. 2022. pp. 2280 2292. 35 493 R. Plant. Giu ffrida. D. Gkatzia. You are what you write. Pre-",
    "chunk_index": 613,
    "start_pos": 276177,
    "end_pos": 276673,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 517
    }
  },
  "706": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_614",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "ffrida. D. Gkatzia. You are what you write. Pre- serving privacy in the era of large language models. arXiv preprint arXiv.2204.09391 2022 35 494 W. Niu. Z. Kong. G. Yuan. W. Jiang. J. Guan. C. Ding. P. Zhao. S. Liu. B. Ren. Wang. Real-time execution of large-scale language models on mobile 2020 arXiv.2009.06823 35 495 C. Guo. J. Tang. W. Hu. J. Leng. C. Zhang. F. Yang. Liu. M. Guo. Zhu. Olive. Accelerating large language models via hardware- friendly outlier-victim pair quantization. in. Proceedings of the 50th",
    "chunk_index": 614,
    "start_pos": 276674,
    "end_pos": 277166,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 95,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 517
    }
  },
  "707": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_615",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "tim pair quantization. in. Proceedings of the 50th Annual International Symposium on Computer Architecture. 2023. pp. 15. 35 496 B. Mesko E. J. Topol. The imperative for regulatory oversight of large language models or generative ai in healthcare. npj Digital Medicine 2023 120. 35 497 J. Zhang. X. Ji. Z. Zhao. X. Hei. K.-K. R. Choo. Ethical considerations and policy implications for large language models. Guiding responsible development and deployment. arXiv preprint arXiv.2308.02678 2023 35",
    "chunk_index": 615,
    "start_pos": 277167,
    "end_pos": 277632,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 496
    }
  },
  "708": {
    "chunk_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96_chunk_616",
    "document_id": "b5957dea-f7b5-41f3-9fcb-75754e9a3a96",
    "content": "oyment. arXiv preprint arXiv.2308.02678 2023 35 498 J. Mo kander. J. Schuett. H. R. Kirk. L. Floridi. Auditing large language models. three-layered approach. AI and Ethics 2023 31. 35 47",
    "chunk_index": 616,
    "start_pos": 277633,
    "end_pos": 277778,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 196,
      "word_count": 30,
      "optimized_for_embedding": true,
      "original_content_length": 196,
      "optimized_content_length": 186
    }
  },
  "709": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_0",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "--- Page --- Comprehensive Overview of Large Language Models Humza Naveeda. Asad Ullah Khanb. Shi Qiuc. Muhammad Saqibd.e. Saeed Anwarf.g. Muhammad Usmanf.g. Naveed Akhtarh.j. Nick Barnesi. Ajmal Mianj aThe University of Sydney. Sydney. Australia bUniversity of Engineering and Technology UET Lahore. Pakistan cThe Chinese University of Hong Kong CUHK HKSAR. China dUniversity of Technology Sydney UTS Sydney. Australia",
    "chunk_index": 0,
    "start_pos": 0,
    "end_pos": 438,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 438,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 438,
      "optimized_content_length": 419
    }
  },
  "710": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_1",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "sity of Technology Sydney UTS Sydney. Australia eCommonwealth Scientific and Industrial Research Organisation CSIRO Sydney. Australia fKing Fahd University of Petroleum and Minerals KFUPM Dhahran. Saudi Arabia gSDAIA-KFUPM Joint Research Center for Artificial Intelligence JRCAI Dhahran. Saudi Arabia hThe University of Melbourne UoM Melbourne. Australia iAustralian National University ANU Canberra. Australia jThe University of Western Australia UWA Perth. Australia Abstract",
    "chunk_index": 1,
    "start_pos": 439,
    "end_pos": 886,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 477
    }
  },
  "711": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_2",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Western Australia UWA Perth. Australia Abstract Large Language Models LLMs have recently demonstrated remarkable capabilities in natural language processing tasks and beyond. This success of LLMs has led to large influx of research contributions in this direction. These works encompass diverse topics such as architectural innovations. better training strategies. context length improvements. fine-tuning. multi-modal LLMs.",
    "chunk_index": 2,
    "start_pos": 887,
    "end_pos": 1267,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 431,
      "word_count": 56,
      "optimized_for_embedding": true,
      "original_content_length": 431,
      "optimized_content_length": 424
    }
  },
  "712": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_3",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ength improvements. fine-tuning. multi-modal LLMs. robotics. datasets. benchmarking. fficiency. and more. With the rapid development of techniques and regular breakthroughs in LLM research. it has become considerably challenging to perceive the bigger picture of the advances in this direction. Considering the rapidly emerging plethora of literature on LLMs. it is imperative that the research community is able to benefit from concise",
    "chunk_index": 3,
    "start_pos": 1268,
    "end_pos": 1657,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 440,
      "word_count": 63,
      "optimized_for_embedding": true,
      "original_content_length": 440,
      "optimized_content_length": 436
    }
  },
  "713": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_4",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "search community is able to benefit from concise yet comprehensive overview of the recent developments in this field. This article provides an overview of the literature on broad range of LLM-related concepts. Our self-contained comprehensive overview of LLMs discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs. This review article is intended to provide not only",
    "chunk_index": 4,
    "start_pos": 1658,
    "end_pos": 2040,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 433,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 433,
      "optimized_content_length": 427
    }
  },
  "714": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_5",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "review article is intended to provide not only systematic survey but also quick. comprehensive reference for the researchers and practitioners to draw insights from extensive. informative summaries of the existing works to advance the LLM research. Keywords. Large Language Models. LLMs. chatGPT. Augmented LLMs. Multimodal LLMs. LLM training. LLM Benchmarking 1. Introduction Language plays fundamental role in facilitating commu- nication and self-expression for humans and their interaction",
    "chunk_index": 5,
    "start_pos": 2041,
    "end_pos": 2491,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 493
    }
  },
  "715": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_6",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "self-expression for humans and their interaction with machines. The need for generalized models stems from the growing demand for machines to handle complex language tasks. including translation. summarization. information re- trieval. conversational interactions. etc. Recently. significant breakthroughs have been witnessed in language models. pri- marily attributed to transformers increased computational capabilities. and the availability of large-scale training data. These developments have brought about revolutionary trans-",
    "chunk_index": 6,
    "start_pos": 2492,
    "end_pos": 2982,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 532
    }
  },
  "716": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_7",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "lopments have brought about revolutionary trans- formation by enabling the creation of LLMs that can approxi- mate human-level performance on various tasks 2. Large Equal contribution Email addresses. humza_naveed yahoo.com Humza Naveed aukhanee gmail.com Asad Ullah Khan shiqiu cse.cuhk.edu.hk Shi Qiu muhammad.saqib data61.csiro.au Muhammad Saqib saeed.anwar kfupm.edu.sa Saeed Anwar muhammad.usman kfupm.edu.sa Muhammad Usman naveed.akhtar1 unimelb.edu.au Naveed Akhtar",
    "chunk_index": 7,
    "start_pos": 2983,
    "end_pos": 3433,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 52,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 472
    }
  },
  "717": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_8",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "naveed.akhtar1 unimelb.edu.au Naveed Akhtar nick.barnes anu.edu.au Nick Barnes ajmal.mian uwa.edu.au Ajmal Mian Figure 1. The trend of papers released over the years containing keywords Large Language Model Large Language Model Fine-Tuning and Large Language Model Alignment Preprint submitted to Elsevier October 18. 2024arXiv.2307.06435v10 cs.CL 17 Oct 2024",
    "chunk_index": 8,
    "start_pos": 3434,
    "end_pos": 3769,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 386,
      "word_count": 45,
      "optimized_for_embedding": true,
      "original_content_length": 386,
      "optimized_content_length": 359
    }
  },
  "718": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_9",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "18. 2024arXiv.2307.06435v10 cs.CL 17 Oct 2024 --- Page --- 2019T5 Oct GPT-3 May WebGPT Dec OPT-IML TK-Instruct May mT0 Dec Wizard-LM Vicuna Alpaca Mar HuaTuo Apr Koala May Wizard-Coder Jun Goat PanGu-\u03b1 Apr CPM-2 Jun GPT-NeoX-20B Apr CodeGen Mar Galactica Nov GLM Oct OPT UL2 May LLaMA Feb LLaMA Jul MPT Jun CodeT5 Code Llama Aug StarCoder Xuan Yuan 2.0 May 20202021202220232024mT5 Oct HyperCLOVA Sep ERNIE 3.0 Codex Jul Jurassic-1 Aug Yuan 1.0 Oct Gopher Dec ERNIE 3.0 Titan GLaM LaMDA",
    "chunk_index": 9,
    "start_pos": 3771,
    "end_pos": 4269,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 485
    }
  },
  "719": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_10",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "1.0 Oct Gopher Dec ERNIE 3.0 Titan GLaM LaMDA T0 Oct ChatGPT Nov Sparrow Sep FLAN-U-PaLM Oct Bard Oct MT-NLG Jan AlphaCode Feb Chinchilla Mar PaLM Apr U-PALM Oct BLOOM Nov AlexaTM Aug PaLM2 May GPT-4 PanGu-\u03a3 Mar BloombergGPT Claude Gemini Dec DeepSeek Jan LLaMA Grok-1 Mar Snowflake Arctic Apr DeepSeek-V2 May Mixtral 8x22B Nemotron Feb GPT-4o May OpenAI o1 Sep Gemini-1.5 Feb Grok-1.5 Apr",
    "chunk_index": 10,
    "start_pos": 4270,
    "end_pos": 4663,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 444,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 443,
      "optimized_content_length": 389
    }
  },
  "720": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_11",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "OpenAI o1 Sep Gemini-1.5 Feb Grok-1.5 Apr Figure 2. Chronological display of LLM releases. blue cards represent pre-trained models. while orange cards correspond to instruction-tuned models. Models on the upper half signify open-source availability. whereas those on the bottom are closed-source. The chart illustrates the increasing trend towards instruction-tuned and open-source models. highlighting the evolving landscape and trends in natural language processing research. Language Models LLMs have emerged as cutting-edge arti-",
    "chunk_index": 11,
    "start_pos": 4664,
    "end_pos": 5161,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 533
    }
  },
  "721": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_12",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Models LLMs have emerged as cutting-edge arti- ficial intelligence systems that can process and generate text with coherent communication and generalize to multiple tasks 5. The historical progress in natural language processing NLP evolved from statistical to neural language modeling and then from pre-trained language models PLMs to LLMs. While conventional language modeling LM trains task-specific mod- els in supervised settings. PLMs are trained in self-supervised",
    "chunk_index": 12,
    "start_pos": 5162,
    "end_pos": 5603,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 471
    }
  },
  "722": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_13",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ed settings. PLMs are trained in self-supervised setting on large corpus of text 7. 8. with the aim of learning generic representation that is shareable among various NLP tasks. After fine-tuning for downstream tasks. PLMs surpass the performance gains of traditional language modeling LM The larger PLMs bring more performance gains. which has led to the transitioning of PLMs to LLMs by significantly increas- ing model parameters tens to hundreds of billions 10 and training dataset many GBs and TBs 10. 11 Following this",
    "chunk_index": 13,
    "start_pos": 5604,
    "end_pos": 6099,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 524
    }
  },
  "723": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_14",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ataset many GBs and TBs 10. 11 Following this development. numerous LLMs have been proposed in the lit- erature 10. 11. 12. 6. 13. 14. 15 An increasing trend in the number of released LLMs and names of few significant LLMs proposed over the years are shown in Fig and Fig 2. respec- tively. The early work on LLMs. such as T5 10 and mT5 11 em- ployed transfer learning until GPT-3 showed LLMs are zero-shot transferable to downstream tasks without fine-tuning. LLMs accurately respond to task queries when prompted with",
    "chunk_index": 14,
    "start_pos": 6100,
    "end_pos": 6588,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 519
    }
  },
  "724": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_15",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "urately respond to task queries when prompted with task descriptions and examples. However. pre-trained LLMs fail to follow user intent and perform worse in zero-shot set- tings than in few-shot. Fine-tuning them with task instruc- tions data 16. 17. 18. 19 and aligning with human prefer- ences 20. 21 enhances generalization to unseen tasks. im- proving zero-shot performance significantly and reducing mis- aligned behavior. In addition to better generalization and domain adaptation. LLMs appear to have emergent abilities. such as reasoning.",
    "chunk_index": 15,
    "start_pos": 6589,
    "end_pos": 7088,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 546
    }
  },
  "725": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_16",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ear to have emergent abilities. such as reasoning. planning. decision-making. in-context learning. answering in zero-shot settings. etc. These abilities are known to be ac- quired by them due to their gigantic scale even when the pre- trained LLMs are not trained specifically to possess these at- tributes 22. 23. 24 Such abilities have led LLMs to be widely adopted in diverse settings. including multi-modal. robotics.tool manipulation. question answering. autonomous agents. etc. Various improvements have also been suggested in these areas",
    "chunk_index": 16,
    "start_pos": 7089,
    "end_pos": 7585,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 544
    }
  },
  "726": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_17",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "provements have also been suggested in these areas either by task-specific training 25. 26. 27. 28. 29. 30. 31 or better prompting 32 The LLMs abilities to solve diverse tasks with human-level performance come at the cost of slow training and inference. extensive hardware requirements. and higher running costs. Such requirements have limited their adoption and opened up opportunities to devise better architectures 15. 33. 34. 35 and training strategies 36. 37. 21. 38. 39. 40. 41 Param-",
    "chunk_index": 17,
    "start_pos": 7586,
    "end_pos": 8035,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 490
    }
  },
  "727": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_18",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng strategies 36. 37. 21. 38. 39. 40. 41 Param- eter fficient tuning 38. 41. 40 pruning 42. 43 quantiza- tion 44. 45 knowledge distillation. and context length inter- polation 46. 47. 48. 49 among others are some of the methods widely studied for fficient LLM utilization. Due to the success of LLMs on wide variety of tasks. the research literature has recently experienced large influx of LLM-related contributions. Researchers have organized the LLMs literature in surveys 50. 51. 52. 53 and topic-specific",
    "chunk_index": 18,
    "start_pos": 8036,
    "end_pos": 8519,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 509
    }
  },
  "728": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_19",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "re in surveys 50. 51. 52. 53 and topic-specific surveys in 54. 55. 56. 57. 58 In contrast to these surveys. our contribution focuses on providing comprehensive yet concise overview of the general direction of LLM research. This arti- cle summarizes architectural and training details of pre-trained LLMs and delves deeper into the details of concepts like fine- tuning. multi-modal LLMs. augmented LLMs. datasets. eval- uation. applications. challenges. and others to provide self- contained comprehensive overview. Our key contributions are",
    "chunk_index": 19,
    "start_pos": 8520,
    "end_pos": 9020,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 541
    }
  },
  "729": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_20",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "comprehensive overview. Our key contributions are summarized as follows. We present survey on the developments in LLM research. providing concise. comprehensive overview of the direc- tion. We present extensive summaries of pre-trained models that include fine-grained details of architecture and training de- tails. We summarize major findings of the popular contributions and provide detailed discussion on the key design and development aspects of LLMs to help practitioners ffec- tively leverage this technology.",
    "chunk_index": 20,
    "start_pos": 9021,
    "end_pos": 9498,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 516
    }
  },
  "730": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_21",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "titioners ffec- tively leverage this technology. In this self-contained article. we cover range of con- cepts to present the general direction of LLMs compre- hensively. including background. pre-training. fine-tuning.",
    "chunk_index": 21,
    "start_pos": 9499,
    "end_pos": 9673,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 225,
      "word_count": 32,
      "optimized_for_embedding": true,
      "original_content_length": 225,
      "optimized_content_length": 218
    }
  },
  "731": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_22",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "including background. pre-training. fine-tuning. --- Page --- Figure 3. broader overview of LLMs. dividing LLMs into seven branches. 1. Pre-Training 2. Fine-Tuning 3. fficient 4. Inference 5. Evaluation 6. Applications 7. Challenges multi-modal LLMs. augmented LLMs. LLMs-powered agents. datasets. evaluation. etc. We loosely follow the existing terminology to ensure stan- dardized outlook of this research direction. For instance. fol- lowing 50 our survey discusses pre-trained LLMs with 10B",
    "chunk_index": 22,
    "start_pos": 9675,
    "end_pos": 10131,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 494
    }
  },
  "732": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_23",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "our survey discusses pre-trained LLMs with 10B parameters or more. We refer the readers interested in smaller pre-trained models to 51. 52. 53 The organization of this paper is as follows. Section discusses the background of LLMs. Section focuses on LLMs overview. architectures. training pipelines and strategies. fine-tuning. andutilization in di fferent domains. Section highlights the config- uration and parameters that play crucial role in the function- ing of these models. Summary and discussions are presented",
    "chunk_index": 23,
    "start_pos": 10132,
    "end_pos": 10614,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 518
    }
  },
  "733": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_24",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "hese models. Summary and discussions are presented in section 3.8. The LLM training and evaluation. datasets. and benchmarks are discussed in section 5. followed by challenges and future directions. and conclusion in sections and 8. re- spectively.",
    "chunk_index": 24,
    "start_pos": 10615,
    "end_pos": 10816,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 252,
      "word_count": 39,
      "optimized_for_embedding": true,
      "original_content_length": 252,
      "optimized_content_length": 248
    }
  },
  "734": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_25",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "conclusion in sections and 8. re- spectively. --- Page --- 2. Background We provide the relevant background to understand the fun- damentals related to LLMs in this section. We briefly discuss necessary components in LLMs and refer the readers interested in details to the original works. 2.1. Tokenization Tokenization 59 is an essential pre-processing step in LLM training that parses the text into non-decomposing units called tokens. Tokens can be characters. subwords 60 sym-",
    "chunk_index": 25,
    "start_pos": 10818,
    "end_pos": 11259,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 480
    }
  },
  "735": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_26",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ens. Tokens can be characters. subwords 60 sym- bols 61 or words. depending on the tokenization process. Some of the commonly used tokenization schemes in LLMs include wordpiece 62 byte pair encoding BPE 61 and un- igramLM 60 Readers are encouraged to refer to 63 for detailed survey. 2.2. Encoding Positions The transformer processes input sequences in parallel and independently of each other. Moreover. the attention mod- ule in the transformer does not capture positional information.",
    "chunk_index": 26,
    "start_pos": 11260,
    "end_pos": 11718,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 488
    }
  },
  "736": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_27",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ansformer does not capture positional information. As result. positional encodings were introduced in trans- former 64 where positional embedding vector is added to the token embedding. Variants of positional embedding include absolute. relative. or learned positional encodings. Within rel- ative encoding. Alibi and RoPE are two widely used positional embeddings in LLMs. Alibi 65 It subtracts scalar bias from the attention score that increases with the distance between token positions. This favors using recent tokens for attention.",
    "chunk_index": 27,
    "start_pos": 11719,
    "end_pos": 12217,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 537
    }
  },
  "737": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_28",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ns. This favors using recent tokens for attention. RoPE 66 It rotates query and key representations at an an- gle proportional to the token absolute position in the input sequence. resulting in relative positional encoding scheme which decays with the distance between the tokens. 2.3. Attention in LLMs Attention assigns weights to input tokens based on impor- tance so that the model gives more emphasis to relevant tokens. Attention in transformers 64 calculates query. key. and value",
    "chunk_index": 28,
    "start_pos": 12218,
    "end_pos": 12661,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 487
    }
  },
  "738": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_29",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "transformers 64 calculates query. key. and value mappings for input sequences. where the attention score is obtained by multiplying the query and key. and later used to weight values. We discuss di fferent attention strategies used in LLMs below. Self-Attention 64 Calculates attention using queries. keys. and values from the same block encoder or decoder Cross Attention. It is used in encoder-decoder architectures. where encoder outputs are the queries. and key-value pairs come from the decoder.",
    "chunk_index": 29,
    "start_pos": 12662,
    "end_pos": 13119,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 500
    }
  },
  "739": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_30",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ueries. and key-value pairs come from the decoder. Sparse Attention 67 Self-attention has n2 time complex- ity which becomes infeasible for large sequences. To speed up the computation. sparse attention 67 iteratively calculates attention in sliding windows for speed gains. Flash Attention 68 Memory access is the major bottleneck in calculating attention using GPUs. To speed up. flash attention employs input tiling to minimize the memory reads and writes between the GPU high bandwidth memory HBM",
    "chunk_index": 30,
    "start_pos": 13120,
    "end_pos": 13582,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 500
    }
  },
  "740": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_31",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "writes between the GPU high bandwidth memory HBM and the on-chip SRAM.2.4. Activation Functions The activation functions serve crucial role in the curve- fitting abilities of neural networks 69 We discuss activation functions used in LLMs in this section. ReLU 70 The Rectified linear unit ReLU is defined as. ReLU max 0.x GeLU 71 The Gaussian Error Linear Unit GeLU is the combination of ReLU. dropout 72 and zoneout 73 GLU variants 74 The Gated Linear Unit 75 is neural",
    "chunk_index": 31,
    "start_pos": 13583,
    "end_pos": 14041,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 471
    }
  },
  "741": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_32",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "iants 74 The Gated Linear Unit 75 is neural network layer that is an element-wise product of linear transformation and sigmoid transformed linear projection of the input given as. GLU x.W.V.b.c xW xV where Xis the input of layer and l.W.b.Vandcare learned parameters. Other GLU variants 74 used in LLMs are. ReGLU x.W.V.b.c max 0.xW GEGLU x.W.V.b.c GELU xW xV wiGLU x.W.V.b.c.\u03b2 wish\u03b2 xW xV 2.5. Layer Normalization Layer normalization leads to faster convergence and is an in-",
    "chunk_index": 32,
    "start_pos": 14042,
    "end_pos": 14538,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 476
    }
  },
  "742": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_33",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "lization leads to faster convergence and is an in- tegrated component of transformers 64 In addition to Layer- Norm 76 and RMSNorm 77 LLMs use pre-layer normal- ization 78 applying it before multi-head attention MHA Pre-norm is shown to provide training stability in LLMs. An- other normalization variant. DeepNorm 79 fixes the issue with larger gradients in pre-norm. 2.6. Distributed LLM Training This section describes distributed LLM training approaches briefly. More details are available in 13. 37. 80. 81",
    "chunk_index": 33,
    "start_pos": 14539,
    "end_pos": 15018,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 511
    }
  },
  "743": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_34",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "y. More details are available in 13. 37. 80. 81 Data Parallelism. Data parallelism replicates the model on multiple devices where data in batch gets divided across de- vices. At the end of each training iteration weights are synchro- nized across all devices. Tensor Parallelism. Tensor parallelism shards tensor compu- tation across devices. It is also known as horizontal parallelism or intra-layer model parallelism. Pipeline Parallelism. Pipeline parallelism shards model layers",
    "chunk_index": 34,
    "start_pos": 15019,
    "end_pos": 15457,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 489,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 489,
      "optimized_content_length": 482
    }
  },
  "744": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_35",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "allelism. Pipeline parallelism shards model layers across di fferent devices. This is also known as vertical paral- lelism. Model Parallelism. combination of tensor and pipeline par- allelism is known as model parallelism. 3D Parallelism. combination of data. tensor. and model par- allelism is known as 3D parallelism. Optimizer Parallelism. Optimizer parallelism also known as zero redundancy optimizer 37 implements optimizer state partitioning. gradient partitioning. and parameter partitioning",
    "chunk_index": 35,
    "start_pos": 15458,
    "end_pos": 15911,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 498
    }
  },
  "745": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_36",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "gradient partitioning. and parameter partitioning across devices to reduce memory consumption while keeping the communication costs as low as possible.",
    "chunk_index": 36,
    "start_pos": 15912,
    "end_pos": 16015,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 154,
      "word_count": 21,
      "optimized_for_embedding": true,
      "original_content_length": 153,
      "optimized_content_length": 151
    }
  },
  "746": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_37",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ping the communication costs as low as possible. --- Page --- 2.7. Libraries Some commonly used libraries for LLMs training are. Transformers 82 The library provides access to various pre- trained transformer models with APIs to train. fine-tune. infer. and develop custom models. DeepSpeed 36 library for scalable distributed training and inference of deep learning models. Megatron-LM 80 It provides GPU-optimized techniques for large-scale training of LLMs. JAX 83 Python library for high-performance numerical",
    "chunk_index": 37,
    "start_pos": 16017,
    "end_pos": 16499,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 513
    }
  },
  "747": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_38",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Python library for high-performance numerical computing and scaleable machine learning. It can di fferenti- ate native Python and NumPy functions and execute them on GPUs. Colossal-AI 84 collection of components to write dis- tributed deep learning models. BMTrain 81 library to write fficient stand-alone LLMs training code. FastMoE 85 Provides API to build mixture-of-experts MoE model in PyTorch. MindSpore 86 deep learning training and inference frame- work extendable to mobile. edge. and cloud computing.",
    "chunk_index": 38,
    "start_pos": 16500,
    "end_pos": 16986,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 510
    }
  },
  "748": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_39",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "extendable to mobile. edge. and cloud computing. PyTorch 87 framework developed by Facebook AI Re- search lab FAIR to build deep learning models. The main features of PyTorch include dynamic computation graph and pythonic coding style. Tensorflow 88 deep learning framework written by Google. The key features of TensorFlow are graph-based com- putation. eager execution. scalability. etc. MXNet 89 Apache MXNet is deep learning framework with support to write programs in multiple languages. includ-",
    "chunk_index": 39,
    "start_pos": 16987,
    "end_pos": 17459,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 500
    }
  },
  "749": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_40",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "to write programs in multiple languages. includ- ing. Python. Scala. R. etc. It also provides support for dynamic and static computation graphs. 2.8. Data PreProcessing This section briefly summarizes data preprocessing tech- niques used in LLMs training. Quality Filtering. For better results. training data quality is essential. Some approaches to filtering data are. classifier- based and heuristics-based. Classifier-based approaches train classifier on high-quality data and predict the quality of",
    "chunk_index": 40,
    "start_pos": 17460,
    "end_pos": 17927,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 502
    }
  },
  "750": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_41",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "er on high-quality data and predict the quality of text for filtering. whereas heuristics-based employ some rules for filtering like language. metrics. statistics. and keywords. Data Deduplication. Duplicated data can ffect model per- formance and increase data memorization. therefore. to train LLMs. data deduplication is one of the preprocessing steps. This can be performed at multiple levels. like sentences. documents. and datasets. Privacy Reduction. Most of the training data for LLMs is",
    "chunk_index": 41,
    "start_pos": 17928,
    "end_pos": 18374,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 495
    }
  },
  "751": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_42",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Reduction. Most of the training data for LLMs is collected through web sources. This data contains private information. therefore. many LLMs employ heuristics-based methods to filter information such as names. addresses. and phone numbers to avoid learning personal information. 2.9. Architectures Here we discuss the variants of the transformer architectures used in LLMs. The di fference arises due to the application of Figure 4. An example of attention patterns in language models. image is taken from 93",
    "chunk_index": 42,
    "start_pos": 18375,
    "end_pos": 18837,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 508
    }
  },
  "752": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_43",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "erns in language models. image is taken from 93 Figure 5. An example of language model training objectives. image from 93 the attention and the connection of transformer blocks. An il- lustration of attention patterns of these architectures is shown in Figure 4. Encoder Decoder. This architecture processes inputs through the encoder and passes the intermediate representation to the decoder to generate the output. Here. the encoder sees the complete sequence utilizing self-attention whereas the decoder",
    "chunk_index": 43,
    "start_pos": 18838,
    "end_pos": 19299,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 506
    }
  },
  "753": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_44",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "uence utilizing self-attention whereas the decoder processes the sequence one after the other with implementing cross-attention. Causal Decoder. type of architecture that does not have an encoder and processes and generates output using decoder. where the predicted token depends only on the previous time steps. Prefix Decoder. It is also known as non-causal decoder. where the attention calculation is not strictly dependent on the past information and the attention is bidirectional. An example",
    "chunk_index": 44,
    "start_pos": 19300,
    "end_pos": 19752,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 503,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 503,
      "optimized_content_length": 497
    }
  },
  "754": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_45",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ion and the attention is bidirectional. An example of non-causal attention mask is shown in Figure 4. Mixture-of-Experts. It is variant of transformer architecture with parallel independent experts and router to route tokens to experts. These experts are feed-forward layers after the at- tention block 90 Mixture-of-Experts MoE is an fficient sparse architecture that ffers comparable performance to dense models and allows increasing the model size without increas- ing the computational cost by activating only few experts at",
    "chunk_index": 45,
    "start_pos": 19753,
    "end_pos": 20249,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 528
    }
  },
  "755": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_46",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ational cost by activating only few experts at time 91. 92 2.10. Pre-Training Objectives This section describes LLMs pre-training objectives. For more details see the paper 93 Full Language Modeling. An autoregressive language model- ing objective where the model is asked to predict future tokens given the previous tokens. an example is shown in Figure 5. Prefix Language Modeling. non-causal training objective. where prefix is chosen randomly and only remaining target tokens are used to calculate the loss. An example is shown in",
    "chunk_index": 46,
    "start_pos": 20250,
    "end_pos": 20747,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 534
    }
  },
  "756": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_47",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "used to calculate the loss. An example is shown in Figure 5.",
    "chunk_index": 47,
    "start_pos": 20748,
    "end_pos": 20759,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 62,
      "word_count": 13,
      "optimized_for_embedding": true,
      "original_content_length": 62,
      "optimized_content_length": 60
    }
  },
  "757": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_48",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Figure 5. --- Page --- Figure 6. basic flow diagram depicting various stages of LLMs from pre-training to prompting utilization. Prompting LLMs to generate responses is possible at different training stages like pre-training. instruction-tuning. or alignment tuning. RL stands for reinforcement learning. RM represents reward-modeling. and RLHF represents reinforcement learning with human feedback. Masked Language Modeling. In this training objective. tokens",
    "chunk_index": 48,
    "start_pos": 20761,
    "end_pos": 21222,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 473,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 473,
      "optimized_content_length": 460
    }
  },
  "758": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_49",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "guage Modeling. In this training objective. tokens or spans sequence of tokens are masked randomly and the model is asked to predict masked tokens given the past and future context. An example is shown in Figure 5. Unified Language Modeling. Unified language modeling 94 is combination of causal. non-causal. and masked language training objectives. Here in masked language modeling. the attention is not bidirectional but unidirectional. attending either left-to-right or right-to-left context. 2.11. LLMs Scaling Laws",
    "chunk_index": 49,
    "start_pos": 21223,
    "end_pos": 21699,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 519
    }
  },
  "759": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_50",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "or right-to-left context. 2.11. LLMs Scaling Laws Scaling laws study the optimal combination of model param- eters. dataset size. and computational resources that predict the improvement in the model performance. It has been shown that the loss scales according to the power-law with model size. dataset size. and compute resources 95 This study suggests larger models are more important than big data for better perfor- mance. Another variant of scaling law 96 suggests the model",
    "chunk_index": 50,
    "start_pos": 21700,
    "end_pos": 22135,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 480
    }
  },
  "760": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_51",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "her variant of scaling law 96 suggests the model size and the number of training tokens should be scaled equally.2.12. LLMs Adaptation Stages This section discusses the fundamentals of LLMs adaptation stages. from pre-training to fine-tuning for downstream tasks and utilization. An example of di fferent training stages and in- ference in LLMs is shown in Figure 6. In this paper. we refer to alignment-tuning as aligning with human preferences. while occasionally the literature uses the term alignment for di fferent purposes.",
    "chunk_index": 51,
    "start_pos": 22136,
    "end_pos": 22616,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 529
    }
  },
  "761": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_52",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "uses the term alignment for di fferent purposes. 2.12.1. Pre-Training In the very first stage. the model is trained in self- supervised manner on large corpus to predict the next to- kens given the input. The design choices of LLMs vary from encoder-decoder to decoder-only architectures with di fferent building blocks and loss functions in sections 2.5. 2.4. 2.10. 2.12.2. Fine-Tuning There are di fferent styles to fine-tune an LLM. This section briefly discusses fine-tuning approaches.",
    "chunk_index": 52,
    "start_pos": 22617,
    "end_pos": 23062,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 490
    }
  },
  "762": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_53",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "section briefly discusses fine-tuning approaches. Transfer Learning. The pre-trained LLMs perform well for various tasks 6. 15 However. to improve the performance for",
    "chunk_index": 53,
    "start_pos": 23063,
    "end_pos": 23184,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 172,
      "word_count": 24,
      "optimized_for_embedding": true,
      "original_content_length": 171,
      "optimized_content_length": 166
    }
  },
  "763": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_54",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "6. 15 However. to improve the performance for --- Page --- downstream task. pre-trained models are fine-tuned with the task-specific data 10. 11 known as transfer learning. Instruction-tuning. To enable model to respond to user queries ffectively. the pre-trained model is fine-tuned on in- struction formatted data i.e. instruction and an input-output pair. Instructions generally comprise multi-task data in plain natural language. guiding the model to respond according to the",
    "chunk_index": 54,
    "start_pos": 23186,
    "end_pos": 23631,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 479
    }
  },
  "764": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_55",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "age. guiding the model to respond according to the prompt and the input. This type of fine-tuning improves zero- shot generalization and downstream task performance. Details on formatting instruction data and its various styles are avail- able in 16. 50. 97 Alignment-tuning. LLMs are prone to generating false. biased. and harmful text. To make them helpful. honest. and harmless. models are aligned using human feedback. Alignment involves asking LLMs to generate unexpected responses and then updat-",
    "chunk_index": 55,
    "start_pos": 23632,
    "end_pos": 24086,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 502
    }
  },
  "765": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_56",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "to generate unexpected responses and then updat- ing their parameters to avoid such responses 20. 21. 98 It ensures LLMs operate according to human intentions and values. model is defined to be an aligned model if the model fulfills three criteria of helpful. honest. and harmless or HHH 99 Researchers employ reinforcement learning with human feed- back RLHF 100 for model alignment. In RLHF. fine-tuned model on demonstrations is further trained with reward model- ing RM and reinforcement learning RL shown in Figure 6.",
    "chunk_index": 56,
    "start_pos": 24087,
    "end_pos": 24583,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 522
    }
  },
  "766": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_57",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nd reinforcement learning RL shown in Figure 6. Below we briefly discuss RM and RL pipelines in RLHF. Reward modeling. trains model to rank generated responses according to human preferences using classification objec- tive. To train the classifier humans annotate LLMs generated responses based on the HHH criteria. Reinforcement learning. in combination with the reward model is used for alignment in the next stage. The previously trained reward model ranks LLM-generated responses into preferred",
    "chunk_index": 57,
    "start_pos": 24584,
    "end_pos": 25039,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 499
    }
  },
  "767": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_58",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "model ranks LLM-generated responses into preferred vs. non-preferred. which is used to align the model with proxi- mal policy optimization PPO This process repeats iteratively until convergence. 2.12.3. Prompting Utilization Prompting is method to query trained LLMs for generating responses. as illustrated in Figure 6. LLMs can be prompted in various prompt setups. where they can be adapted to the instruc- tions without fine-tuning and in other cases with fine-tuning on data containing di fferent prompt styles 16. 101. 102 good",
    "chunk_index": 58,
    "start_pos": 25040,
    "end_pos": 25533,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 533
    }
  },
  "768": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_59",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng di fferent prompt styles 16. 101. 102 good guide on prompt engineering is available at 32 Below. we will discuss various widely used prompt setups. Zero-Shot Prompting. LLMs are zero-shot learners and ca- pable of answering queries never seen before. This style of prompting requires LLMs to answer user questions without see- ing any examples in the prompt. In-context Learning. Also known as few-shot learning. here. multiple input-output demonstration pairs are shown to the model to generate the desired response. This adaptation style",
    "chunk_index": 59,
    "start_pos": 25534,
    "end_pos": 26033,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 542
    }
  },
  "769": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_60",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nerate the desired response. This adaptation style is also called few-shot learning. discussion on formatting in- context learning ICL templates is available in 54. 50. 18. 16 Reasoning in LLMs. LLMs are zero-shot reasoners and can be provoked to generate answers to logical problems. task planning. critical thinking. etc. with reasoning. Generating reasons is possible only by using di fferent prompting styles.whereas to improve LLMs further on reasoning tasks many methods 16. 97 train them on reasoning datasets. We discuss",
    "chunk_index": 60,
    "start_pos": 26034,
    "end_pos": 26520,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 528
    }
  },
  "770": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_61",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "97 train them on reasoning datasets. We discuss various prompting techniques for reasoning below. Chain-of-Thought CoT special case of prompting where demonstrations contain reasoning information aggregated with inputs and outputs so that the model generates outcomes with step-by-step reasoning. More details on CoT prompts are avail- able in 55. 103. 101 Self-Consistency. Improves CoT performance by generat- ing multiple responses and selecting the most frequent an- swer 104 Tree-of-Thought ToT Explores multiple reasoning paths",
    "chunk_index": 61,
    "start_pos": 26521,
    "end_pos": 27020,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 533
    }
  },
  "771": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_62",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "f-Thought ToT Explores multiple reasoning paths with possibilities to look ahead and backtrack for problem- solving 105 Single-Turn Instructions. In this prompting setup. LLMs are queried only once with all the relevant information in the prompt. LLMs generate responses by understanding the con- text either in zero-shot or few-shot setting. Multi-Turn Instructions. Solving complex task requires mul- tiple interactions with LLMs. where feedback and responses from the other tools are given as input to the LLM for the next",
    "chunk_index": 62,
    "start_pos": 27021,
    "end_pos": 27505,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 525
    }
  },
  "772": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_63",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tools are given as input to the LLM for the next rounds. This style of using LLMs in the loop is common in autonomous agents. 3. Large Language Models This section reviews LLMs. briefly describing their architec- tures. training objectives. pipelines. datasets. and fine-tuning details. 3.1. Pre-Trained LLMs Here. we provide summaries of various well-known pre- trained LLMs with significant discoveries. changing the course of research and development in NLP. These LLMs have consid- erably improved the performance in NLU and NLG domains.",
    "chunk_index": 63,
    "start_pos": 27506,
    "end_pos": 27998,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 541
    }
  },
  "773": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_64",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "improved the performance in NLU and NLG domains. and are widely fine-tuned for downstream tasks. Moreover. We also identify key findings and insights of pre-trained LLMs in Table and that improve their performance. 3.1.1. General Purpose T5 10 An encoder-decoder model employing unified text- to-text training for all NLP problems is shown in Figure 7. T5 places layer normalization outside the residual path in conven- tional transformer model 64 It uses masked language mod-",
    "chunk_index": 64,
    "start_pos": 27999,
    "end_pos": 28440,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 476
    }
  },
  "774": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_65",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ansformer model 64 It uses masked language mod- eling as pre-training objective where spans consecutive to- kens are replaced with single mask instead of separate masks for each token. This type of masking speeds up the training as it produces shorter sequences. After pre-training. the model is fine-tuned using adapter layers 106 for downstream tasks. GPT-3 The GPT-3 architecture is the same as the GPT- but with dense and sparse attention in transformer layers similar to the Sparse Transformer 67 It shows that large mod-",
    "chunk_index": 65,
    "start_pos": 28441,
    "end_pos": 28941,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 526
    }
  },
  "775": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_66",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Sparse Transformer 67 It shows that large mod- els can train on larger batch sizes with lower learning rate to decide the batch size during training. GPT-3 uses the gradient noise scale as in 107 Overall. GPT-3 increases model param- eters to 175B showing that the performance of large language",
    "chunk_index": 66,
    "start_pos": 28942,
    "end_pos": 29196,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 305,
      "word_count": 52,
      "optimized_for_embedding": true,
      "original_content_length": 304,
      "optimized_content_length": 294
    }
  },
  "776": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_67",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "showing that the performance of large language --- Page --- Figure 7. Unified text-to-text training example. source image from 10 Figure 8. The image is the article of 108 showing an example of PanGu- architecture. models improves with the scale and is competitive with the fine- tuned models. mT5 11 multilingual T5 model 10 trained on the mC4 dataset with 101 languages. The dataset is extracted from the public common crawl scrape. The model uses larger vocab- ulary size of 250.000 to cover multiple languages. To avoid",
    "chunk_index": 67,
    "start_pos": 29198,
    "end_pos": 29693,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 523
    }
  },
  "777": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_68",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "of 250.000 to cover multiple languages. To avoid over-fitting or under-fitting for language. mT5 employs data sampling procedure to select samples from all languages. The paper suggests using small amount of pre-training datasets. including all languages when fine-tuning for task using En- glish language data. This allows the model to generate correct non-English outputs. PanGu-\u03b1 108 An autoregressive model that has query layer at the end of standard transformer layers. example shown",
    "chunk_index": 68,
    "start_pos": 29694,
    "end_pos": 30145,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 488
    }
  },
  "778": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_69",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "end of standard transformer layers. example shown in Figure 8. to predict the next token. Its structure is similar to the transformer layer but with an additional embedding for the next position in the attention mechanism. given in Eq. 3. pnWq hWk hT HT CPM-2 12 Cost-e fficient Pre-trained language Models CPM-2 pre-trains bilingual English and Chinese 11B and 198B mixture-of-experts MoE models on the WuDaoCor- pus 109 dataset. The tokenization process removes white",
    "chunk_index": 69,
    "start_pos": 30146,
    "end_pos": 30588,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 469
    }
  },
  "779": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_70",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ataset. The tokenization process removes white space tokens in the sentencepiece tokenizer. The models are trained with knowledge inheritance. starting with only the Chi- nese language in the first stage and then adding English and Chinese data. This trained model gets duplicated multiple times to initialize the 198B MoE model. Moreover. to use the model for downstream tasks. CPM-2 experimented with both com-plete fine-tuning and prompt fine-tuning as in 40 where only prompt-related parameters are updated by inserting prompts at",
    "chunk_index": 70,
    "start_pos": 30589,
    "end_pos": 31078,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 534
    }
  },
  "780": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_71",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ted parameters are updated by inserting prompts at various positions. front. middle. and back. CPM-2 also pro- poses the INFMOE. memory-e fficient framework with strat- egy to dynamically ffload parameters to the CPU for inference at 100B scale. It overlaps data movement with inference com- putation for lower inference time. ERNIE 3.0 110 ERNIE 3.0 takes inspiration from multi- task learning to build modular architecture using Transformer- XL 111 as the backbone. The universal representation mod-",
    "chunk_index": 71,
    "start_pos": 31079,
    "end_pos": 31544,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 501
    }
  },
  "781": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_72",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "as the backbone. The universal representation mod- ule is shared by all the tasks. which serve as the basic block for task-specific representation modules. which are all trained jointly for natural language understanding. natural language generation. and knowledge extraction. This LLM is primar- ily focused on the Chinese language. It claims to train on the largest Chinese text corpora for LLM training. and achieved state-of-the-art in 54 Chinese NLP tasks. Jurassic-1 112 pair of auto-regressive language mod-",
    "chunk_index": 72,
    "start_pos": 31545,
    "end_pos": 32013,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 514
    }
  },
  "782": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_73",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "c-1 112 pair of auto-regressive language mod- els. including 7B-parameter J1-Large model and 178B- parameter J1-Jumbo model. The training vocabulary of Jurassic-1 comprise word pieces. complete words. and multi- word expressions without any word boundaries. where possible out-of-vocabulary instances are interpreted as Unicode bytes. Compared to the GPT-3 counterparts. the Jurassic-1 models apply more balanced depth-to-width self-attention architec- ture 113 and an improved tokenizer for faster prediction",
    "chunk_index": 73,
    "start_pos": 32014,
    "end_pos": 32487,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 509
    }
  },
  "783": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_74",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "and an improved tokenizer for faster prediction based on broader resources. achieving comparable perfor- mance in zero-shot learning tasks and superior performance in few-shot learning tasks given the ability to feed more examples as prompt. HyperCLOVA 114 Korean language model with GPT-3 architecture. Yuan 1.0 115 Trained on Chinese corpus with 5TB of high-quality text collected from the Internet. Massive Data Filtering System MDFS built on Spark is developed to pro-",
    "chunk_index": 74,
    "start_pos": 32488,
    "end_pos": 32932,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 472
    }
  },
  "784": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_75",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "System MDFS built on Spark is developed to pro- cess the raw data via coarse and fine filtering techniques. To speed up the training of Yuan 1.0 to save energy expenses and carbon emissions. various factors that improve the performance of distributed training are incorporated in architecture and train- ing. like increasing the hidden state size improves pipeline and tensor parallelism performance. larger micro batches improve pipeline parallelism performance. and larger global batch size",
    "chunk_index": 75,
    "start_pos": 32933,
    "end_pos": 33377,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 492
    }
  },
  "785": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_76",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "allelism performance. and larger global batch size improve data parallelism performance. In practice. the Yuan 1.0 model performs well on text classification. Winograd Schema. natural language inference. and reading comprehension tasks. Gopher 116 The Gopher family of models ranges from 44M to 280B parameters in size to study the ffect of scale on the LLMs performance. The 280B model beats GPT-3 Jurrasic-1 112 MT-NLG 117 and others on 81 of the evaluated tasks. ERNIE 3.0 TITAN 35 ERNIE 3.0 Titan extends ERNIE 3.0",
    "chunk_index": 76,
    "start_pos": 33378,
    "end_pos": 33865,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 518
    }
  },
  "786": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_77",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "3.0 TITAN 35 ERNIE 3.0 Titan extends ERNIE 3.0 by training larger model with 26x the number of parameters of the latter. This bigger model outperformed other state-of-the- art models in 68 NLP tasks. LLMs produce text with incorrect facts. In order to have control of the generated text with fac- tual consistency. ERNIE 3.0 Titan adds another task. Credible and Controllable Generations to its multi-task learning setup.",
    "chunk_index": 77,
    "start_pos": 33866,
    "end_pos": 34246,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 431,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 430,
      "optimized_content_length": 421
    }
  },
  "787": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_78",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Generations to its multi-task learning setup. --- Page --- It introduces additional self-supervised adversarial and control- lable language modeling losses to the pre-training step. which enables ERNIE 3.0 Titan to beat other LLMs in their manually selected Factual QA task set evaluations. GPT-NeoX-20B 118 An auto-regressive model that largely follows GPT-3 with few deviations in architecture design. trained on the Pile dataset without any data deduplication. GPT- NeoX has parallel attention and feed-forward layers in trans-",
    "chunk_index": 78,
    "start_pos": 34248,
    "end_pos": 34741,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 530
    }
  },
  "788": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_79",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "llel attention and feed-forward layers in trans- former block. given in Eq. 4. that increases throughput by 15 It uses rotary positional embedding 66 applying it to only 25 of embedding vector dimension as in 119 This reduces the computation without performance degradation. As opposed to GPT-3. which uses dense and sparse layers. GPT-NeoX-20B uses only dense layers. The hyperparameter tuning at this scale is difficult. therefore. the model chooses hyperparameters from the method and interpolates values between 13B and 175B",
    "chunk_index": 79,
    "start_pos": 34742,
    "end_pos": 35234,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 528
    }
  },
  "789": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_80",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "and interpolates values between 13B and 175B models for the 20B model. The model training is distributed among GPUs using both tensor and pipeline parallelism. Attn LN1 FF LN2 OPT 14 It is clone of GPT-3. developed to open-source model that replicates GPT-3 performance. Training of OPT employs dynamic loss scaling 120 and restarts from an earlier checkpoint with lower learning rate whenever loss divergence is observed. Overall. the performance of OPT-175B models is comparable to the GPT3-175B model.",
    "chunk_index": 80,
    "start_pos": 35235,
    "end_pos": 35719,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 504
    }
  },
  "790": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_81",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "-175B models is comparable to the GPT3-175B model. BLOOM 13 causal decoder model trained on the ROOTS corpus to open-source an LLM. The architecture of BLOOM is shown in Figure 9. with di fferences like ALiBi positional em- bedding. an additional normalization layer after the embedding layer as suggested by the bitsandbytes1library. These changes stabilize training with improved downstream performance. GLaM 91 Generalist Language Model GLaM represents family of language models using sparsely activated decoder-",
    "chunk_index": 81,
    "start_pos": 35720,
    "end_pos": 36198,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 515
    }
  },
  "791": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_82",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "anguage models using sparsely activated decoder- only mixture-of-experts MoE structure 121. 90 To gain more model capacity while reducing computation. the experts are sparsely activated where only the best two experts are used to process each input token. The largest GLaM model. GLaM 64B 64E is about larger than GPT-3 while only part of the parameters are activated per input token. The largest GLaM 64B 64E model achieves better overall results as compared to GPT-3 while consuming only one-third of GPT-3 training energy.",
    "chunk_index": 82,
    "start_pos": 36199,
    "end_pos": 36694,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 525
    }
  },
  "792": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_83",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nsuming only one-third of GPT-3 training energy. MT-NLG 117 530B causal decoder based on the GPT- architecture that has roughly GPT-3 model parameters. MT-NLG is trained on filtered high-quality data collected from various public datasets and blends various types of datasets in single batch. which beats GPT-3 on several evaluations. Chinchilla 96 causal decoder trained on the same dataset as the Gopher 116 but with little di fferent data sampling distribution sampled from MassiveText The model architec-",
    "chunk_index": 83,
    "start_pos": 36695,
    "end_pos": 37178,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 508
    }
  },
  "793": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_84",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "on sampled from MassiveText The model architec- ture is similar to the one used for Gopher. with the exception of AdamW optimizer instead of Adam. Chinchilla identifies the 1https. github.com TimDettmers bitsandbytes Figure 9. The BLOOM architecture example sourced from 13 relationship that model size should be doubled for every dou- bling of training tokens. Over 400 language models ranging from 70 million to over 16 billion parameters on to 500 bil- lion tokens are trained to get the estimates for compute-optimal",
    "chunk_index": 84,
    "start_pos": 37179,
    "end_pos": 37660,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 520
    }
  },
  "794": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_85",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "trained to get the estimates for compute-optimal training under given budget. The authors train 70B model with the same compute budget as Gopher 280B but with times more data. It outperforms Gopher 116 GPT-3 and others on various downstream tasks. after fine-tuning. AlexaTM 122 An encoder-decoder model. where encoder weights and decoder embeddings are initialized with pre- trained encoder to speed up training. The encoder stays frozen for the initial 100k steps and is later unfrozen for end-to-end",
    "chunk_index": 85,
    "start_pos": 37661,
    "end_pos": 38135,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 502
    }
  },
  "795": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_86",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "al 100k steps and is later unfrozen for end-to-end training. The model is trained on combination of denoising and causal language modeling CLM objectives. concatenat- ing CLM token at the beginning for mode switching. Dur- ing training. the CLM task is applied for 20 of the time. which improves the in-context learning performance. PaLM 15 causal decoder with parallel attention and feed-forward layers similar to Eq. 4. speeding up training by factor of 15. Additional changes to the conventional trans-",
    "chunk_index": 86,
    "start_pos": 38136,
    "end_pos": 38608,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 505
    }
  },
  "796": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_87",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "15. Additional changes to the conventional trans- former model include SwiGLU activation. RoPE embeddings. multi-query attention that saves computation cost during decod- ing. and shared input-output embeddings. During training. loss spiking was observed. and to fix it. model training was restarted from 100-step earlier checkpoint by skipping 200-500 batches around the spike. Moreover. the model was found to memo- rize around 2.4 of the training data at the 540B model scale. whereas this number was lower for smaller models.",
    "chunk_index": 87,
    "start_pos": 38609,
    "end_pos": 39091,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 529
    }
  },
  "797": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_88",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "whereas this number was lower for smaller models. PaLM-2 123 smaller multi-lingual variant of PaLM. trained for larger iterations on better quality dataset. PaLM- shows significant improvements over PaLM. while reducing training and inference costs due to its smaller size. To lessen toxicity and memorization. it appends special tokens with fraction of pre-training data. which shows reduction in gener- ating harmful responses. U-PaLM 124 This method trains PaLM for 0.1 addi-",
    "chunk_index": 88,
    "start_pos": 39092,
    "end_pos": 39537,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 478
    }
  },
  "798": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_89",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "PaLM 124 This method trains PaLM for 0.1 addi- tional compute with the UL2 also named as UL2Restore ob- jective 125 using the same dataset it outperforms the baseline significantly on various NLP tasks. including zero-shot. few- shot. commonsense reasoning. CoT. etc. Training with UL2R involves converting causal decoder PaLM to non-causal de- coder PaLM and employing 50 sequential denoising. 25 regular denoising. and 25 extreme denoising loss functions.",
    "chunk_index": 89,
    "start_pos": 39538,
    "end_pos": 39962,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 475,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 475,
      "optimized_content_length": 457
    }
  },
  "799": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_90",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ising. and 25 extreme denoising loss functions. --- Page 10 --- UL2 125 An encoder-decoder architecture trained using mixture of denoisers MoD objective. Denoisers include R-Denoiser. regular span masking. S-Denoiser. which cor- rupts consecutive tokens of large sequence and X-Denoiser. which corrupts large number of tokens randomly. During pre- training. UL2 includes denoiser token from R.S.Xto rep- resent denoising setup. It helps improve fine-tuning perfor-",
    "chunk_index": 90,
    "start_pos": 39964,
    "end_pos": 40406,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 464
    }
  },
  "800": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_91",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "oising setup. It helps improve fine-tuning perfor- mance for downstream tasks that bind the task to one of the up- stream training modes. This MoD style of training outperforms the T5 model on many benchmarks. GLM-130B 33 GLM-130B is bilingual English and Chi- nese model trained using an auto-regressive mask infilling pre- training objective similar to the GLM 126 This training style makes the model bidirectional as compared to GPT-3. which is unidirectional. As opposed to GLM. the training of GLM-130B",
    "chunk_index": 91,
    "start_pos": 40407,
    "end_pos": 40873,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 507
    }
  },
  "801": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_92",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ional. As opposed to GLM. the training of GLM-130B includes small amount of multi-task instruction pre-training data of the total data along with self-supervised mask in- filling. To stabilize the training. it applies embedding layer gra- dient shrink. LLaMA 127. 21 set of decoder-only language models varying from 7B to 70B parameters. LLaMA models series is the most famous among the community for parameter fficiency and instruction tuning. LLaMA-1 127 Implements fficient causal attention 128",
    "chunk_index": 92,
    "start_pos": 40874,
    "end_pos": 41341,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 497
    }
  },
  "802": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_93",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "127 Implements fficient causal attention 128 by not storing and computing masked attention weights and key query scores. Another optimization is reducing the number of activations recomputed in the backward pass. as in 129 LLaMA-2 21 This work is more focused on fine-tuning safer and better LLaMA-2-Chat model for dialogue generation. The pre-trained model has 40 more training data with larger context length and grouped-query attention. LLaMA-3 3.1 130 collection of models trained on",
    "chunk_index": 93,
    "start_pos": 41342,
    "end_pos": 41802,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 487
    }
  },
  "803": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_94",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "A-3 3.1 130 collection of models trained on seven times larger dataset as compared to LLaMA-2 with dou- ble the context length. outperforming its previous variants and other models. PanGu- 92 An autoregressive model with parameters copied from PanGu- \u03b1and extended to trillion scale with Ran- dom Routed Experts RRE the architectural diagram is shown in Figure 10. RRE is similar to the MoE architecture. with distinctions at the second level. where tokens are randomly routed to experts in domain instead of using learnable gat-",
    "chunk_index": 94,
    "start_pos": 41803,
    "end_pos": 42301,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 529
    }
  },
  "804": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_95",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "erts in domain instead of using learnable gat- ing method. The model has bottom layers densely activated and shared across all domains. whereas top layers are sparsely ac- tivated according to the domain. This training style allows for extracting task-specific models and reduces catastrophic forget- ting ffects in the case of continual learning. Mixtral8x22b 131 mixture-of-experts MoE model with eight distinct experts routes each token to two experts at each layer and combines the outputs additively.",
    "chunk_index": 95,
    "start_pos": 42302,
    "end_pos": 42769,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 505
    }
  },
  "805": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_96",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "at each layer and combines the outputs additively. Snowflake Arctic 132 Arctic LLM is hybrid of dense and mixture-of-experts MoE architecture. The MoE 128 3.66B MLP experts is parallel to the dense transformer 10B with only two experts activated. The model has many experts. com- pared to other MoE LLMs 131. 133 to increase the model capacity and provide an opportunity to choose among many ex- perts for diverse configuration. The model has 480B param-",
    "chunk_index": 96,
    "start_pos": 42770,
    "end_pos": 43190,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 471,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 471,
      "optimized_content_length": 454
    }
  },
  "806": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_97",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "diverse configuration. The model has 480B param- eters. and only 17B are active during forward pass. reducingthe computation significantly. Grok 133. 134 Grok is family of LLMs including Grok-1 and Grok-1.5. released by XAI. Grok-1 133 Grok-1 is 314B parameters language MoE model eight experts where two experts are activated per to- ken. Grok-1.5 134 Grok-1.5 is multi-modal LLM with larger context length and improved performance. Gemini 135. 136 Gemini replaces Bard based on PaLM",
    "chunk_index": 97,
    "start_pos": 43191,
    "end_pos": 43653,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 484
    }
  },
  "807": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_98",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "135. 136 Gemini replaces Bard based on PaLM with multi-modal capabilities and significant language model- ing performance improvements. Gemini-1 135 The first-ever auto-regressive model to achieve human-level capabilities on the MMLU benchmark. Gemini-1.5 136 multi-modal LLM with MoE architec- ture builds on the findings of Gemini-1. The model has 2M context window and can reason over information up to 10M tokens. Such large context windows were never achieved pre- viously and shown to have huge impact on performance gain.",
    "chunk_index": 98,
    "start_pos": 43654,
    "end_pos": 44150,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 528
    }
  },
  "808": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_99",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "shown to have huge impact on performance gain. Nemotron-4 340B 137 decoder-only model that has been aligned on 98 synthetic data and only manually annotated data. Utilizing synthetic data at large proportion improves the model performance significantly. The paper suggested intro- ducing alignment data with smaller subset of previously seen data during the late stage of the model pre-training. enabling the smooth transition from the pre-trained stage to the final train-",
    "chunk_index": 99,
    "start_pos": 44151,
    "end_pos": 44590,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 473
    }
  },
  "809": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_100",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ion from the pre-trained stage to the final train- ing stage. To train better instruction-following models. weaker models are trained into stronger models iteratively. The syn- thetic data generated by the weaker instruction-tuned model is used to train base model which is later supervised fine-tuned outperforming the weaker model. DeepSeek 138 DeepSeek studies the LLMs scaling laws in detail to determine the optimal non-embedding model size and training data. The experiments were performed for bud-",
    "chunk_index": 100,
    "start_pos": 44591,
    "end_pos": 45051,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 504
    }
  },
  "810": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_101",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng data. The experiments were performed for bud- gets ranging from 1e17to 3e20training FLOPs. Each compute budget was tested against ten di fferent models data scales. The batch size and learning rates were also fitted for the given com- pute budget finding that the batch size should increase with the increased compute budget while decreasing the learning rate. Following are the equations for the optimal batch-size learning rate model size and data Bopt 0.2920.C0.3271 \u03b7opt 0.3118.C 0.1250 Mopt Mbase.Ca Dopt Dbase.Cb",
    "chunk_index": 101,
    "start_pos": 45052,
    "end_pos": 45549,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 521
    }
  },
  "811": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_102",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "\u03b7opt 0.3118.C 0.1250 Mopt Mbase.Ca Dopt Dbase.Cb Mbase 0.1715.Dbase 5.8316.a 0.5243.b 0.4757 DeepSeek-v2 139 An MoE model that introduces multi- head latent attention MLA to reduce inference costs. by com- pressing Key-Value KV cache into latent vector. MLA achieves better performance than multi-head attention MHA and other fficient attention mechanisms such as grouped query attention GQA multi-query attention MQA etc. Because of MLA. DeepSeek-v2 achieves 5.76 times faster inference",
    "chunk_index": 102,
    "start_pos": 45550,
    "end_pos": 46011,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 487
    }
  },
  "812": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_103",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "DeepSeek-v2 achieves 5.76 times faster inference throughput as compared to DeepSeek 138 10",
    "chunk_index": 103,
    "start_pos": 46012,
    "end_pos": 46056,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 95,
      "word_count": 14,
      "optimized_for_embedding": true,
      "original_content_length": 95,
      "optimized_content_length": 90
    }
  },
  "813": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_104",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "throughput as compared to DeepSeek 138 10 --- Page 11 --- 3.1.2. Coding CodeGen 140 CodeGen has similar architecture to PaLM 15 i.e. parallel attention. MLP layers. and RoPE em- beddings. The model is trained on both natural language and programming language data sequentially trained on the first dataset. then the second. and so on on the following datasets PILE. BIGQUERY and BIGPYTHON. CodeGen pro- posed multi-step approach to synthesizing code. The purpose",
    "chunk_index": 104,
    "start_pos": 46058,
    "end_pos": 46502,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 489,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 489,
      "optimized_content_length": 462
    }
  },
  "814": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_105",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ti-step approach to synthesizing code. The purpose is to simplify the generation of long sequences where the previ- ous prompt and generated code are given as input with the next prompt to generate the next code sequence. CodeGen open- source Multi-Turn Programming Benchmark MTPB to eval- uate multi-step program synthesis. Codex 141 This LLM is trained on subset of public Python Github repositories to generate code from docstrings. Com- puter programming is an iterative process where the programs",
    "chunk_index": 105,
    "start_pos": 46503,
    "end_pos": 46962,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 501
    }
  },
  "815": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_106",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ramming is an iterative process where the programs are often debugged and updated before fulfilling the require- ments. Similarly. Codex generates 100 versions of program by repetitive sampling for given description. which produces working solution for 77.5 of the problems passing unit tests. Its powerful version powers Github Copilot2. AlphaCode 142 set of large language models. ranging from 300M to 41B parameters. designed for competition-level code generation tasks. It uses the multi-query attention 143 to",
    "chunk_index": 106,
    "start_pos": 46963,
    "end_pos": 47440,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 514
    }
  },
  "816": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_107",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tasks. It uses the multi-query attention 143 to reduce memory and cache costs. Since competitive program- ming problems highly require deep reasoning and an under- standing of complex natural language algorithms. the Alpha- Code models are pre-trained on filtered GitHub code in popular languages and then fine-tuned on new competitive program- ming dataset named CodeContests. The CodeContests dataset mainly contains problems. solutions. and test cases collected from the Codeforces platform3. The pre-training employs stan-",
    "chunk_index": 107,
    "start_pos": 47441,
    "end_pos": 47921,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 526
    }
  },
  "817": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_108",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "deforces platform3. The pre-training employs stan- dard language modeling objectives. while GOLD 144 with tempering 145 serves as the training objective for the fine- tuning on CodeContests data. To evaluate the performance of AlphaCode. simulated programming competitions are hosted on the Codeforces platform. overall. AlphaCode ranks at the top 54.3 among over 5000 competitors. where its Codeforces rating is within the top 28 of recently participated users. CodeT5 34 CodeT5 is based on CodeT5 146 with",
    "chunk_index": 108,
    "start_pos": 47922,
    "end_pos": 48392,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 507
    }
  },
  "818": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_109",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "deT5 34 CodeT5 is based on CodeT5 146 with shallow encoder and deep decoder. trained in multiple stages initially unimodal data code and later bimodal data text-code pairs Each training stage has di fferent training objectives and activates di fferent model blocks encoder. decoder. or both ac- cording to the task. The unimodal pre-training includes span denoising and CLM objectives. whereas bimodal pre-training objectives contain contrastive learning. matching. and CLM for",
    "chunk_index": 109,
    "start_pos": 48393,
    "end_pos": 48832,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 477
    }
  },
  "819": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_110",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ontain contrastive learning. matching. and CLM for text-code pairs. CodeT5 adds special tokens with the text to enable task modes. for example. CLS for contrastive loss. Match for text-code matching. etc. StarCoder 147 decoder-only model with the SantaCoder architecture. employing Flash attention to scale up the context length to 8k. The StarCoder trains an encoder to filter names. 2https. github.com features copilot 3https. codeforces.com emails. and other personal data from the training data. Its fine-",
    "chunk_index": 110,
    "start_pos": 48833,
    "end_pos": 49311,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 509
    }
  },
  "820": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_111",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "er personal data from the training data. Its fine- tuned variant outperforms PaLM. LLaMA. and LAMDA on HumanEval and MBPP benchmarks. 3.1.3. Scientific Knowledge Galactica 148 large curated corpus of human scientific knowledge with 48 million papers. textbooks. lecture notes. millions of compounds and proteins. scientific websites. en- cyclopedias. and more are trained using the metaseq library3. which is built on PyTorch and fairscale 149 The model wraps reasoning datasets with the work token to provide step-by-",
    "chunk_index": 111,
    "start_pos": 49312,
    "end_pos": 49788,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 518
    }
  },
  "821": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_112",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "datasets with the work token to provide step-by- step reasoning context to the model. which has been shown to improve the performance on reasoning tasks. 3.1.4. Dialog LaMDA 150 decoder-only model pre-trained on pub- lic dialog data. public dialog utterances. and public web doc- uments. where more than 90 of the pre-training data is in English. LaMDA is trained with the objective of producing re- sponses that exhibit high levels of quality. safety. and grounded- ness. To achieve this. discriminative and generative fine-tuning",
    "chunk_index": 112,
    "start_pos": 49789,
    "end_pos": 50277,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 531
    }
  },
  "822": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_113",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ve this. discriminative and generative fine-tuning techniques are incorporated to enhance the model safety and quality aspects. As result. the LaMDA models can be utilized as general language model performing various tasks. 3.1.5. Finance BloombergGPT 151 non-causal decoder model trained using both financial FINPILE from the Bloomberg archive and general-purpose datasets. The model architecture is sim- ilar to the BLOOM 13 and OPT 14 It allocates 50B param- eters to di fferent blocks of the model using the approach 113",
    "chunk_index": 113,
    "start_pos": 50278,
    "end_pos": 50776,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 524
    }
  },
  "823": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_114",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rent blocks of the model using the approach 113 For ffective training. BloombergGPT packs documents to- gether with endo text to use the maximum sequence length. uses warmup batch size starting from 1024 to 2048. and manually reduces the learning rate multiple times during the training. Xuan Yuan 2.0 152 Chinese financial chat model with BLOOM 13 architecture trained on combination of general purpose. financial. general purpose instructions. and financial institutions datasets. Xuan Yuan 2.0 combined the pre-training",
    "chunk_index": 114,
    "start_pos": 50777,
    "end_pos": 51269,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 522
    }
  },
  "824": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_115",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "datasets. Xuan Yuan 2.0 combined the pre-training and fine-tuning stages to avoid catastrophic forgetting. 3.2. Fine-Tuned LLMs Pre-trained LLMs have excellent generalization abilities to unseen tasks. However. because they are generally trained with the objective of next token prediction. LLMs have limited ca- pacity to follow user intent and are prone to generate unethical. toxic or inaccurate responses 20 For their ffective utiliza- tion. LLMs are fine-tuned to follow instructions 16. 17. 97 and",
    "chunk_index": 115,
    "start_pos": 51270,
    "end_pos": 51730,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 503
    }
  },
  "825": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_116",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "fine-tuned to follow instructions 16. 17. 97 and generate safe responses 20 which also results in increasing zero-shot. few-shot. and cross-task generalization 97. 16. 18 with minimal compute increment. e.g. 0.2 of the total pre- training for PaLM 540B 16 We review various fine-tuned LLMs and strategies for ffective fine-tuning in this section. 11",
    "chunk_index": 116,
    "start_pos": 51731,
    "end_pos": 52044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 364,
      "word_count": 55,
      "optimized_for_embedding": true,
      "original_content_length": 364,
      "optimized_content_length": 349
    }
  },
  "826": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_117",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ies for ffective fine-tuning in this section. 11 --- Page 12 --- Table 1. Noteworthy findings and insights of pre-trained Large Language Models. Models Findings Insights T5 Encoder and decoder with shared parameters perform equivalently when parameters are not shared Fine-tuning model layers adapter layers work better than the conventional way of training on only classification layers GPT-3 Few-shot performance of LLMs is better than the zero-shot. suggesting that LLMs are meta- learners",
    "chunk_index": 117,
    "start_pos": 52046,
    "end_pos": 52494,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 492
    }
  },
  "827": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_118",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "zero-shot. suggesting that LLMs are meta- learners mT5 Large multi-lingual models perform equivalently to single language models on downstream tasks. However. smaller multi-lingual models perform worse PanGu-\u03b1 LLMs have good few shot capabilities CPM-2 Prompt fine-tuning requires updating very few parameters while achieving performance compara- ble to full model fine-tuning Prompt fine-tuning takes more time to converge as compared to full model fine-tuning",
    "chunk_index": 118,
    "start_pos": 52495,
    "end_pos": 52907,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 463,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 463,
      "optimized_content_length": 461
    }
  },
  "828": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_119",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "to converge as compared to full model fine-tuning Inserting prompt tokens in-between sentences can allow the model to understand relations between sentences and long sequences In an analysis. CPM-2 finds that prompts work as provider additional context and aggregator aggregate information with the input text for the model ERNIE 3.0 modular LLM architecture with universal representation module and task-specific representa- tion module helps in the finetuning phase",
    "chunk_index": 119,
    "start_pos": 52908,
    "end_pos": 53337,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 479,
      "optimized_content_length": 467
    }
  },
  "829": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_120",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "resenta- tion module helps in the finetuning phase Optimizing the parameters of task-specific representation network during the fine-tuning phase is an efficient way to take advantage of the powerful pre-trained model Jurassic-1 The performance of LLM is highly related to the network size To improve runtime performance. more operations can be performed in parallel width rather than sequential depth To efficiently represent and fit more text in the same context length. the model uses larger vo-",
    "chunk_index": 120,
    "start_pos": 53338,
    "end_pos": 53796,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 498
    }
  },
  "830": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_121",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "same context length. the model uses larger vo- cabulary to train SentencePiece tokenizer without restricting it to word boundaries. This further benefits in few-shot learning tasks HyperCLOV By employing prompt-based tuning. the performances of models can be improved. often surpassing those of state-of-the-art models when the backward gradients of inputs are accessible Yuan 1.0 The model architecture that excels in pre-training and fine-tuning cases may exhibit contrasting behavior in zero-shot and few-shot learning",
    "chunk_index": 121,
    "start_pos": 53797,
    "end_pos": 54275,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 521
    }
  },
  "831": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_122",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "asting behavior in zero-shot and few-shot learning Gopher Relative encodings enable the model to evaluate for longer sequences than training. ERNIE 3.0 Titan Additional self-supervised adversarial loss to distinguish between real and generated text improves the model performance as compared to ERNIE 3.0 GPT-NeoX-20B Parallel attention FF layers speed-up training 15 with the same performance as with cascaded layers Initializing feed-forward output layers before residuals with scheme in 153 avoids activations",
    "chunk_index": 122,
    "start_pos": 54276,
    "end_pos": 54743,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 512
    }
  },
  "832": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_123",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "residuals with scheme in 153 avoids activations from growing with increasing depth and width Training on Pile outperforms GPT-3 on five-shot Table Continued on Next Page 12",
    "chunk_index": 123,
    "start_pos": 54744,
    "end_pos": 54869,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 176,
      "word_count": 27,
      "optimized_for_embedding": true,
      "original_content_length": 175,
      "optimized_content_length": 172
    }
  },
  "833": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_124",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "GPT-3 on five-shot Table Continued on Next Page 12 --- Page 13 --- Models Findings Insights OPT Restart training from an earlier checkpoint with lower learning rate if loss diverges Model is prone to generate repetitive text and stuck in loop Galactica Galactica performance has continued to improve across validation set. in-domain. and out-of- domain benchmarks. even with multiple repetitions of the corpus. which is superior to existing research on LLMs",
    "chunk_index": 124,
    "start_pos": 54871,
    "end_pos": 55286,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 466,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 466,
      "optimized_content_length": 457
    }
  },
  "834": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_125",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "us. which is superior to existing research on LLMs working memory token approach can achieve strong performance over existing methods on mathematical MMLU and MATH benchmarks. It sets new state-of-the-art on several downstream tasks such as PubMedQA 77.6 and MedMCQA dev 52.9 GLaM The model capacity can be maintained at reduced computation by replacing the feed-forward layer in each transformer layer with mixture-of-experts MoE The model trained on filtered data shows consistently better performances on both NLG and NLU",
    "chunk_index": 125,
    "start_pos": 55287,
    "end_pos": 55776,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 524
    }
  },
  "835": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_126",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nsistently better performances on both NLG and NLU tasks. where the ffect of filtering is more significant on the former tasks Filtered pretraining corpora play crucial role in the generation capability of LLMs. especially for the downstream tasks The scaling of GLaM MoE models can be achieved by increasing the size or number of experts in the MoE layer. Given fixed budget of computation. more experts contribute to better perfor- mance",
    "chunk_index": 126,
    "start_pos": 55777,
    "end_pos": 56175,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 449,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 449,
      "optimized_content_length": 439
    }
  },
  "836": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_127",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "more experts contribute to better perfor- mance LaMDA The model can be fine-tuned to learn to call di fferent external information resources and tools AlphaCode For higher ffectiveness and fficiency. transformer model can be asymmetrically constructed with shallower encoder and deeper decoder To achieve better performances. it is necessary to employ strategies such as massively scaling upsampling. followed by the filtering and clustering of samples into compact set",
    "chunk_index": 127,
    "start_pos": 56176,
    "end_pos": 56611,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 469
    }
  },
  "837": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_128",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ering and clustering of samples into compact set The utilization of novel sampling-e fficient transformer architectures designed to facilitate large- scale sampling is crucial Simplifying problem descriptions can ffectively improve the model performance Chinchilla The model size and the number of training tokens should be scaled proportionately. for each dou- bling of the model size. the number of training tokens should be doubled as well",
    "chunk_index": 128,
    "start_pos": 56612,
    "end_pos": 57011,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 450,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 450,
      "optimized_content_length": 442
    }
  },
  "838": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_129",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "umber of training tokens should be doubled as well PaLM English-centric models produce better translations when translating to English as compared to non- English Generalized models can have equivalent performance for language translation to specialized small models Larger models have higher percentage of training data memorization Performance has not yet saturated even at 540B scale. which means larger models are likely to perform better",
    "chunk_index": 129,
    "start_pos": 57012,
    "end_pos": 57408,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 447,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 447,
      "optimized_content_length": 442
    }
  },
  "839": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_130",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "means larger models are likely to perform better AlexaTM Encoder-decoder architecture is more suitable to train LLMs given bidirectional attention to the context than decoder-only Causal Language Modeling CLM task can be added to benefit the model with fficient in-context learning Placing layer norm at the beginning of each transformer layer improves the training stability Table Continued on Next Page 13",
    "chunk_index": 130,
    "start_pos": 57409,
    "end_pos": 57773,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 415,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 415,
      "optimized_content_length": 407
    }
  },
  "840": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_131",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "training stability Table Continued on Next Page 13 --- Page 14 --- Models Findings Insights U-PaLM Training with mixture of denoisers outperforms PaLM when trained further for few more FLOPs Training with mixture of denoisers improves the infilling ability and open-ended text generation diversity UL2 Mode switching training enables better performance on downstream tasks CoT prompting outperforms standard prompting for UL2 GLM-130B Pre-training data with small proportion of multi-task instruction data improves the overall model",
    "chunk_index": 131,
    "start_pos": 57775,
    "end_pos": 58268,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 532
    }
  },
  "841": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_132",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "i-task instruction data improves the overall model performance CodeGen Multi-step prompting for code synthesis leads to better user intent understanding and code gen- eration LLaMA constant performance improvement is observed when scaling the model Smaller models can achieve good performances with more training data and computing time PanGu- Sparse models provide the benefits of large models at lower computation cost Randomly Routed Experts reduces catastrophic forgetting ffects which in turn is essential for continual learning",
    "chunk_index": 132,
    "start_pos": 58269,
    "end_pos": 58763,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 533
    }
  },
  "842": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_133",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "which in turn is essential for continual learning Randomly Routed Experts allow extracting domain-specific sub-model in deployment which is cost-e fficient while maintaining performance similar to the original BloombergGPT Pre-training with general-purpose and task-specific data improves task performance without hurt- ing other model capabilities XuanYuan 2.0 Combining pre-training and fine-tuning stages in single training avoids catastrophic forgetting",
    "chunk_index": 133,
    "start_pos": 58764,
    "end_pos": 59177,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 464,
      "word_count": 58,
      "optimized_for_embedding": true,
      "original_content_length": 463,
      "optimized_content_length": 457
    }
  },
  "843": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_134",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "in single training avoids catastrophic forgetting CodeT5 Causal LM is crucial for model generation capability in encoder-decoder architectures Multiple training objectives like span corruption. Causal LM. matching. etc complement each other for better performance StarCoder HHH prompt by Anthropic allows the model to follow instructions without fine-tuning LLaMA-2 Model trained on unfiltered data is more toxic but may perform better on downstream tasks after fine-tuning",
    "chunk_index": 134,
    "start_pos": 59178,
    "end_pos": 59609,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 482,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 473
    }
  },
  "844": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_135",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rform better on downstream tasks after fine-tuning Model trained on unfiltered data requires fewer samples for safety alignment PaLM-2 Data quality is important to train better models Model and data size should be scaled with 1.1 proportions Smaller models trained for larger iterations outperform larger models LLaMA-3 3.1 Increasing batch size gradually stabilizes the training without loss spikes High-quality data at the final stages of training improves the model performance",
    "chunk_index": 135,
    "start_pos": 59610,
    "end_pos": 60044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 480
    }
  },
  "845": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_136",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "stages of training improves the model performance Increasing model context length windows step-wise allows it to better adapt to various sequence lengths Nemotron-40B Model aligned iteratively on synthetic data with data generated from the previously aligned model achieves competitive performance DeepSeek Batch size should increase with the increase in compute budget while decreasing the learning rate DeepSeek-v2 Mult-head latent attention MLA performs better than multi-head attention MHA while requiring",
    "chunk_index": 136,
    "start_pos": 60045,
    "end_pos": 60510,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 509
    }
  },
  "846": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_137",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "er than multi-head attention MHA while requiring significantly smaller KV cache. therefore achieving faster data generation 14",
    "chunk_index": 137,
    "start_pos": 60511,
    "end_pos": 60590,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 130,
      "word_count": 18,
      "optimized_for_embedding": true,
      "original_content_length": 130,
      "optimized_content_length": 126
    }
  },
  "847": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_138",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "che. therefore achieving faster data generation 14 --- Page 15 --- Table 2. Key insights and findings from the study of instruction-tuned Large Language Models. Models Findings Insights T0 Multi-task prompting enables zero-shot generalization and outperforms baselines Even single prompt per dataset task is enough to improve performance WebGPT To aid the model in ffectively filtering and utilizing relevant information. human labelers play crucial role in answering questions regarding the usefulness of the retrieved documents",
    "chunk_index": 138,
    "start_pos": 60592,
    "end_pos": 61079,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 529
    }
  },
  "848": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_139",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "egarding the usefulness of the retrieved documents Interacting fine-tuned language model with text-based web-browsing environment can improve end-to-end retrieval and synthesis via imitation learning and reinforcement learning Generating answers with references can make labelers easily judge the factual accuracy of answers Tk-INSTRUCT Instruction tuning leads to stronger generalization of unseen tasks More tasks improve generalization whereas only increasing task instances does not help",
    "chunk_index": 139,
    "start_pos": 61080,
    "end_pos": 61529,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 491
    }
  },
  "849": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_140",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ereas only increasing task instances does not help Supervised trained models are better than generalized models Models pre-trained with instructions and examples perform well for di fferent types of inputs mT0 and BLOOMZ Instruction tuning enables zero-shot generalization to tasks never seen before Multi-lingual training leads to even better zero-shot generalization for both English and non- English Training on machine-translated prompts improves performance for held-out tasks with non-English prompts",
    "chunk_index": 140,
    "start_pos": 61530,
    "end_pos": 61989,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 506
    }
  },
  "850": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_141",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rmance for held-out tasks with non-English prompts English only fine-tuning on multilingual pre-trained language model is enough to generalize to other pre-trained language tasks OPT-IML Creating batch with multiple task examples is important for better performance Only example proportional sampling is not enough. training datasets should also be proportional for better generalization performance Fully held-out and partially supervised tasks performance improves by scaling tasks or categories whereas fully supervised tasks have no ffect",
    "chunk_index": 141,
    "start_pos": 61990,
    "end_pos": 62489,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 542
    }
  },
  "851": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_142",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ies whereas fully supervised tasks have no ffect Including small amounts i.e. of pretraining data during fine-tuning is ffective Only reasoning data improves the performance. adding more deteriorates performance Adding dialogue data makes the performance worse Sparrow Labelers judgment and well-defined alignment rules help the model generate better responses Good dialogue goals can be broken down into detailed natural language rules for the agent and the raters",
    "chunk_index": 142,
    "start_pos": 62490,
    "end_pos": 62919,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 480,
      "optimized_content_length": 465
    }
  },
  "852": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_143",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "atural language rules for the agent and the raters The combination of reinforcement learning RL with reranking yields optimal performance in terms of preference win rates and resilience against adversarial probing Flan Finetuning with CoT improves performance on held-out tasks Fine-tuning along with CoT data improves reasoning abilities CoT tuning improves zero-shot reasoning Performance improves with more tasks Instruction fine-tuning improves usability which otherwise is challenging for pre-trained models",
    "chunk_index": 143,
    "start_pos": 62920,
    "end_pos": 63388,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 512
    }
  },
  "853": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_144",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ch otherwise is challenging for pre-trained models Improving the model performance with instruction tuning is compute-e fficient Multitask prompting enables zero-shot generalization abilities in LLM WizardCoder Fine-tuning with re-written instruction-tuning data into complex set improves performance LLaMA-2-Chat Model learns to write safe responses with fine-tuning on safe demonstrations. while additional RLHF step further improves model safety and make it less prone to jailbreak attacks",
    "chunk_index": 144,
    "start_pos": 63389,
    "end_pos": 63837,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 492
    }
  },
  "854": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_145",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "safety and make it less prone to jailbreak attacks LIMA Less high quality data is enough for fine-tuned model generalization 15",
    "chunk_index": 145,
    "start_pos": 63838,
    "end_pos": 63915,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 128,
      "word_count": 21,
      "optimized_for_embedding": true,
      "original_content_length": 128,
      "optimized_content_length": 127
    }
  },
  "855": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_146",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "is enough for fine-tuned model generalization 15 --- Page 16 --- Figure 10. This example illustrates the PanGu-Parchitecture. as depicted in the image sourced from 92 3.2.1. Instruction-Tuning with Manually Created Datasets Numerous hand-crafted instruction-tuning datasets with different design choices are proposed in the literature to instruction-tune LLMs. The performance of fine-tuned LLMs depends on multiple factors. such as dataset. instruction diver- sity. prompting templates. model size. and training objectives.",
    "chunk_index": 146,
    "start_pos": 63917,
    "end_pos": 64395,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 524
    }
  },
  "856": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_147",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng templates. model size. and training objectives. Keeping this in view. diverse fine-tuned models have emerged in the literature using manually created datasets. The models T0 17 and mT0 multi-lingual 154 employ templates to convert existing datasets into prompt datasets. They have shown improvements in generalization to zero-shot and held-out tasks. Tk-Instruct 18 fine-tuned the T5 model with in-context instructions to study generalization on unseen tasks when given in-context instructions during test time. The",
    "chunk_index": 147,
    "start_pos": 64396,
    "end_pos": 64871,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 518
    }
  },
  "857": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_148",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "iven in-context instructions during test time. The model outperformed Instruct-GPT. despite being smaller in size. i.e. 11B parameters as compared to 175B of GPT-3. Increasing Tasks and Prompt Setups. Zero-shot and few-shot performance improves significantly by expanding task collec- tion and prompt styles. OPT-IML 97 and Flan 16 curated larger 2k and 1.8k task datasets. respectively. While increasing task size alone is not enough. OPT-IML and Flan add more prompting setups in their datasets. zero-shot. few-shot. and",
    "chunk_index": 148,
    "start_pos": 64872,
    "end_pos": 65348,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 522
    }
  },
  "858": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_149",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "setups in their datasets. zero-shot. few-shot. and CoT. In continuation. CoT Collection 101 fine-tunes Flan-T5 further on 1.88M CoT samples. Another method 102 uses symbolic tasks with tasks in T0. Flan. etc. 3.2.2. Instruction-Tuning with LLMs Generated Datasets Generating an instruction-tuning dataset requires carefully writing instructions and input-output pairs. which are often written by humans. smaller in size. and less diverse. To over- come this. self-instruct 19 proposed an approach to prompt",
    "chunk_index": 149,
    "start_pos": 65349,
    "end_pos": 65810,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 506
    }
  },
  "859": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_150",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "self-instruct 19 proposed an approach to prompt available LLMs to generate instruction-tuning datasets. Self- instruct outperformed models trained on manually created dataset SUPER-NATURALINSTRUCTIONS dataset with 1600 tasks 18 by 33 It starts with seed of 175 tasks. instruction. and sample per task and iteratively generates new instructions 52k and instances 82k input-output pairs using Figure 11. An example image shows an instance of the Flan training paradigm. taken from 16",
    "chunk_index": 150,
    "start_pos": 65811,
    "end_pos": 66265,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 481
    }
  },
  "860": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_151",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ce of the Flan training paradigm. taken from 16 GPT-3 Contrary to this. Dynosaur 155 uses the meta-data of datasets on Huggingface to prompt LLMs to generate multi- ple task instruction-tuning datasets. LLaMA Tuned. Various models in the literature instruction- tune LLaMA 156 with GPT-3 or GPT-4 157 gener- ated datasets. Among these. Alpaca 158 Vicuna 159 and LLaMA-GPT-4 160 are few general-purpose fine-tuned models. where Alpaca is trained on 52k samples from text-",
    "chunk_index": 151,
    "start_pos": 66266,
    "end_pos": 66713,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 470
    }
  },
  "861": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_152",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "where Alpaca is trained on 52k samples from text- davinci-003. Vicuna on 70k samples from ShareGPT.com. and LLaMA-GPT-4 by re-creating Alpaca instructions from GPT- 4. Goat 161 fine-tunes LLaMA for arithmetic tasks million samples by generating data from ChatGPT and outperforms GPT-4. PaLM. BLOOM. OPT. etc. attributing its success to the LLaMA consistent tokenization of numbers. HuaTuo 162 is medical knowledge model. fine-tuned with generated QA dataset of 8k instructions.",
    "chunk_index": 152,
    "start_pos": 66714,
    "end_pos": 67156,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 477
    }
  },
  "862": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_153",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ed with generated QA dataset of 8k instructions. Complex Instructions. Evol-Instruct 163. 164 prompts LLMs to convert given instructions into more complex set. The in- structions are iteratively evolved with re-writing instructions in complex wording and creating new instructions. With this style of automated instruction generation. WizardLM 163 fine- tuned LLaMA on 250k instructions outperforms Vicuna and Alpaca. and WizardCoder 164 fine-tuned StarCoder beats Claude-Plus. Bard. and others. 3.2.3. Aligning with Human Preferences",
    "chunk_index": 153,
    "start_pos": 67157,
    "end_pos": 67655,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 534
    }
  },
  "863": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_154",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "and others. 3.2.3. Aligning with Human Preferences Incorporating human preferences into LLMs presents significant advantage in mitigating undesirable behaviors and ensuring accurate outputs. The initial work on alignment. such as InstructGPT 20 aligns GPT-3 using 3-step approach. instruction-tuning. reward modeling. and fine-tuning with reinforcement learning RL The supervised fine-tuned GPT-3 on demonstrations is queried to generate responses. which human labelers rank according to human values. and reward",
    "chunk_index": 154,
    "start_pos": 67656,
    "end_pos": 68128,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 512
    }
  },
  "864": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_155",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "elers rank according to human values. and reward model is trained on the ranked data. Lastly. the GPT-3 is trained with proximal policy optimization PPO using rewards on the generated data from the reward model. LLaMA 2-Chat 21 improves alignment by dividing reward modeling into help- fulness and safety rewards and using rejection sampling in addition to PPO. The initial four versions of LLaMA 2-Chat are fine-tuned with rejection sampling and then with PPO on 16",
    "chunk_index": 155,
    "start_pos": 68129,
    "end_pos": 68550,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 472,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 472,
      "optimized_content_length": 466
    }
  },
  "865": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_156",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ed with rejection sampling and then with PPO on 16 --- Page 17 --- top of rejection sampling. Aligning with Supported Evidence. This style of alignment allows the model to generate responses with proofs and facts. reduces hallucination. and assists humans more ffectively. which increases trust in the model output. Similar to the RLHF training style. reward model is trained to rank generated responses containing web citations in answers to questions. which is later used to train the model. as in",
    "chunk_index": 156,
    "start_pos": 68552,
    "end_pos": 69006,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 499
    }
  },
  "866": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_157",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ons. which is later used to train the model. as in GopherCite 165 WebGPT 166 and Sparrow 167 The ranking model in Sparrow 167 is divided into two branches. preference reward and rule reward. where human annotators adversarial probe the model to break rule. These two rewards together rank response to train with RL. Aligning Directly with SFT. The PPO in the RLHF pipeline is complex. memory-intensive. and unstable. requiring mul- tiple models. reward. value. policy. and reference models.",
    "chunk_index": 157,
    "start_pos": 69007,
    "end_pos": 69461,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 490
    }
  },
  "867": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_158",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "dels. reward. value. policy. and reference models. Avoiding this sophisticated alignment pipeline is possible by incorporating minimal changes in the supervised fine-tuning SFT pipeline as in 168. 169. 170 with better or compa- rable performance to PPO. Direct preference optimization DPO 168 trains model directly on the human-preferred responses to maximize the likelihood of preferred against unpreferred responses. with per-sample importance weight. Reward ranked fine-tuning RAFT 169 fine-tunes the model",
    "chunk_index": 158,
    "start_pos": 69462,
    "end_pos": 69933,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 509
    }
  },
  "868": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_159",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ranked fine-tuning RAFT 169 fine-tunes the model on ranked responses by the reward model. Preference ranking optimization PRO 171 and RRHF 170 penalize the model to rank responses with human preferences and supervised loss. On the other hand. chain-of-hindsight CoH 172 provides feedback to the model in language rather than reward. to learn good versus bad responses. Aligning with Synthetic Feedback. Aligning LLMs with human feedback is slow and costly. The literature suggests",
    "chunk_index": 159,
    "start_pos": 69934,
    "end_pos": 70377,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 480
    }
  },
  "869": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_160",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "back is slow and costly. The literature suggests semi-automated process to align LLMs by prompting LLMs to generate helpful. honest. and ethical responses to the queries. and fine-tuning using the newly created dataset. Constitutional AI 173 replaces human feedback in RLHF with AI. calling it RL from AI feedback RLAIF AlpacaFarm 174 designs prompts to imitate human feedback using LLMs APIs. Oppo- site to constitutional AI. AlpacaFarm injects noise in feedback to replicate human mistakes. Self-Align 98 prompts the",
    "chunk_index": 160,
    "start_pos": 70378,
    "end_pos": 70856,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 518
    }
  },
  "870": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_161",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "licate human mistakes. Self-Align 98 prompts the LLM with ICL examples. instructing the LLM about what the response should contain to be considered useful and ethical. The same LLM is later fine-tuned with the new dataset. Aligning with Prompts. LLMs can be steered with prompts to generate desirable responses without training 175. 176 The self-correction prompting in 176 concatenates instructions and CoT with questions. guiding the model to answer its instruction following strategy to ensure moral safety before",
    "chunk_index": 161,
    "start_pos": 70857,
    "end_pos": 71331,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 516
    }
  },
  "871": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_162",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "following strategy to ensure moral safety before the actual answer. This strategy is shown to reduce the harm in generated responses significantly. Red-Teaming Jailbreaking Adversarial Attacks. LLMs exhibit harmful behaviors. hallucinations. leaking personal in- formation. and other shortcomings through adversarial probing. The models are susceptible to generating harmful responses even though they are aligned for safety 177. 178 Red- teaming is common approach to address illicit outputs. where",
    "chunk_index": 162,
    "start_pos": 71332,
    "end_pos": 71789,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 499
    }
  },
  "872": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_163",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "common approach to address illicit outputs. where the LLMs are prompted to generate harmful outputs 178. 179 .The dataset collected through red-teaming is used to fine-tune models for safety. While red-teaming largely relies on human annotators. another work 180 red-team LLMs to find prompts that lead to harmful outputs for other LLMs. 3.2.4. Continue Pre-Training Although fine-tuning boosts model performance. it leads to catastrophic forgetting of previously learned information.",
    "chunk_index": 163,
    "start_pos": 71790,
    "end_pos": 72231,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 484
    }
  },
  "873": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_164",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "phic forgetting of previously learned information. Concatenating fine-tuning data with few randomly selected pre-training samples in every iteration avoids network forget- ting 181. 152 This is also ffective in adapting LLMs for cases where fine-tuning data is small and the original capac- ity is to be maintained. Prompt-based continued pre-training PCP 182 trains the model with text and instructions related to tasks and then finally instruction-tunes the model for down- stream tasks. 3.2.5. Sample fficiency",
    "chunk_index": 164,
    "start_pos": 72232,
    "end_pos": 72707,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 513
    }
  },
  "874": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_165",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "for down- stream tasks. 3.2.5. Sample fficiency While fine-tuning data is generally many-fold smaller than the pre-training data. it still has to be large enough for accept- able performance 16. 97. 18 and requires proportional com- puting resources. Studying the ffects on performance with less data. existing literature 183. 184 finds that models trained on less data can outperform models trained with more data. In 183 25 of the total downstream data is found enough for state-of-the-art performance. Selecting coreset-based 0.5",
    "chunk_index": 165,
    "start_pos": 72708,
    "end_pos": 73203,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 532
    }
  },
  "875": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_166",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "-the-art performance. Selecting coreset-based 0.5 of the total instruction-tuning data improves the model perfor- mance by in 184 as compared to the complete data tun- ing. Less is more for alignment LIMA 185 uses only 1000 carefully created demonstrations to fine-tune the model and has achieved comparable performance to GPT-4. 3.3. Increasing Context Window LLMs are trained with limited context windows due to ex- pensive attention and high memory requirements. model trained on limited sequence lengths fails to generalize to unseen",
    "chunk_index": 166,
    "start_pos": 73204,
    "end_pos": 73703,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 537
    }
  },
  "876": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_167",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ted sequence lengths fails to generalize to unseen lengths at inference time 186. 49 Alternatively. LLMs with ALiBi 65 positional encodings can perform zero-shot length extrapolation. However. ALiBi has less expressive power 66 and inferior performance on multiple benchmarks 46 and many LLMs use RoPE positional embedding that is unable to perform zero-shot extrapolation. larger context length has benefits such as better understanding of longer documents. more samples in in-context learning. execution of bigger rea-",
    "chunk_index": 167,
    "start_pos": 73704,
    "end_pos": 74187,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 520
    }
  },
  "877": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_168",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "in in-context learning. execution of bigger rea- soning processes. etc. Expanding context length during fine- tuning is slow. ine fficient. and computationally expensive 49 Therefore. researchers employ various context window extrap- olation techniques discussed below. Position Interpolation. Rather than extrapolating. 49 shows that interpolating position encodings within the pre-trained con- text window are more ffective. The work demonstrates that only 1000 steps of fine-tuning are enough to achieve better re-",
    "chunk_index": 168,
    "start_pos": 74188,
    "end_pos": 74663,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 517
    }
  },
  "878": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_169",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ps of fine-tuning are enough to achieve better re- sults on larger windows without reducing performance com- pared to the original context size. Gira ffe 46 uses power scal- ing in RoPE. and YaRN 47 proposed NTK-aware interpola- tion. 17",
    "chunk_index": 169,
    "start_pos": 74664,
    "end_pos": 74854,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 241,
      "word_count": 40,
      "optimized_for_embedding": true,
      "original_content_length": 241,
      "optimized_content_length": 237
    }
  },
  "879": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_170",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "YaRN 47 proposed NTK-aware interpola- tion. 17 --- Page 18 --- Efficient Attention Mechanism. Dense global attention is one of the major constraints in training larger context win- dow LLMs. Using fficient attention variants. such as lo- cal. sparse. and dilated attention. reduces the computation cost significantly. LongT5 48 proposes transient global atten- tion TGlobal applying attention to local and global tokens windowed token averaging The model replaces attention in T5 10 with TGlobal attention. pre-trains the model on",
    "chunk_index": 170,
    "start_pos": 74856,
    "end_pos": 75351,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 530
    }
  },
  "880": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_171",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "with TGlobal attention. pre-trains the model on 4098 sequence length. fine-tunes on larger window sizes. as large as 16k. and improves task performance on longer inputs. This shows the extrapolation ability of TGlobal attention with only fine-tuning. COLT5 187 uses two branches. one with lightweight and the other with heavyweight attention and feed- forward layers. All tokens are processed from the lightweight branch. and only important tokens are routed to the heavy- weight branch. LongNet 188 replaces standard attention with",
    "chunk_index": 171,
    "start_pos": 75352,
    "end_pos": 75840,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 532
    }
  },
  "881": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_172",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ch. LongNet 188 replaces standard attention with dilated attention. expanding sequence length to billion tokens. LongLoRA 189 proposes shift-short attention. used during fine-tuning to reduce dense attention costs. However. the model during inference uses dense attention and achieves similar per- formance as full attention fine-tuning. Extrapolation without Training. LM-Infinite 186 and par- allel context windows PCW 190 show length extrapolation is possible using pre-trained LLMs. LM-Infinite suggested \u039b-",
    "chunk_index": 172,
    "start_pos": 75841,
    "end_pos": 76313,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 511
    }
  },
  "882": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_173",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "using pre-trained LLMs. LM-Infinite suggested \u039b- shaped attention applied within the original context window limits. Likewise. PCW chunks larger inputs into the pre-trained context lengths and applies the same positional encodings to each chunk. 3.4. Augmented LLMs LLMs are capable of learning from the examples concate- nated with the input. known as context augmentation. in- context learning ICL or few-shot prompting. They show ex- cellent generalization to unseen tasks with few-shot prompt-",
    "chunk_index": 173,
    "start_pos": 76314,
    "end_pos": 76765,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 497
    }
  },
  "883": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_174",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "neralization to unseen tasks with few-shot prompt- ing. enabling LLMs to answer queries beyond the capacity ac- quired during training 6. 55 These emergent abilities allow for adapting the model without fine-tuning costly process. Aside from this. hallucination. producing inaccurate. unsafe. or factually incorrect responses. is common for LLMs. which is avoided by augmenting contextual data. While the user can pro- vide in-context samples in the query 54. 32 here we specifi-",
    "chunk_index": 174,
    "start_pos": 76766,
    "end_pos": 77202,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 487,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 487,
      "optimized_content_length": 479
    }
  },
  "884": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_175",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "xt samples in the query 54. 32 here we specifi- cally refer to the methods that access external storage program- matically. calling them augmented LLMs. The literature suggests various external memory designs to aug- ment LLMs. long-term 191. 192. 193. 194 short-term 195 symbolic 196 and non-symbolic 197. 198 The memory can be maintained in di fferent formats such as documents. vec- tors. or databases. few systems maintain intermediate mem- ory representations to retain information across multiple iter-",
    "chunk_index": 175,
    "start_pos": 77203,
    "end_pos": 77677,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 508
    }
  },
  "885": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_176",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ations to retain information across multiple iter- ations 194. 192 while others extract important information from the datasets and save it in memory for recall 199 The memory read and write operations are performed either with or without LLMs cooperation 192. 200. 194. 201 acting as feedback signal in 195 We discuss di fferent types of aug- mented LLMs below. Figure 12. flow diagram of Retrieval Augmented LLMs. The retriever ex- tracts similar context to the input and forwards it to the LLM either in simple",
    "chunk_index": 176,
    "start_pos": 77678,
    "end_pos": 78158,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 513
    }
  },
  "886": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_177",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "input and forwards it to the LLM either in simple language or encoded through Fusion-in-Decoder FiD Depending on the task. retrieval and generation may repeat multiple times. 3.4.1. Retrieval Augmented LLMs LLMs may have limited memory and outdated information. leading to inaccurate responses. Retrieving relevant informa- tion from external up-to-date storage enables the LLMs to accurately answer with references and utilize more informa- tion. With retrieval augmentation. smaller models have been",
    "chunk_index": 177,
    "start_pos": 78159,
    "end_pos": 78613,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 501
    }
  },
  "887": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_178",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "retrieval augmentation. smaller models have been shown to perform at par with larger models. For instance. the 11B model can become competitive to 540B PaLM in 25 and 7.5B to 280B Gopher in 193 Retrieval augmented language modeling RALM has two major components. shown in Figure 12. namely. retriever and language model. In RALM. the retriever plays crucial role in driving LLM response. where incorrect information can steer LLMs to false behavior. This leads to the development of various methods to",
    "chunk_index": 178,
    "start_pos": 78614,
    "end_pos": 79081,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 501
    }
  },
  "888": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_179",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "his leads to the development of various methods to retrieve accurate information and fuse with the query for better performance. Zero-Shot Retrieval Augmentation. This kind of augmen- tation keeps the original LLM architecture and weights unchanged and uses BM25 202 nearest neighbors. or frozen pre-trained models like Bert as retriever. The retrieved information is provided as input to the model for response generation. shown to improve performance over LLMs without retrieval 198. 203 In some scenarios. multiple retrieval",
    "chunk_index": 179,
    "start_pos": 79082,
    "end_pos": 79570,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 527
    }
  },
  "889": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_180",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "198. 203 In some scenarios. multiple retrieval iterations are required to complete the task. The output generated in the first iteration is forwarded to the retriever to fetch similar documents. Forward-looking active retrieval FLARE 197 initially generates the response and corrects the output by retrieving relevant documents if the response contains low-confidence tokens. Similarly. RepoCoder 204 fetches code snippets recursively for code completion. Training with Retrieval Augmentation. To reduce failures in",
    "chunk_index": 180,
    "start_pos": 79571,
    "end_pos": 80045,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 515
    }
  },
  "890": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_181",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "with Retrieval Augmentation. To reduce failures in retrieval augmentation generation RAG researchers train or fine-tune retrievers and LLMs with retrieval augmentation pipeline. We discuss the literature below based on their focus on the respective training processes of the pipeline. Training LLM. Retrieval-enhanced transformer RETRO 193 shows pre-training smaller LLMs with RAG pipeline outper- forms larger LLMs. such as GPT-3 trained without RAG. RETRO uses 2-trillion token subset of MassiveText as 18",
    "chunk_index": 181,
    "start_pos": 80046,
    "end_pos": 80513,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 507
    }
  },
  "891": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_182",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ses 2-trillion token subset of MassiveText as 18 --- Page 19 --- database. The retrieval pipeline divides the input query into subsets and retrieves relevant chunks from the database for each subset. encoded together with input intermediate representations for generating tokens. It uses cross-chunked attention to attend to previous chunks auto-regressively. study on RETRO 205 shows models pre-trained without RAG but fine-tuned using RAG lack the performance gains obtained by pre-training with RAG.",
    "chunk_index": 182,
    "start_pos": 80515,
    "end_pos": 80974,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 502
    }
  },
  "892": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_183",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rformance gains obtained by pre-training with RAG. Training Retriever. Quality of responses generated by LLMs is highly dependent on the in-context examples. There- fore. 206. 207. 208. 209 train retrievers to retrieve accurate few-shot samples while keeping the LLM frozen for gener- ation. Retrieved samples are ranked to build ground-truth data to train retrievers with contrastive learning in 206. 208 RoBERTa is trained for downstream tasks in 207 for ICL samples retrieval. REPLUG 209 trains the retriever with",
    "chunk_index": 183,
    "start_pos": 80975,
    "end_pos": 81449,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 516
    }
  },
  "893": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_184",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "retrieval. REPLUG 209 trains the retriever with supervised signals from the frozen LLM-generated outputs. Training Retriever and LLM. Further benefits are achieved by training both the retriever and the model in 25. 210. 211 In this case. the error propagates back to the retriever. updating both the language model and the retriever. While masked language modeling MLM is common pre-training objec- tive 25. 211 retrieval pre-trained transformer RPT 210 used document chunk prediction as pre-training objective for",
    "chunk_index": 184,
    "start_pos": 81450,
    "end_pos": 81933,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 515
    }
  },
  "894": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_185",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "chunk prediction as pre-training objective for long text modeling. Encoded Context Augmentation. Concatenating retrieved documents with the query becomes infeasible as the sequence length and sample size grow. Encoding the context and fusing it with the decoder Fusion-in-Decoder using cross-attention makes it possible to augment more samples without increasing computation costs significantly 212. 193. 210. 25 Web Augmented. Locally stored memory. but external to LLM. has limited information. However. large amount of",
    "chunk_index": 185,
    "start_pos": 81934,
    "end_pos": 82415,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 521
    }
  },
  "895": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_186",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "as limited information. However. large amount of information is available on the internet. which gets updated regularly. Rather than storing information locally. various methods retrieve query-related context through web search and forward it to LLMs 213. 214. 166 3.4.2. Tool Augmented LLMs While RAG relies on the retriever to provide context to the LLM to answer queries. tool augmented LLMs capitalize on the reasoning abilities of LLMs to iteratively plan by dividing tasks into sub-tasks. selecting necessary tools. and taking actions to",
    "chunk_index": 186,
    "start_pos": 82416,
    "end_pos": 82915,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 543
    }
  },
  "896": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_187",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "selecting necessary tools. and taking actions to complete the task 215. 216. 217. 27 generic pipeline of tool-augmented LLMs is shown in Figure 13. where di fferent modules in Figure 13 are selected in loop until the task com- pletion. Zero-Shot Tool Augmentation. LLMs in-context learning and reasoning abilities enable them to interact with tools with- out training. Automatic reasoning and tool-use ART 217 builds task library with demonstrations of reasoning steps and calling external tools. It retrieves similar task examples and",
    "chunk_index": 187,
    "start_pos": 82916,
    "end_pos": 83415,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 535
    }
  },
  "897": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_188",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rnal tools. It retrieves similar task examples and provides the context to the LLM for inference. Aside from this. 218 shows tool documentation is enough to teach LLMs to use tools without demonstrations. RestGPT 219 integrates LLMs with RESTful APIs by decomposing tasks into planning Figure 13. basic flow diagram of tool augmented LLMs. Given an input and set of available tools. the model generates plan to complete the task. The tool augmented LLMs utilize di fferent modules iteratively. such as retriever.",
    "chunk_index": 188,
    "start_pos": 83416,
    "end_pos": 83887,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 512
    }
  },
  "898": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_189",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "di fferent modules iteratively. such as retriever. tool execution. read-write to memory. feedback. etc. depending on the task. and API selection steps. The API selector understands the API documentation to select suitable API for the task and plan the execution. ToolkenGPT 220 uses tools as tokens by concate- nating tool embeddings with other token embeddings. During inference. the LLM generates the tool tokens representing the tool call. stops text generation. and restarts using the tool exe- cution output.",
    "chunk_index": 189,
    "start_pos": 83888,
    "end_pos": 84355,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 513
    }
  },
  "899": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_190",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "n. and restarts using the tool exe- cution output. Training with Tool Augmentation. LLMs are trained to inter- act with diverse tools. enhancing planning abilities to overcome the limitations of zero-shot tool augmentation 221. 27. 222. 223 Gorilla 221 instruction-tunes LLaMA with information retrieval from API documentation. It uses the self-instruct 19 data generation pipeline with GPT-4 by providing in-context examples retrieved from API documentation. Tool augmented language model TALM 27 fine-tunes T5 10 for tool use",
    "chunk_index": 190,
    "start_pos": 84356,
    "end_pos": 84845,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 527
    }
  },
  "900": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_191",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "model TALM 27 fine-tunes T5 10 for tool use with self-play approach. where it iteratively completes tool manipulation tasks and includes them back in the training set. ToolLLM 223 collects 16k APIs from RapidAPI. It samples APIs from the list to generate an instruction-tuning dataset us- ing ChatGPT in single-tool and multi-tool scenarios. For high- quality datasets. ToolLLM suggested depth-first search-based decision tree DFSDT method to generate ground-truths with diverse reasoning and planning.",
    "chunk_index": 191,
    "start_pos": 84846,
    "end_pos": 85312,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 502
    }
  },
  "901": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_192",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ground-truths with diverse reasoning and planning. Multimodal Tool Augmentation. The compositional reasoning capacity of LLMs allows them to manipulate tools in multi- modal settings 215. 216. 224 Following the pipeline shown in Figure 13. the LLM outlines plan. generally executing in sequence. Plan Tool selection Execute Inspect Generate. to respond to the user query. Here. the database of tools is rich in modalities. including text. images. etc. Many of the multimodal tool augmentation systems employ multimodal",
    "chunk_index": 192,
    "start_pos": 85313,
    "end_pos": 85788,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 518
    }
  },
  "902": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_193",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "imodal tool augmentation systems employ multimodal LLMs 31. 225. 224. 216 while others utilize single modality 19",
    "chunk_index": 193,
    "start_pos": 85789,
    "end_pos": 85854,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 116,
      "word_count": 17,
      "optimized_for_embedding": true,
      "original_content_length": 116,
      "optimized_content_length": 113
    }
  },
  "903": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_194",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "224. 216 while others utilize single modality 19 --- Page 20 --- LLMs and generate plan on using di fferent modality tools to solve multimodal queries 226 3.5. LLMs-Powered Agents AI agents are autonomous entities. capable of planning. decision-making. and performing actions to achieve complex goals. In the early days. AI agents were rule-based. de- signed for narrow tasks. and had limited capabilities. such as Clippy 227 and Deep Blue 228 In contrast to this. LLMs abilities to respond to dynamic scenarios have made it",
    "chunk_index": 194,
    "start_pos": 85856,
    "end_pos": 86341,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 524
    }
  },
  "904": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_195",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ities to respond to dynamic scenarios have made it possible to incorporate them in diverse applications. includ- ing LLMs-powered agents 224. 216 where LLMs behave as the brain of agents. LLMs have been incorporated in web agents 166. 167 coding agents 229 tool agents 27. 223 embodied agents 26 and conversational agents 195 requir- ing minimal to no fine-tuning Below we summarize the re- search in LLMs-based autonomous agents. For more detailed discussion. please refer to 230. 231",
    "chunk_index": 195,
    "start_pos": 86342,
    "end_pos": 86801,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 485
    }
  },
  "905": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_196",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "detailed discussion. please refer to 230. 231 LLMs Steering Autonomous Agents. LLMs are the cognitive controllers of the autonomous agents. They generate plans. rea- son about tasks. incorporate memory to complete tasks. and adapt the outline depending on the feedback from the environ- ment. Depending on the acquired capabilities of LLMs. many methods fine-tune. propose better prompting approach. or uti- lize di fferent modules to enhance agents performance. Mod- ules and strategies employed in autonomous agents are briefly",
    "chunk_index": 196,
    "start_pos": 86802,
    "end_pos": 87288,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 529
    }
  },
  "906": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_197",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rategies employed in autonomous agents are briefly discussed below. Planning and Reasoning. Completing complex task requires human-like logical thinking. planning necessary steps. and reasoning current and future directions. Prompting methods like chain-of-thoughts 103 tree-of-thoughts 105 and self- consistency 104 are central to agents. eliciting LLMs to rea- son its actions and choose among di fferent paths for task com- pletion. When LLMs are prompted with task description and",
    "chunk_index": 197,
    "start_pos": 87289,
    "end_pos": 87734,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 484
    }
  },
  "907": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_198",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "When LLMs are prompted with task description and sequence of actions. they can accurately generate plan ac- tions without any fine-tuning 232 Reasoning via planning RAP 233 incorporates re-purposed LLM as world model to reason about future outcomes and explore alternative paths for task completion. Retroformer 234 uses retrospective LLM to improve main LLM planning and reasoning capabil- ities by providing helpful task cues. Feedback. LLMs in open-loop systems generate plans and as-",
    "chunk_index": 198,
    "start_pos": 87735,
    "end_pos": 88190,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 487
    }
  },
  "908": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_199",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "LLMs in open-loop systems generate plans and as- sume that the agent will complete them successfully. However. the actual scenario is di fferent with failures and variable re- sponses from the environment. To correctly complete tasks. many methods use LLMs in closed-loop where the action re- sponse is provided as feedback to the LLMs to re-assess and update the plan as required 235. 236. 237. 195 Another di- rection of research exploits LLMs as reward functions to train reinforcement learning RL policies instead of humans 238",
    "chunk_index": 199,
    "start_pos": 88191,
    "end_pos": 88683,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 531
    }
  },
  "909": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_200",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nt learning RL policies instead of humans 238 Memory. LLMs can learn from the context provided in the prompt. In addition to internal memory. various systems em- ploy external memory to save the response history. Reflex- ion 195 maintains an episodic memory to use previous re- sponses as feedback to improve future decision-making. Retro- former 234 improves its responses by employing short-termand long-term memory. where short-term memory contains re- cent responses and long-term memory keeps summarized failed",
    "chunk_index": 200,
    "start_pos": 88684,
    "end_pos": 89157,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 515
    }
  },
  "910": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_201",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "onses and long-term memory keeps summarized failed attempts to add in the prompt as reflection. Multi-Agents Systems. LLMs can play user-defined roles and behave like specific domain expert. In multi-agent systems. each LLM is assigned unique role. simulating human behav- ior and collaborating with other agents to complete complex task 229. 239 LLMs in Physical Environment. LLMs are good at instruction-following. however. utilizing them for physically grounded tasks requires adaptation. as they lack real-world",
    "chunk_index": 201,
    "start_pos": 89158,
    "end_pos": 89631,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 515
    }
  },
  "911": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_202",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tasks requires adaptation. as they lack real-world knowledge. This could lead to generating illogical responses for particular physical situation 240. 26 SayCan 240 make LLMs aware of the available low-level task operations. LLM Say builds high-level plan to complete the task and learned ffordance function Can explores the possibility of executing the plan in the real world. SayCan uses RL to train the language-conditioned ffordance function. PaLM-E enables the LLM to solve grounded tasks by training multi-modal LLM",
    "chunk_index": 202,
    "start_pos": 89632,
    "end_pos": 90121,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 521
    }
  },
  "912": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_203",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "solve grounded tasks by training multi-modal LLM feeding inputs directly from the sensors. Manipulation. In the area of manipulation 236. 241 LLMs enhance robot dexterity and adaptability. excelling in tasks like object recognition. grasping. and collaboration. They ana- lyze visual and spatial information to determine the most ffec- tive approach to interact with objects. Navigation. LLMs enhance robot ability to navigate com- plex environments with precision and adaptability 242. 243.",
    "chunk_index": 203,
    "start_pos": 90122,
    "end_pos": 90578,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 491
    }
  },
  "913": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_204",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "onments with precision and adaptability 242. 243. 244. 245 They generate feasible paths and trajectories for robots. accounting for intricate environmental details 246 This ability is valuable in scenarios requiring precise and dynamically adaptable navigation in environments like ware- houses. transport. healthcare facilities. and residences. 3.6. fficient LLMs Deploying LLMs in production is expensive. Reducing their running costs while preserving performance is an appealing area of research. This section summarizes the approaches sug-",
    "chunk_index": 204,
    "start_pos": 90579,
    "end_pos": 91079,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 543
    }
  },
  "914": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_205",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "earch. This section summarizes the approaches sug- gested to enhance LLMs fficiency. 3.6.1. Parameter fficient Fine-Tuning Fine-tuning LLMs with tens or hundreds of billions of pa- rameters. such as GPT-3 175B BLOOM 176B MT-NLG 540B etc. is computationally intensive and time-consuming. To avoid complete model fine-tuning. numerous parameter- efficient fine-tuning PEFT techniques 40. 247. 41. 38. 39 try to achieve acceptable model fine-tuning performance at reduced costs. As compared to full fine-tuning 248 PEFT performs",
    "chunk_index": 205,
    "start_pos": 91080,
    "end_pos": 91576,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 525
    }
  },
  "915": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_206",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "compared to full fine-tuning 248 PEFT performs better in low-resource setups. achieves comparable perfor- mance on medium-resource scenarios. and performs worse than full fine-tuning under high-resource availability. An overview of different PEFT approaches is shown in Figure 14. Adapter Tuning. Adds few trainable parameters within the transformer block. The adapter layer is sequence of feature downscaling. non-linearity. and upscaling 106 Variants of adapter tuning inject adapter layers sequentially 106 and in",
    "chunk_index": 206,
    "start_pos": 91577,
    "end_pos": 92055,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 516
    }
  },
  "916": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_207",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng inject adapter layers sequentially 106 and in parallel 38 whereas the mixture of adapter AdaMix 249 20",
    "chunk_index": 207,
    "start_pos": 92056,
    "end_pos": 92119,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 114,
      "word_count": 18,
      "optimized_for_embedding": true,
      "original_content_length": 114,
      "optimized_content_length": 105
    }
  },
  "917": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_208",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "whereas the mixture of adapter AdaMix 249 20 --- Page 21 --- Figure 14. Illustration of parameter-e fficient fine-tuning paradigms. where xis input and his hidden state. figure courtesy 38 Parallel adapter and LoRA fall in the adapter tuning category. employs multiple adapter modules in single layer. AdaMix routes input instances randomly to one of the multiple down- scale and upscale modules. The mixture of adapters is averaged out for inference to avoid additional latency. Low-Rank Adap-",
    "chunk_index": 208,
    "start_pos": 92121,
    "end_pos": 92575,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 494
    }
  },
  "918": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_209",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "erence to avoid additional latency. Low-Rank Adap- tation LoRA 250 learns low-rank decomposed matrices to freeze original weights. The learned weights are fused with the original weights for inference. avoiding latency. Prompt Tuning. Prompting is an ffective way to adapt pre-trained LLM for the downstream task. However. manual prompts bring uncertainty in the model prediction. where change in single word drops the performance 247 Prompt tuning alleviates this problem by fine-tuning only 0.001 -3",
    "chunk_index": 209,
    "start_pos": 92576,
    "end_pos": 93044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 501
    }
  },
  "919": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_210",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "eviates this problem by fine-tuning only 0.001 -3 additional parameters 251 It concatenates trainable prompt parameters with the model embeddings 247. 40. 251 Task- specific fixed discrete prompts are concatenated with input em- beddings in 40 As discrete prompts bring instability. prompts are encoded through learnable mapping in P-Tuning 247 naming continuous prompts. which are appended with the dis- crete prompts. Only the prompt encoder is trainable in the model. In an extension of P-Tuning. continuous prompts are",
    "chunk_index": 210,
    "start_pos": 93045,
    "end_pos": 93531,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 522
    }
  },
  "920": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_211",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "an extension of P-Tuning. continuous prompts are concatenated with each layer of the network in 251 Progres- sive prompts 252 avoid catastrophic forgetting and transfer previously learned knowledge by sequentially adding trainable prompt embeddings to the previously frozen task embeddings. Prefix Tuning. set of trainable task-specific prefix vectors are appended to the frozen transformer layers in prefix tun- ing 41 The prefix vectors are virtual tokens attended by the context tokens on the right. In addition. adaptive prefix tun-",
    "chunk_index": 211,
    "start_pos": 93532,
    "end_pos": 94029,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 536
    }
  },
  "921": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_212",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ns on the right. In addition. adaptive prefix tun- ing 253 applies gating mechanism to control the information from the prefix and actual tokens. Bias Tuning. Fine-tuning only bias terms in small to medium training data has been found ffective in BitFit 254 This method achieves full fine-tuning performance for tasks with less training data and comparable performance with more training data. 3.6.2. Quantization LLMs require extensive computing and memory for infer- ence. Deploying 175B parameter GPT-3 model needs at",
    "chunk_index": 212,
    "start_pos": 94030,
    "end_pos": 94510,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 520
    }
  },
  "922": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_213",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "e. Deploying 175B parameter GPT-3 model needs at least five 80GB A100 GPUs and 350GB of memory to store inFP16 format 44 Such demanding requirements for deploying LLMs make it harder for smaller organizations to utilize them. Model compression is an ffective solution but comes at the cost of degraded performance. especially at large scales greater than 6B. These models exhibit very large magnitude outliers that do not exist in smaller models 255 making it challenging and re- quiring specialized methods for quantizing LLMs 44. 256",
    "chunk_index": 213,
    "start_pos": 94511,
    "end_pos": 95008,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 535
    }
  },
  "923": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_214",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "specialized methods for quantizing LLMs 44. 256 Post-Training Quantization. Minimal or no training is re- quired in this type of quantization. without significantly com- promising the model performance. LLM-8-bit 255 uses full- precision matrix multiplication for weights associated with out- lier features and 8-bit for remaining features. The lower pre- cision multiplication outputs are converted to FP-16 and con- catenated with others. The quantized models have homogenous word embeddings. which may degrade their performance. To",
    "chunk_index": 214,
    "start_pos": 95009,
    "end_pos": 95497,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 534
    }
  },
  "924": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_215",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "mbeddings. which may degrade their performance. To fix this. token-level knowledge distillation is employed in 45 along with independent quantization scaling factors for each module due to varying weight distribution. Feature distribu- tions are asymmetric and appear in di fferent channels. outlier suppression 257 shifts and scales per-channel activation dis- tributions for ffective quantization. SmoothQuant 44 quan- tizes activations and weights to INT8 format by smoothing activations and migrating the quantization di fficulty toward",
    "chunk_index": 215,
    "start_pos": 95498,
    "end_pos": 95995,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 540
    }
  },
  "925": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_216",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "and migrating the quantization di fficulty toward weights. It multiplies the inverse of the smoothing factor with weights. which introduces few outliers in the weights but is easier to quantify than unsmoothed activations. OPTQ 256 uses the optimal brain compression OBC 258 algorithm to quantize the model layer-by-layer and update weights to com- pensate for quantization error. To improve speed and per- formance. OPTQ updates weights in arbitrary order. employs lazy updates. and uses better Cholesky kernels. Outlier-aware",
    "chunk_index": 216,
    "start_pos": 95996,
    "end_pos": 96481,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 527
    }
  },
  "926": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_217",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "s. and uses better Cholesky kernels. Outlier-aware weight quantization OWQ 259 uses the OPTQ algorithm for quantization but assigns higher precision to vulnerable weights. causing outliers and lower precision for others. Quantization-Aware Training. To compensate for perfor- mance degradation. quantized model is fine-tuned in quantization-aware training QAT 260. 261. 262 Al- pha Tuning quantizes the model using binary coding quan- tization BCQ 263 and fine-tunes only quantization scal-",
    "chunk_index": 217,
    "start_pos": 96482,
    "end_pos": 96936,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 490
    }
  },
  "927": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_218",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "BCQ 263 and fine-tunes only quantization scal- ing factors. This approach improves performance over 21",
    "chunk_index": 218,
    "start_pos": 96937,
    "end_pos": 96992,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 106,
      "word_count": 15,
      "optimized_for_embedding": true,
      "original_content_length": 106,
      "optimized_content_length": 102
    }
  },
  "928": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_219",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "actors. This approach improves performance over 21 --- Page 22 --- parameter-e fficient fine-tuning of the pre-trained model. Sim- ilarly. parameter-e fficient and quantization-aware adaptation PEQA 264 reduces the precision of fully-connected layers and fine-tunes only quantization scaling parameters. LLM- QAT 262 generates training data from the pre-trained network and trains quantized student model with knowledge distilla- tion. QLoRA 261 fine-tunes 4-bit quantized pre-trained LLM",
    "chunk_index": 219,
    "start_pos": 96994,
    "end_pos": 97441,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 488
    }
  },
  "929": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_220",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "261 fine-tunes 4-bit quantized pre-trained LLM with LoRA 250 using 4-bit normal float. which shows better performance over 4-bit integer and float. 3.6.3. Pruning Pruning is an alternative approach to quantization to com- press model size. thereby reducing LLMs deployment costs significantly. Compared to task-agnostic pruning. task-specific pruning is easily achievable with good performance. where model is fine-tuned on the downstream task and pruned for faster inference. It is possible to prune LLMs for individual",
    "chunk_index": 220,
    "start_pos": 97442,
    "end_pos": 97923,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 520
    }
  },
  "930": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_221",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rence. It is possible to prune LLMs for individual tasks. but the cost of pruning and deploying task-specific mod- els is high. To overcome this. many structured and unstructured pruning methods for LLMs have been proposed to maintain rea- sonable performance across all tasks while shrinking the model size 265. 42. 266 Unstructured Pruning. This kind of pruning removes less im- portant weights without maintaining any structure. Existing LLM pruning methods take advantage of the unique charac-",
    "chunk_index": 221,
    "start_pos": 97924,
    "end_pos": 98373,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 497
    }
  },
  "931": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_222",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "uning methods take advantage of the unique charac- teristics of LLMs. uncommon for smaller models. where small subset of hidden states are activated with large magni- tude 255 Pruning by weights and activations Wanda 265 prunes weights in every row based on importance. calculated by multiplying the weights with the norm of input. The pruned model does not require fine-tuning. thereby saving computa- tional costs. Outlier weighed layerwise sparsity OWL 267 extends Wanda with non-uniform layer pruning. It shows that",
    "chunk_index": 222,
    "start_pos": 98374,
    "end_pos": 98855,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 519
    }
  },
  "932": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_223",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "anda with non-uniform layer pruning. It shows that the number of outliers varies for di fferent layers. therefore. the model should have variable pruning ratios for better perfor- mance for every layer. Contrastive pruning CAP 43 itera- tively prunes the model by training the sparse model using con- trastive loss between pre-trained. fine-tuned. and snapshots of previous sparse models to learn task-specific and task-agnostic knowledge. Structured Pruning. Here. the parameters are removed in",
    "chunk_index": 223,
    "start_pos": 98856,
    "end_pos": 99304,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 495
    }
  },
  "933": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_224",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tured Pruning. Here. the parameters are removed in groups. rows. columns. or matrices. which speeds up the inference because of ffective hardware tensor core utiliza- tion 265 LLM-Pruner 42 employs 3-stage structured pruning strategy. identifying the groups of hidden states caus- ing each other to activate during the forward-pass. keeping im- portant groups and removing less important ones. and fine- tuning the pruned model with LoRA. Sparsity-induced mask learning SIMPLE 268 prunes the network using learnable",
    "chunk_index": 224,
    "start_pos": 99305,
    "end_pos": 99782,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 515
    }
  },
  "934": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_225",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "SIMPLE 268 prunes the network using learnable masks. Similarly. another method prunes LLMs by learning masks and removing unimportant rank-1 components of the factorized weight matrix 266 3.7. Multimodal LLMs Inspired by the success of LLMs in natural language process- ing applications. an increasing number of research works arenow facilitating LLMs to perceive di fferent modalities of infor- mation like image 269. 270. 271 video 272. 273. 274 au- dio 275. 274. 276 etc. Multimodal LLMs MLLMs present",
    "chunk_index": 225,
    "start_pos": 99783,
    "end_pos": 100255,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 504
    }
  },
  "935": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_226",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "5. 274. 276 etc. Multimodal LLMs MLLMs present substantial benefits compared to standard LLMs that process only text. By incorporating information from various modal- ities. MLLMs can achieve deeper understanding of context. leading to more intelligent responses infused with variety of expressions. Importantly. MLLMs align closely with human perceptual experiences. leveraging the synergistic nature of our multisensory inputs to form comprehensive understanding of the world 276. 26 Coupled with user-friendly interface.",
    "chunk_index": 226,
    "start_pos": 100256,
    "end_pos": 100743,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 523
    }
  },
  "936": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_227",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "276. 26 Coupled with user-friendly interface. MLLMs can ffer intuitive. flexible. and adaptable interactions. allowing users to engage with intelligent assistants through spectrum of input methods. According to the ways of construct- ing models. current MLLMs can be generally divided into three streams. pre-training. fine-tuning. and prompting. In this sec- tion. we will discuss more details of these main streams. as well as the important application of MLLMs in visual reasoning.",
    "chunk_index": 227,
    "start_pos": 100744,
    "end_pos": 101186,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 484
    }
  },
  "937": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_228",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "mportant application of MLLMs in visual reasoning. Pre-training. This stream of MLLMs intends to support di ffer- ent modalities using unified end-to-end models. For instance. Flamingo 269 applies gated cross-attention to fuse vision and language modalities. which are collected from pre-trained and frozen visual encoder and LLM. respectively. Moreover. BLIP- 270 proposes two-stage strategy to pre-train Querying Transformer Q-Former for the alignment between vision and language modalities. in the first stage. vision-language repre-",
    "chunk_index": 228,
    "start_pos": 101187,
    "end_pos": 101684,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 536
    }
  },
  "938": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_229",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "lities. in the first stage. vision-language repre- sentation learning is bootstrapped from frozen visual encoder. and in the second stage. frozen LLM bootstraps vision-to- language generative learning for zero-shot image-to-text gen- eration. Similarly. MiniGPT-4 277 deploys pre-trained and frozen ViT 278 Q-Former and Vicuna LLM 159 only train- ing the linear projection layer for vision and language modali- ties alignment. Fine-tuning. Derived from instruction tuning 16 for NLP",
    "chunk_index": 229,
    "start_pos": 101685,
    "end_pos": 102130,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 482
    }
  },
  "939": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_230",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ning. Derived from instruction tuning 16 for NLP tasks 20. 16. 97 researchers are fine-tune pre-trained LLMs using multimodal instructions. Following this method. LLMs can be easily and ffectively extended as multimodal chat- bots 277. 271. 29 and multimodal task solvers 279. 30. 280 The key issue of this stream of MLLMs is to collect multi- modal instruction-following data for fine-tuning 58 To ad- dress this issue. the solutions of benchmark adaptation 279. 281. 282 self-instruction 19. 31. 283 and hybrid composi-",
    "chunk_index": 230,
    "start_pos": 102131,
    "end_pos": 102622,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 521
    }
  },
  "940": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_231",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "elf-instruction 19. 31. 283 and hybrid composi- tion 284. 280 are employed. respectively. To mitigate the gap between the original language modality and additional modal- ities. the learnable interface is introduced to connect di ffer- ent modalities from frozen pre-trained models. Particularly. the learnable interface is expected to work in parameter- efficient tuning manner. e.g. LLaMA-Adapter 285 applies an efficient transformer-based adapter module for training. and LaVIN 284 dynamically learns the multimodal feature",
    "chunk_index": 231,
    "start_pos": 102623,
    "end_pos": 103110,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 526
    }
  },
  "941": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_232",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "IN 284 dynamically learns the multimodal feature weights using mixture-of-modality adapter. Di fferent from the learnable interface. the expert models can directly convert multimodalities into language. e.g. VideoChat-Text 272 in- corporates Whisper 286 speech recognition expert model. to generate the captions of given videos for the understanding of following LLMs. Prompting. Different from the fine-tuning technique that 22",
    "chunk_index": 232,
    "start_pos": 103111,
    "end_pos": 103500,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 440,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 440,
      "optimized_content_length": 428
    }
  },
  "942": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_233",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Different from the fine-tuning technique that 22 --- Page 23 --- directly updates the model parameters given task-specific datasets. the prompting technique provides certain context. ex- amples. or instructions to the model. fulfilling specialized tasks without changing the model parameters. Since prompting can significantly reduce the need for large-scale multimodal data. this technique is widely used to construct MLLMs. Particularly. to solve multimodal Chain of Thought CoT problems 103",
    "chunk_index": 233,
    "start_pos": 103502,
    "end_pos": 103951,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 493
    }
  },
  "943": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_234",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "multimodal Chain of Thought CoT problems 103 LLMs are prompted to generate both the reasoning process and the answer given multimodal inputs 287 On this front. di ffer- ent learning paradigms are exploited in practice. for example. Multimodal-CoT 287 involves two stages of rationale genera- tion and answer inference. where the input of the second stage is combination of the original input and the output of the first stage. and CoT-PT 288 applies both prompt tuning and spe-",
    "chunk_index": 234,
    "start_pos": 103952,
    "end_pos": 104393,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 477
    }
  },
  "944": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_235",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "CoT-PT 288 applies both prompt tuning and spe- cific visual bias to generate chain of reasoning implicitly. In addition to CoT problems. LLMs can also be prompted with multimodal descriptions and tools. ffectively dividing complex tasks into sub-tasks 289. 290 Visual Reasoning Application. Recent visual reasoning sys- tems 291. 292. 216. 293 tend to apply LLMs for better visual information analysis and visual-language integration. Di ffer- ent from previous works 294. 295 that rely on limited VQA",
    "chunk_index": 235,
    "start_pos": 104394,
    "end_pos": 104859,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 501
    }
  },
  "945": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_236",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "previous works 294. 295 that rely on limited VQA datasets and small-scale neural networks. current LLM-aided methods ffer benefits of stronger generalization ability. emer- gent ability. and interactivity 58 To realize visual reasoning with the help of LLMs. prompting and fine-tuning techniques can also be utilized. for example. PointClip V2 292 applies LLMs to generate 3D-specific prompts. which are encoded as textual features and then combined with visual features for 3D recognition. and GPT4Tools 31 employs LoRA 250 to",
    "chunk_index": 236,
    "start_pos": 104860,
    "end_pos": 105349,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 527
    }
  },
  "946": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_237",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ognition. and GPT4Tools 31 employs LoRA 250 to fine-tune LLMs following tool-related instructions. Serving as controller 293 decision maker 296 or semantics re- finer 291. 297 LLMs significantly facilitates the progress of visual reasoning research. 3.8. Summary and Discussion 3.8.1. Architecture Due to the gigantic scale of LLMs. minor changes in archi- tecture and training strategies have big impact on performance and stability. Here. we summarize key architectural modules",
    "chunk_index": 237,
    "start_pos": 105350,
    "end_pos": 105795,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 479
    }
  },
  "947": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_238",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "lity. Here. we summarize key architectural modules used in various LLMs. leading to better performance. reduced training time and memory. and better training stability. Layer Normalization. The performance and training stability of LLMs are ffected significantly by layer normalization. Pre- norm. that is normalizing inputs rather than outputs. is more common among LLMs stabilizing the training 6. 127. 108 BLOOM 13 and AlexaTM 122 utilize an additional layer normalization before embedding layer to stabilize the training",
    "chunk_index": 238,
    "start_pos": 105796,
    "end_pos": 106278,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 524
    }
  },
  "948": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_239",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "before embedding layer to stabilize the training of large-scale models. while the model zero-shot generaliza- tion ability can be negatively impacted 13 However. another study 33 finds that pre-norm degrades fine-tuned model per- formance as compared to post-norm. and there are no stability benefits of pre-norm beyond the 100B scale. Therefore. GLM- 130B 33 used deep-norm which is variant of post-norm for better downstream task performance after fine-tuning.",
    "chunk_index": 239,
    "start_pos": 106279,
    "end_pos": 106703,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 475,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 475,
      "optimized_content_length": 462
    }
  },
  "949": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_240",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ter downstream task performance after fine-tuning. Positional Encoding. Like other building blocks of the model.positional encoding also ffects the performance and training stability of LLMs. BLOOM 13 finds ALiBi outperforms learned and rotary positional encodings. Contrary to this. GLM-130B 33 identifies rotary positional encoding as being better than ALiBi. So. there is no conclusion in the literature about positional encodings yet. Parallel Attention. In this type of attention. feed-forward and",
    "chunk_index": 240,
    "start_pos": 106704,
    "end_pos": 107161,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 502
    }
  },
  "950": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_241",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ntion. In this type of attention. feed-forward and attention layers are parallel to each other rather than sequen- tial in transformer block. It has been shown to reduce train- ing time by 15 There is no evidence of performance drop due to this change in the literature and it is used by the models PaLM 15 GPT-NeoX 118 and CodeGen 140 Multi-Query Attention It has shared key and value attention heads in transformer block while query attention heads are projected as usual. This reduces memory usage and speeds up",
    "chunk_index": 241,
    "start_pos": 107162,
    "end_pos": 107640,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 514
    }
  },
  "951": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_242",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "as usual. This reduces memory usage and speeds up sampling in autoregressive decoding. No performance degrada- tion has been observed with this change and it makes the train- ing efficient allowing larger batch sizes. Multi-query attention is used in 15. 142 Mixture of Experts. This type of architecture enables eas- ily scaling models to trillions of parameters 92. 91 Only few experts are activated during the computation making them compute-e fficient. The performance of MoE models is better",
    "chunk_index": 242,
    "start_pos": 107641,
    "end_pos": 108095,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 496
    }
  },
  "952": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_243",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "fficient. The performance of MoE models is better than dense models for the same amount of data and requires less computation during fine-tuning to achieve performance similar to dense models as discussed in 91 MoE architectures are less prone to catastrophic forgetting. therefore are more suited for continual learning 92 Extracting smaller sub-models for downstream tasks is possible without losing any performance. making MoE architecture hardware-friendly 92 Sparse vs Dense Activated. GPT-3 uses sparse transform-",
    "chunk_index": 243,
    "start_pos": 108096,
    "end_pos": 108578,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 519
    }
  },
  "953": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_244",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Dense Activated. GPT-3 uses sparse transform- ers 67 whereas GLaM 91 and PanGu-P 92 use MoE 121 architectures to lower computational costs and increase the model size and capacity. According to the literature. sparse modules do not degrade the model performance 67 How- ever. more experiments are required to verify this statement. 3.8.2. Training Strategies Training models at huge scale require tricks to reduce train- ing costs. avoid loss divergence. and achieve better perfor-",
    "chunk_index": 244,
    "start_pos": 108579,
    "end_pos": 109028,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 481
    }
  },
  "954": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_245",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "avoid loss divergence. and achieve better perfor- mance. We summarize and discuss some of these key tricks used in di fferent LLMs. Mixed Precision. It is famous method for LLMs to reduce memory usage and improve training fficiency. In mixed pre- cision. forward and backward passes are performed in FP16 format whereas optimizer states and master weights are kept in FP32 format 120 drawback associated with this for- mat change is training instability due to smaller value range",
    "chunk_index": 245,
    "start_pos": 109029,
    "end_pos": 109470,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 480
    }
  },
  "955": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_246",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "training instability due to smaller value range resulting in loss spikes 33 An alternative to FP16 is BF16 which has comparatively larger range and performs precision- sensitive operations like gradient accumulation and softmax in FP32 13 BF16 has better performance and training stability but uses more memory and is supported on specific hardware. for example. A100 GPUs. Therefore. its adoption in LLMs is limited. Training Instability. Loss divergence or spiking is common",
    "chunk_index": 246,
    "start_pos": 109471,
    "end_pos": 109909,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 489,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 488,
      "optimized_content_length": 476
    }
  },
  "956": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_247",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nstability. Loss divergence or spiking is common issue in LLMs that occurs multiple times during training. This 23",
    "chunk_index": 247,
    "start_pos": 109910,
    "end_pos": 109975,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 116,
      "word_count": 19,
      "optimized_for_embedding": true,
      "original_content_length": 116,
      "optimized_content_length": 114
    }
  },
  "957": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_248",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "hat occurs multiple times during training. This 23 --- Page 24 --- happens in the presence of gradient clipping 15 To mitigate this problem. many approaches suggest restarting training from an earlier checkpoint 15. 33. 91 skipping 200-500 earlier data batches at the point of divergence in 15 and re-shu ffling batches in 91 The embedding layer gradient shrink proves to further stabilize the training as its gradient norm is significantly larger than the other layers 33 Another suggestion to improve",
    "chunk_index": 248,
    "start_pos": 109977,
    "end_pos": 110442,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 502
    }
  },
  "958": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_249",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "other layers 33 Another suggestion to improve training stability for larger models is not to use biases in dense and norm layers as in 15 Weight Initialization. It plays significant role in model con- vergence and training stability. GPT-NeoX 118 initializes feed-forward layers before residuals with2 das in 153 and other layers with the small initialization scheme 298 This avoids activations growing exponentially with increasing depth. MT-NLG 117 found higher variance for weight initialization",
    "chunk_index": 249,
    "start_pos": 110443,
    "end_pos": 110912,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 498
    }
  },
  "959": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_250",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "found higher variance for weight initialization leads to unstable training. hence validating small initialization scheme 298 Various models perform random weight initial- ization which can cause bad initialization. Galactica 148 sug- gests longer warmup to negate the ffect. Learning Rate. suitable learning rate is important for sta- ble training. It is suggested to use lower value 13. 15. 124 with warmup and decay cosine or linear Usually. the learn- ing rate is within the range 4to 8e 4. Moreover. MT-NLG",
    "chunk_index": 250,
    "start_pos": 110913,
    "end_pos": 111397,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 510
    }
  },
  "960": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_251",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "is within the range 4to 8e 4. Moreover. MT-NLG 530B 117 and GPT-NeoX 20B 118 suggest interpolat- ing learning rates based on the model size using the GPT-3 models ranging between 13B and 175B. This avoids tuning the learning rate hyperparameter. Training Parallelism. 3D parallelism. combination of data. pipeline. and tensor parallelism. is the most utilized training parallelism approach in LLMs 33. 15. 14. 13. 117. 115. 112 In addition to 3D parallelism. BLOOM 13 uses zero op-",
    "chunk_index": 251,
    "start_pos": 111398,
    "end_pos": 111853,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 481
    }
  },
  "961": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_252",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tion to 3D parallelism. BLOOM 13 uses zero op- timizer 37 to shard optimizer states. PanGu- 108 and PanGu- 92 go beyond 3D parallelism and apply 5D paral- lelism which additionally contains optimizer parallelism and rematerialization. Mode Switching. It adds task-related tokens at the beginning of the text during training. These tokens refer to the natural language understanding and natural language generation tasks which are shown to improve downstream task performance in 125. 124. 122 During fine-tuning and inference. tokens",
    "chunk_index": 252,
    "start_pos": 111854,
    "end_pos": 112350,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 532
    }
  },
  "962": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_253",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "24. 122 During fine-tuning and inference. tokens are appended based on the downstream tasks. Controllable Text Generation. Generating credible and con- trolled text from pre-trained model is challenging. GPT-3 and other LLMs use in-context learning to control generated text. While in-context learning helps in controlling the gener- ated text. ERNIE 3.0 Titan 35 suggests using adversarial loss to rank its generated text for credibility and soft prompts such as genre. topic. keywords. sentiment. and length for better control",
    "chunk_index": 253,
    "start_pos": 112351,
    "end_pos": 112838,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 528
    }
  },
  "963": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_254",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "keywords. sentiment. and length for better control on generated text. 3.8.3. Supervised Models vs Generalized Models Although generalized models are capable of performing di- verse tasks with good performance they have not yet outper- formed models trained in supervised settings. The supervised trained models are still state-of-the-art in various NLP tasks by large margin as shown in 6. 15. 18 .3.8.4. Zero-Shot vs Few-Shot LLMs perform well in zero-shot and few-shot settings. But the performance di fference between zero-shot and few-shot is",
    "chunk_index": 254,
    "start_pos": 112839,
    "end_pos": 113337,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 546
    }
  },
  "964": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_255",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ance di fference between zero-shot and few-shot is large for pre-trained models 6. 15 naming LLMs as meta- learners LLMs zero-shot evaluations underperform unsu- pervised methods in neural machine translation The liter- ature shows pre-training is not enough for good zero-shot per- formance 15. 16 To improve the zero-shot performance the literature suggests using instruction fine-tuning that improves the zero-shot performance significantly and outperforms base- lines. Instruction fine-tuning has also been shown to improve",
    "chunk_index": 255,
    "start_pos": 113338,
    "end_pos": 113830,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 527
    }
  },
  "965": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_256",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ruction fine-tuning has also been shown to improve zero-shot generalization to unseen tasks. Another model. Flan- PaLM 16 unlocks zero-shot reasoning with CoT training. 3.8.5. Encoder vs Decoder vs Encoder-Decoder Traditionally. these architectures perform well for di fferent tasks. for example. encoder-only for NLU tasks. decoder-only for NLG. and encoder-decoder for sequence2sequence model- ing. Encoder-only models are famous for smaller models such as Bert RoBERTa 299 etc. whereas LLMs are either",
    "chunk_index": 256,
    "start_pos": 113831,
    "end_pos": 114296,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 504
    }
  },
  "966": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_257",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "RoBERTa 299 etc. whereas LLMs are either decoder-only 6. 118. 13 or encoder-decoder 10. 11. 122 While decoder-only models are good at NLG tasks. various LLMs. PaLM 15 OPT 14 GPT-3 BLOOM 13 LLaMA 156 are decoder-only models with significant per- formance gains on both NLU and NLG tasks. In contradic- tion to this. T5 10 and UL2 125 identify encoder-decoder models out-performing decoder-only models. In another study. PaLM 15 finds increasing the size of decoder-only models",
    "chunk_index": 257,
    "start_pos": 114297,
    "end_pos": 114759,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 475
    }
  },
  "967": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_258",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "finds increasing the size of decoder-only models can reduce the performance gap between decoder-only and encoder-decoder architectures. Although decoder-only architectures have become trend for LLMs. many recently proposed approaches 125. 122 use mode-switching tokens in text with encoder-decoder architec- tures to enable task-specific modes. Similarly. CodeT5 34 uses an encoder-decoder architecture with multiple training ob- jectives for di fferent tasks. activating the encoder. decoder. or",
    "chunk_index": 258,
    "start_pos": 114760,
    "end_pos": 115214,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 496
    }
  },
  "968": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_259",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "fferent tasks. activating the encoder. decoder. or both according to the tasks. These variations in architecture and training objectives allow model to perform well in di ffer- ent settings. Because of this dynamic configuration. the future of LLMs can be attributed to encoder-decoder architectures. 4. Model Configurations We provide di fferent statistics of pre-trained and instruction- tuned models in this section. This includes information such as publication venue. license type. model creators. steps trained.",
    "chunk_index": 259,
    "start_pos": 115215,
    "end_pos": 115683,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 517
    }
  },
  "969": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_260",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "enue. license type. model creators. steps trained. parallelism. etc in Table and Table 4. Architecture details of pre-trained LLMs are available in Table 5. Providing these details for instruction-tuned models is unnecessary because it fine-tunes pre-trained models for instruction datasets. Hence. architectural details are the same as the baselines. Moreover. optimization settings for various LLMs are available in Table and Table 7. We do not include details on precision. warmup.",
    "chunk_index": 260,
    "start_pos": 115684,
    "end_pos": 116121,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 488,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 488,
      "optimized_content_length": 484
    }
  },
  "970": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_261",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "7. We do not include details on precision. warmup. and weight decay in Table 7. These details are not as important as others to mention for instruction-tuned models. and are not provided by the papers. 24",
    "chunk_index": 261,
    "start_pos": 116122,
    "end_pos": 116275,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 204,
      "word_count": 36,
      "optimized_for_embedding": true,
      "original_content_length": 204,
      "optimized_content_length": 204
    }
  },
  "971": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_262",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ned models. and are not provided by the papers. 24 --- Page 25 --- Table 3. Summary of pre-trained LLMs 10B Only the LLMs discussed individually in the previous sections are summarized. Data Tokens is the model pre-training data. which is either the number of tokens or data size. Data Cleaning indicates whether data cleaning is performed or not. This includes heuristics Heur deduplication Dedup quality filtering QF and privacy filtering PF Cost is the calculated training cost obtained by multiplying the GPUs TPUs",
    "chunk_index": 262,
    "start_pos": 116277,
    "end_pos": 116771,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 518
    }
  },
  "972": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_263",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "aining cost obtained by multiplying the GPUs TPUs hourly rate with the number of GPUs and the training time. The actual cost may vary due to many reasons such as using in-house GPUs or getting discounted rate. re-training. number of employees working on the problem. etc. Training Parallelism indicates distributed training using data parallelism tensor parallelism pipeline parallelism context parallelism model parallelism optimizer parallelism OP and rematerialization where for Library column.",
    "chunk_index": 263,
    "start_pos": 116772,
    "end_pos": 117258,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 497
    }
  },
  "973": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_264",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rematerialization where for Library column. DS is short form for Deep Speed. In column Commercial Use we assumed model is for non-commercial purposes if its license is unavailable. ModelsPublication VenueLicense TypeModel Creators PurposeNo. of ParamsCommercial UseSteps TrainedData TokensData CleaningNo. of Processing UnitsProcessing Unit TypeTraining TimeCalculated Train. CostTraining Parallelism Library T5 10 JMLR 20 Apache-2.0 Google General 11B 1M 1T Heur Dedup 1024 TPU v3 Mesh TensorFlow",
    "chunk_index": 264,
    "start_pos": 117259,
    "end_pos": 117735,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 497
    }
  },
  "974": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_265",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "1T Heur Dedup 1024 TPU v3 Mesh TensorFlow GPT-3 NeurIPS 20 OpenAI General 175B 300B Dedup QF V100 mT5 11 NAACL 21 Apache-2.0 Google General 13B 1M 1T PanGu-\u03b1 108 arXiv 21 Apache-2.0 Huawei General 200B 260k 1.1TB Heur Dedup 2048 Ascend 910 OP MindSpore CPM-2 12 AI Open 21 MIT Tsinghua General 198B 1M 2.6TB Dedup JAXFormer Codex 141 arXiv 21 OpenAI Coding 12B 100B Heur",
    "chunk_index": 265,
    "start_pos": 117736,
    "end_pos": 118152,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 467,
      "word_count": 95,
      "optimized_for_embedding": true,
      "original_content_length": 466,
      "optimized_content_length": 370
    }
  },
  "975": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_266",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "iv 21 OpenAI Coding 12B 100B Heur ERNIE 3.0 110 arXiv 21 Baidu General 10B 120k 375B Heur Dedup 384 V100 PaddlePaddle Jurassic-1 112 White-Paper 21 Apache-2.0 AI21 General 178B 300B 800 GPU Megatron DS HyperCLOV 114 EMNLP 21 Naver General 82B 300B Clf Dedup PF 1024 A100 321h 1.32 Mil Megatron Yuan 1.0 115 arXiv 21 Apache-2.0 General 245B 26k 180B Heur Clf Dedup 2128 GPU Gopher 116 arXiv 21 Google General 280B 300B QF Dedup 4096 TPU v3 920h 13.19 Mil JAX Haiku",
    "chunk_index": 266,
    "start_pos": 118153,
    "end_pos": 118653,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 101,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 463
    }
  },
  "976": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_267",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "QF Dedup 4096 TPU v3 920h 13.19 Mil JAX Haiku ERNIE 3.0 Titan 35 arXiv 21 Baidu General 260B 300B Heur Dedup Ascend 910 PaddlePaddle GPT-NeoX-20B 118 BigScience 22 Apache-2.0 EleutherAI General 20B 150k 825GB None 96 40G A100 Megatron DS PyTorch OPT 14 arXiv 22 MIT Meta General 175B 150k 180B Dedup 992 80G A100 Megatron BLOOM 13 arXiv 22 RAIL-1.0 BigScience General 176B 366B Dedup PR 384 80G A100 2520h 3.87 Mil Megatron DS",
    "chunk_index": 267,
    "start_pos": 118654,
    "end_pos": 119093,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 489,
      "optimized_content_length": 426
    }
  },
  "977": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_268",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "PR 384 80G A100 2520h 3.87 Mil Megatron DS Galactica 148 arXiv 22 Apache-2.0 Meta Science 120B 225k 106B Dedup 128 80GB A100 Metaseq GLaM 91 ICML 22 Google General 1.2T 600k 600B Clf 1024 TPU v4 GSPMD LaMDA 150 arXiv 22 Google Dialog 137B 3M 2.81T Filtered 1024 TPU v3 1384h 4.96 Mil Lingvo MT-NLG 117 arXiv 22 Apache-v2.0 MS. Nvidia General 530B 270B 4480 80G A100 Megatron DS AlphaCode 142 Science 22 Apache-v2.0 Google Coding 41B 205k 967B Heur Dedup TPU v4 JAX Haiku",
    "chunk_index": 268,
    "start_pos": 119094,
    "end_pos": 119580,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 97,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 470
    }
  },
  "978": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_269",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "1B 205k 967B Heur Dedup TPU v4 JAX Haiku Chinchilla 96 arXiv 22 Google General 70B 1.4T QF Dedup TPUv4 JAX Haiku PaLM 15 arXiv 22 Google General 540B 255k 780B Heur 6144 TPU v4 JAX T5X AlexaTM 122 arXiv 22 Apache v2.0 Amazon General 20B 500k 1.1T Filtered 128 A100 2880h 1.47 Mil DS U-PaLM 124 arXiv 22 Google General 540B 20k 512 TPU v4 120h 0.25 Mil UL2 125 ICLR 23 Apache-2.0 Google General 20B 2M 1T 512 TPU v4 JAX T5X",
    "chunk_index": 269,
    "start_pos": 119581,
    "end_pos": 120020,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 99,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 422
    }
  },
  "979": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_270",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "gle General 20B 2M 1T 512 TPU v4 JAX T5X GLM 33 ICLR 23 Apache-2.0 Multiple General 130B 400B 768 40G A100 1440h 3.37 Mil CodeGen 140 ICLR 23 Apache-2.0 Salesforce Coding 16B 650k 577B Heur Dedup TPU v4 JAXFormer LLaMA 127 arXiv 23 Meta General 65B 350k 1.4T Clf Heur Dedup 2048 80G A100 504h 4.12 Mil xFormers PanGu 92 arXiv 23 Huawei General 1.085T 329B 512 Ascend 910 2400h OP MindSpore",
    "chunk_index": 270,
    "start_pos": 120021,
    "end_pos": 120423,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 453,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 453,
      "optimized_content_length": 389
    }
  },
  "980": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_271",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "329B 512 Ascend 910 2400h OP MindSpore BloombergGPT 151 arXiv23 Bloomberg Finance 50B 139k 569B Dedup 512 40G A100 1272h 1.97 Mil PyTorch Xuan Yuan 2.0 152 arXiv23 RAIL-1.0 Du Xiaoman Finance 176B 366B Filtered 80GB A100 DS CodeT5 34 arXiv 23 BSD-3 Salesforce Coding 16B 110k 51.5B Dedup 16 40G A100 DS StarCoder 147 arXiv 23 OpenRAIL-M BigCode Coding 15.5B 250k 1T Dedup QF PF 512 80G A100 624h 1.28 Mil Megatron-LM",
    "chunk_index": 271,
    "start_pos": 120424,
    "end_pos": 120844,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 471,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 471,
      "optimized_content_length": 416
    }
  },
  "981": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_272",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "QF PF 512 80G A100 624h 1.28 Mil Megatron-LM LLaMA-2 21 arXiv 23 LLaMA-2.0 Meta General 70B 500k 2T Minimal Filtering 80G A100 1.7Mh PaLM-2 123 arXiv 23 Google General -Ddedup PF QF LLaMA-3.1 130 arXiv 24 LLaMA-3.0 Meta General 405B 1.2M 15T Dedup QF 16k 80G H100 30.84Mh PyTorch Mixtral 8x22B 131 web 24 Apache-2.0 Mistral AI General 141B Snowflake Arctic 132 web 24 Apache-2.0 Snowflake General 480B 3.5T DS",
    "chunk_index": 272,
    "start_pos": 120845,
    "end_pos": 121298,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 99,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 409
    }
  },
  "982": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_273",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "2.0 Snowflake General 480B 3.5T DS Nemotron-4 340B 137 web 24 Nvidia Nvidia General 340B 9T 6144 80G H100 DeepSeek 138 arXiv 24 MIT DeepSeek General 67B 2T Dedup QF 300.6Kh DS DeepSeek-v2 139 arXiv 24 MIT DeepSeek General 67B 8.1T QF H800 172.8Kh HAI-LLM Table 4. Summary of instruction tuned LLMs 10B All abbreviations are the same as Table 3. Entries in Data Tokens starting with S- represent the number of training samples. ModelsPublication VenueLicense TypeModel",
    "chunk_index": 273,
    "start_pos": 121299,
    "end_pos": 121794,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 100,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 467
    }
  },
  "983": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_274",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "samples. ModelsPublication VenueLicense TypeModel Creators PurposeNo. of ParamsCommercial UsePre-trained ModelsSteps TrainedData TokensNo. of Processing UnitsProcessing Unit TypeTrain. TimeCalculated Train. CostTrain. Parallelism Library WebGPT 166 arXiv 21 OpenAI General 175B GPT-3 T0 17 ICLR 22 Apache-2.0 BigScience General 11B T5 250B 512 TPU v3 270h 0.48 Mil Tk-Instruct 18 EMNLP 22 MIT AI2 General 11B T5 1000 256 TPU v3 4h 0.0036 Mil Google T5",
    "chunk_index": 274,
    "start_pos": 121795,
    "end_pos": 122238,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 451
    }
  },
  "984": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_275",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "T5 1000 256 TPU v3 4h 0.0036 Mil Google T5 OPT-IML 97 arXiv 22 Meta General 175B OPT 8k 2B 128 40G A100 Megatron Flan-U-PaLM 16 ICLR 22 Apache-2.0 Google General 540B U-PaLM 30k 512 TPU v4 JAX T5X mT0 154 ACL 23 Apache-2.0 HuggingFace General 13B mT5 Sparrow 167 arXiv 22 Google Dialog 70B Chinchilla 64 TPU v3 WizardCoder 164 arXiv 23 Apache-2.0 HK Bapt. Coding 15B StarCoder 200 S-78k",
    "chunk_index": 275,
    "start_pos": 122239,
    "end_pos": 122660,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 472,
      "word_count": 100,
      "optimized_for_embedding": true,
      "original_content_length": 472,
      "optimized_content_length": 386
    }
  },
  "985": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_276",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Bapt. Coding 15B StarCoder 200 S-78k Alpaca 158 Github 23 Apache-2.0 Stanford General 13B LLaMA 3-Epoch S-52k 80G A100 3h 600 FSDP PyTorch Vicuna 159 Github 23 Apache-2.0 LMSYS General 13B LLaMA 3-Epoch S-125k FSDP PyTorch LIMA 185 arXiv 23 Meta General 65B LLaMA 15-Epoch S-1000 Koala 300 Github 23 Apache-2.0 UC-Berkley General 13B LLaMA 2-Epoch S-472k A100 6h 100 JAX FLAX 5. Datasets and Evaluation Generating training and evaluation datasets is expensive be-",
    "chunk_index": 276,
    "start_pos": 122661,
    "end_pos": 123131,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 463
    }
  },
  "986": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_277",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "training and evaluation datasets is expensive be- cause of the large-scale data demand of LLMs. Hence. datasets for training and benchmarking these models are topics of key importance. summary of datasets commonly used by LLMs is provided next.5.1. Training Datasets The performance of LLMs largely depends on the training data quality. size. and diversity. Preparing training datasets of high quality at large scale is laborious. Researchers have suggested various pre-training and fine-tuning datasets to en-",
    "chunk_index": 277,
    "start_pos": 123132,
    "end_pos": 123598,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 510
    }
  },
  "987": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_278",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rious pre-training and fine-tuning datasets to en- hance LLMs capabilities. We summarize these fforts in Ta- ble 8. While numerous training datasets are available in the literature. we cover the most widely used ones in our summary. 25",
    "chunk_index": 278,
    "start_pos": 123599,
    "end_pos": 123785,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 237,
      "word_count": 39,
      "optimized_for_embedding": true,
      "original_content_length": 237,
      "optimized_content_length": 235
    }
  },
  "988": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_279",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "cover the most widely used ones in our summary. 25 --- Page 26 --- Table 5. Architecture details of LLMs. Here. PE is the positional embedding. nL is the number of layers. nH is the number of attention heads. HS is the size of hidden states. Models TypeTraining ObjectiveAttention Vocab Tokenizer Norm PE Activation Bias nL nH HS T5 11B Enc-Dec Span Corruption Standard 32k SentencePiece Pre-RMS Relative ReLU 24 128 1024 GPT3 175B Causal-Dec Next Token Dense Sparse Layer Learned GeLU 96 96 12288",
    "chunk_index": 279,
    "start_pos": 123787,
    "end_pos": 124253,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 497
    }
  },
  "989": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_280",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Dense Sparse Layer Learned GeLU 96 96 12288 mT5 13B Enc-Dec Span Corruption Standard 250k SentencePiece Pre-RMS Relative ReLU PanGu-\u03b1 200B Causal-Dec Next Token Standard 40k BPE Layer 64 128 16384 CPM-2 198B Enc-Dec Span Corruption Standard 250k SentencePiece Pre-RMS Relative ReLU 24 64 Codex 12B Causal-Dec Next Token Standard BPE Pre-Layer Learned GeLU 96 96 12288 ERNIE 3.0 10B Causal-Dec Next Token Standard WordPiece Post-Layer Relative GeLU 48 64 4096",
    "chunk_index": 280,
    "start_pos": 124254,
    "end_pos": 124704,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 458
    }
  },
  "990": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_281",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "WordPiece Post-Layer Relative GeLU 48 64 4096 Jurassic-1 178B Causal-Dec Next Token Standard 256k SentencePiece Pre-Layer Learned GeLU 76 96 13824 HyperCLOV 82B Causal-Dec Next Token Dense Sparse BPE Pre-Layer Learned GeLU 64 80 10240 Yuan 1.0 245B Causal-Dec Next Token Standard 76 -16384 Gopher 280B Causal-Dec Next Token Standard 32k SentencePiece Pre-RMS Relative GeLU 80 128 16384 ERNIE 3.0 Titan 260B Causal-Dec Next Token Standard WordPiece Post-Layer Relative GeLU 48 192 12288",
    "chunk_index": 281,
    "start_pos": 124705,
    "end_pos": 125182,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 485
    }
  },
  "991": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_282",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "WordPiece Post-Layer Relative GeLU 48 192 12288 GPT-NeoX-20B Causal-Dec Next Token Parallel 50k BPE Layer Rotary GeLU 44 64 OPT 175B Causal-Dec Next Token Standard BPE ReLU 96 96 BLOOM 176B Causal-Dec Next Token Standard 250k BPE Layer ALiBi GeLU 70 112 14336 Galactica 120B Causal-Dec Next Token Standard 50k BPE custom Layer Learned GeLU 96 80 10240 GLaM 1.2T MoE-Dec Next Token Standard 256k SentencePiece Layer Relative GeLU 64 128 32768",
    "chunk_index": 282,
    "start_pos": 125183,
    "end_pos": 125603,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 471,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 470,
      "optimized_content_length": 441
    }
  },
  "992": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_283",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "SentencePiece Layer Relative GeLU 64 128 32768 LaMDA 137B Causal-Dec Next Token Standard 32k BPE Layer Relative GeGLU 64 128 8192 MT-NLG 530B Causal-Dec Next Token Standard 50k BPE Pre-Layer Learned GeLU 105 128 20480 AlphaCode 41B Enc-Dec Next Token Multi-query 8k SentencePiece 64 128 6144 Chinchilla 70B Causal-Dec Next Token Standard 32k SentencePiece-NFKC Pre-RMS Relative GeLU 80 64 8192 PaLM 540B Causal-Dec Next Token Parallel Multi-query 256k SentencePiece Layer RoPE SwiGLU 118 48 18432",
    "chunk_index": 283,
    "start_pos": 125604,
    "end_pos": 126078,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 496
    }
  },
  "993": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_284",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "256k SentencePiece Layer RoPE SwiGLU 118 48 18432 AlexaTM 20B Enc-Dec Denoising Standard 150k SentencePiece Pre-Layer Learned GeLU 78 32 4096 Sparrow 70B Causal-Dec Pref. Rule RM 32k SentencePiece-NFKC Pre-RMS Relative GeLU 16 64 8192 U-PaLM 540B Non-Causal-Dec MoD Parallel Multi-query 256k SentencePiece Layer RoPE SwiGLU 118 48 18432 UL2 20B Enc-Dec MoD Standard 32k SentencePiece 64 16 4096 GLM 130B Non-Causal-Dec AR Blank Infilling Standard 130k SentencePiece Deep RoPE GeGLU 70 96 12288",
    "chunk_index": 284,
    "start_pos": 126079,
    "end_pos": 126549,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 493
    }
  },
  "994": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_285",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "130k SentencePiece Deep RoPE GeGLU 70 96 12288 CodeGen 16B Causal-Dec Next Token Parallel BPE Layer RoPE 34 24 LLaMA 65B Causal-Dec Next Token Standard 32k BPE Pre-RMS RoPE SwiGLU 80 64 8192 PanGu- 1085B Causal-Dec Next Token Standard BPE Fused Layer FastGeLU 40 40 5120 BloombergGPT 50B Causal-Dec Next Token Standard 131k Unigram Layer ALiBi GeLU 70 40 7680 Xuan Yuan 2.0 176B Causal-Dec Next Token Self 250k BPE Layer ALiBi GeLU 70 112 14336",
    "chunk_index": 285,
    "start_pos": 126550,
    "end_pos": 126978,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 479,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 479,
      "optimized_content_length": 444
    }
  },
  "995": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_286",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "oken Self 250k BPE Layer ALiBi GeLU 70 112 14336 CodeT5 16B Enc-Dec SC NT Cont. Match Standard Code-Specific StarCoder 15.5B Causal-Dec FIM Multi-query 49k BPE Learned 40 48 6144 LLaMA-2 70B Causal-Dec Next Token Grouped-query 32k BPE Pre-RMS RoPE SwiGLUE PaLM-2 MoD Parallel LLaMA-3.1 405B Causal-Dec Next Token Grouped-query 128k BPE Pre-RMS RoPE SwiGLU -126 128 16384 Nemotron-4 340B Causal-Dec Next Token Standard 256k SentencePiece RoPE ReLU 96 96 18432",
    "chunk_index": 286,
    "start_pos": 126979,
    "end_pos": 127453,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 96,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 458
    }
  },
  "996": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_287",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "andard 256k SentencePiece RoPE ReLU 96 96 18432 DeepSeek 67B Causal-Dec Next Token Grouped-query 100k BBPE Pre-RMS RoPE SwiGLU 95 64 8192 DeepSeek-v2 67B MoE-Dec Next Token Multi-Head Latent 100k BBPE Pre-RMS RoPE SwiGLU 60 128 5120 5.2. Evaluation Datasets and Tasks The evaluation of LLMs is important in gauging their profi- ciency and limitations. This process measures the model abil- ity to comprehend. generate. and interact with human language across spectrum of tasks. Evaluating language model LM",
    "chunk_index": 287,
    "start_pos": 127454,
    "end_pos": 127928,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 506
    }
  },
  "997": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_288",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "pectrum of tasks. Evaluating language model LM is divided into two broader categories. natural language un- derstanding NLU and natural language generation NLG It is emphasized that tasks in NLU and NLG are softly catego- rized and are often used interchangeably in the literature. Natural Language Understanding. It measures the language understanding capacity of LMs. It encompasses multiple tasks. including sentiment analysis. text classification. natural lan- guage inference NLI question answering QA common-",
    "chunk_index": 288,
    "start_pos": 127929,
    "end_pos": 128413,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 514
    }
  },
  "998": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_289",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "inference NLI question answering QA common- sense reasoning CR mathematical reasoning MR reading comprehension RC etc. Natural Language Generation. It assesses the language gener- ation capabilities of LLMs by understanding the provided input context. It includes tasks such as summarization. sentence com- pletion. machine translation MT dialogue generation. etc. Numerous datasets are proposed for each task. evaluating LLMs against di fferent characteristics. To provide an overview",
    "chunk_index": 289,
    "start_pos": 128414,
    "end_pos": 128867,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 503,
      "optimized_content_length": 485
    }
  },
  "999": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_290",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "di fferent characteristics. To provide an overview of evaluation datasets. we briefly discuss few famous datasets within each category and ffer comprehensive list of datasets in Table 9. Moreover. we show detailed overview of the train- ing datasets and evaluation tasks and benchmarks used by vari-ous pre-trained LLMs in Table 10 and fine-tuned LLMs in Ta- ble 11. We also compare the top-performing LLMs in various NLP tasks in Table 12. 5.2.1. Multi-task MMLU 307 benchmark that measures the knowledge",
    "chunk_index": 290,
    "start_pos": 128868,
    "end_pos": 129335,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 505
    }
  },
  "1000": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_291",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "MLU 307 benchmark that measures the knowledge acquired by models during pretraining and evaluates models in zero-shot and few-shot settings across 57 subjects. testing both world knowledge and problem-solving ability. SuperGLUE more challenging and diverse successor to the GLUE 309 benchmark. SuperGLUE includes variety of language understanding tasks. such as question answering. natural language inference. and co-reference resolution. It is designed to provide rigorous test of language understanding",
    "chunk_index": 291,
    "start_pos": 129336,
    "end_pos": 129807,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 504
    }
  },
  "1001": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_292",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "provide rigorous test of language understanding and requires significant progress in areas like sample-e fficient. transfer. multi-task. and unsupervised or self-supervised learn- ing. BIG-bench 308 The BIG-bench Behavior of Intelligent Generative Models Benchmark is large-scale benchmark de- signed to test the abilities of LLMs across wide range of tasks. including reasoning. creativity. ethics. and understanding of specific domains. GLUE 309 The General Language Understanding Evalua-",
    "chunk_index": 292,
    "start_pos": 129808,
    "end_pos": 130262,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 490
    }
  },
  "1002": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_293",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "309 The General Language Understanding Evalua- tion GLUE benchmark is collection of resources for train- ing. evaluating. and analyzing natural language understanding 26",
    "chunk_index": 293,
    "start_pos": 130263,
    "end_pos": 130389,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 177,
      "word_count": 24,
      "optimized_for_embedding": true,
      "original_content_length": 176,
      "optimized_content_length": 169
    }
  },
  "1003": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_294",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "g. and analyzing natural language understanding 26 --- Page 27 --- Table 6. Summary of optimization settings used for pre-trained LLMs. The values for weight decay. gradient clipping. and dropout are 0.1. 1.0. and 0.1. respectively. for most of the LLMs. Sequence LR Optimizers Precision Weight Grad Models Batch Size Length LR Warmup Decay AdaFactor Adam AdamW FP16 BF16 Mixed Decay Clip Dropout T5 11B 211512 0.01 inverse square root GPT3 175B 32K 6e-5 cosine",
    "chunk_index": 294,
    "start_pos": 130391,
    "end_pos": 130834,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 461
    }
  },
  "1004": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_295",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "GPT3 175B 32K 6e-5 cosine mT5 13B 1024 1024 0.01 inverse square root PanGu-\u03b1 200B 1024 2e-5 CPM-2 198B 1024 1024 0.001 Codex 12B 6e-5 cosine ERNIE 3.0 12B 6144 512 1e-4 linear Jurassic-1 178B 3.2M 2048 6e-5 cosine HyperCLOV 82B 1024 6e-5 cosine Yuan 1.0 245B 10M 2048 1.6e-4 cosine decay to 10 Gopher 280B 3M 2048 4e-5 cosine decay to 10",
    "chunk_index": 295,
    "start_pos": 130835,
    "end_pos": 131315,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 147,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 337
    }
  },
  "1005": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_296",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "280B 3M 2048 4e-5 cosine decay to 10 ERNIE 3.0 Titan 260B 512 1e-4 linear GPT-NeoX-20B 1538 2048 0.97e-5 cosine OPT 175B 2M 2048 1.2e-4 linear BLOOM 176B 2048 2048 6e-5 cosine Galactica 120B 2M 2048 7e-6 linear decay to 10 GLaM 1.2T 1M 1024 0.01 inverse square root FP32 LaMDA 137B 256K MT-NLG 530B 1920 2048 5e-5 cosine decay to 10 AlphaCode 41B 2048 1536 768 1e-4 cosine decay to 10",
    "chunk_index": 296,
    "start_pos": 131316,
    "end_pos": 131808,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 138,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 384
    }
  },
  "1006": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_297",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "2048 1536 768 1e-4 cosine decay to 10 Chinchilla 70B 1.5M 2048 1e-4 cosine decay to 10 PaLM 540B 2048 2048 0.01 inverse square root AlexaTM 20B 2M 1024 1e-4 linear decay to U-PaLM 540B 32 2048 1e-4 cosine UL2 20B 1024 1024 inverse square root GLM 130B 4224 2048 8e-5 cosine CodeGen 16B 2M 2048 5e-5 cosine LLaMA 65B 4M Tokens 2048 1.5e-4 cosine decay to 10",
    "chunk_index": 297,
    "start_pos": 131809,
    "end_pos": 132268,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 133,
      "optimized_for_embedding": true,
      "original_content_length": 510,
      "optimized_content_length": 356
    }
  },
  "1007": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_298",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ns 2048 1.5e-4 cosine decay to 10 PanGu- 1.085T 512 1024 2e-5 BloombergGPT 50B 2048 2048 6e-5 cosine Xuan Yuan 2.0 176B 2048 2048 6e-5 cosine CodeT5 16B 2048 1024 2e-4 linear StarCoder 15.5B 512 8k 3e-4 cosine LLaMA-2 70B 4M Tokens 4k 1.5e-4 cosine LLaMA-3.1 405B 16M 8192 8e-5 linear cosine Nemotron-4 340B 2304 4096 linear DeepSeek 67B 4608 4096 3.2e-4 cosine",
    "chunk_index": 298,
    "start_pos": 132269,
    "end_pos": 132731,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 127,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 361
    }
  },
  "1008": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_299",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "DeepSeek 67B 4608 4096 3.2e-4 cosine DeepSeek-v2 67B 9216 4k 2.4e-4 step-decay Table 7. Summary of optimization settings used for instruction-tuned LLMs. Values for gradient clipping and dropout are the same as the pre-trained models. while no model uses weight decay for instruction tuning. Sequence Optimizers Grad Models Batch Size Length LR Warmup LR_Decay AdaFactor Adam AdamW Clip Dropout WebGPT 175B BC.512. RM.32 -6e-5 T0 11B 1024 1280 1e-3",
    "chunk_index": 299,
    "start_pos": 132732,
    "end_pos": 133185,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 448
    }
  },
  "1009": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_300",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "-6e-5 T0 11B 1024 1280 1e-3 Tk-Instruct 11B 1024 -1e-5 constant OPT-IML 175B 128 2048 5e-5 linear Flan-U-PaLM 540B 32 -1e-3 constant Sparrow 70B RM. 16. RL.16 -2e-6 cosine decay to 10 WizardCoder 15B 512 2048 2e-5 cosine Alpaca 13B 128 512 1e-5 cosine Vicuna 13B 128 -2048 2e-5 cosine LIMA 65B 32 2048 1e-5 linear systems. It includes variety of tasks that test wide range of",
    "chunk_index": 300,
    "start_pos": 133186,
    "end_pos": 133628,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 113,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 375
    }
  },
  "1010": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_301",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ludes variety of tasks that test wide range of linguistic phenomena. making it comprehensive tool for eval- uating language understanding in AI. 5.2.2. Language Understanding WinoGrande 354 large-scale dataset inspired by the orig- inal Winograd 357 Schema Challenge tests models on their ability to resolve pronoun ambiguity and encourages the devel- opment of models that understand the broad context in naturallanguage text. CoQA 316 conversational question-answering dataset.",
    "chunk_index": 301,
    "start_pos": 133629,
    "end_pos": 134075,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 479
    }
  },
  "1011": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_302",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "316 conversational question-answering dataset. CoQA challenges models with questions that rely on conver- sation history and require free-form text answers. Its diverse content from seven domains makes it rigorous test for mod- els ability to handle wide range of topics and conversational contexts. WiC 317 This dataset assesses model ability to dis- cern word meanings based on context. aiding in tasks related 27",
    "chunk_index": 302,
    "start_pos": 134076,
    "end_pos": 134456,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 431,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 431,
      "optimized_content_length": 415
    }
  },
  "1012": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_303",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nings based on context. aiding in tasks related 27 --- Page 28 --- Table 8. Details of various well-known pre-training and fine-tuning datasets. Here. alignment means aligning with human preferences. Dataset Type Size Samples Tasks Source Creation Comments C4 10 Pretrain 806GB Common Crawl Automated clean. multilingual dataset with billions of tokens mC4 11 Pretrain 38.49TB Common Crawl Automated multilingual extension of the C4 dataset. mC4 identifies over 100 lan- guages using cld3 from 71 monthly web scrapes of Common Crawl.",
    "chunk_index": 303,
    "start_pos": 134458,
    "end_pos": 134952,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 533
    }
  },
  "1013": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_304",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "cld3 from 71 monthly web scrapes of Common Crawl. PILE 301 Pretrain 825GB -Common Crawl. PubMed Central. OpenWebText2. ArXiv. GitHub. Books3. and othersAutomated massive dataset comprised of 22 con- stituent sub-datasets ROOTs 302 Pretrain 1.61TB 498 Hugging Face datasets Automated 46 natural and 13 programming lan- guages MassiveText 116 Pretrain 10.5TB -MassiveWeb. Books. News. Wikipedia. Github. C4Automated 99 of the data is in English Wikipedia 303 Pretrain Wikipedia Automated Dump of wikipedia",
    "chunk_index": 304,
    "start_pos": 134953,
    "end_pos": 135423,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 503
    }
  },
  "1014": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_305",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Pretrain Wikipedia Automated Dump of wikipedia RedPajama 304 Pretrain 5TB -CommonCrawl. C4. Wikipedia. Github. Books. StackExchangeAutomated Open-source replica of LLaMA dataset PushShift.io Reddit Pretrain 21.1GB Reddit Automated Submissions and comments on Reddit from 2005 to 2019 BigPython 140 Pretrain 5.5TB Coding GitHub Automated Pool of Prompt P3 17 Instructions 12M 62 PromptSource Manual Subset of PromptSource. created from 177 datasets including summarization. QA. classification. etc.",
    "chunk_index": 305,
    "start_pos": 135424,
    "end_pos": 135888,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 515,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 497
    }
  },
  "1015": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_306",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "including summarization. QA. classification. etc. xP3 154 Instructions 81M 71 P3 Multilingual datasets Manual Extending P3 to total 46 languages Super-NaturalInstructions SNI 18 Instructions 12.4M 1616 Multiple datasets Manual Extending P3 with additional multi- lingual datasets. total 46 languages Flan 16 Instructions 15M 1836 Muffin T0-SF NIV2 Manual Total 60 languages OPT-IML 97 Instructions 18.1M 1667 Manual Self-Instruct 19 Instructions 82k 175 Automated Generated 52k instructions with 82k sam-",
    "chunk_index": 306,
    "start_pos": 135889,
    "end_pos": 136362,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 504
    }
  },
  "1016": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_307",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Automated Generated 52k instructions with 82k sam- ples from 175 seed tasks using GPT-3 Alpaca 158 Instructions 52k Automated Employed self-instruct method to gener- ate data from text-davinci-003 Vicuna 159 Instructions 125k ShareGPT Automated Conversations shared by users on ShareGPT using public APIs LLaMA-GPT-4 160 Instructions 52k Alpaca Automated Recreated Alpaca dataset with GPT-4 in English and Chinese Unnatural Instructions 305 Instructions 68k 15-Seeds SNI Automated",
    "chunk_index": 307,
    "start_pos": 136363,
    "end_pos": 136814,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 480
    }
  },
  "1017": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_308",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "305 Instructions 68k 15-Seeds SNI Automated LIMA 185 Instructions 1k Multiple datasets Manual Carefully created samples to test perfor- mance with fine-tuning on less data Anthropic-HH-RLHF 306 Alignment 142k Manual Anthropic-HH-RLHF-2 178 Alignment 39k Manual to Word Sense Disambiguation. Wikitext103 318 With over 100 million tokens from Wikipedia top articles. this dataset is rich resource for tasks that require understanding long-term dependencies. such as lan- guage modeling and translation.",
    "chunk_index": 308,
    "start_pos": 136815,
    "end_pos": 137294,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 500
    }
  },
  "1018": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_309",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "cies. such as lan- guage modeling and translation. PG19 319 This is digital library of diverse books from Project Gutenberg. It is specifically designed to facilitate re- search in unsupervised learning and language modeling. with special focus on long-form content. C4 10 clean. multilingual dataset. C4 ffers billions of to- kens from web-crawled data. It is comprehensive resource for training advanced Transformer models on various languages. LCQMC 320 The Large-scale Chinese Question Matching",
    "chunk_index": 309,
    "start_pos": 137295,
    "end_pos": 137761,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 498
    }
  },
  "1019": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_310",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "320 The Large-scale Chinese Question Matching Corpus LCQMC is dataset for evaluating the performance of models in semantic matching tasks. It contains pairs of ques- tions in Chinese and their matching status. making it valuable resource for research in Chinese language understanding. 5.2.3. Story Cloze and Sentence Completion StoryCloze 334 It introduces new StoryCloze Test commonsense reasoning framework for evaluating story under-",
    "chunk_index": 310,
    "start_pos": 137762,
    "end_pos": 138169,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 458,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 458,
      "optimized_content_length": 437
    }
  },
  "1020": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_311",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "se reasoning framework for evaluating story under- standing. generation. and script learning. It considers model sability to understand and generate coherent and sensible stories. LAMBADA 335 This dataset evaluates contextual text un- derstanding through word prediction task. Models must pre- dict the last word of passage. which is easy for humans when given the whole passage. but not when given only the last sen- tence. 5.2.4. Physical Knowledge and World Understanding PIQA 340 dataset that probes the physical knowledge of",
    "chunk_index": 311,
    "start_pos": 138170,
    "end_pos": 138662,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 529
    }
  },
  "1021": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_312",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "dataset that probes the physical knowledge of models. aiming to understand how well they are learning about the real world. TriviaQA 341 dataset that tests models on reading com- prehension and open domain question answering QA tasks. with focus on Information Retrieval IR -style QA. ARC 342 larger version of the ARC-Challenge. this dataset contains both easy and challenging grade-school level. multiple-choice science questions. It is comprehensive test of model ability to understand and answer complex questions.",
    "chunk_index": 312,
    "start_pos": 138663,
    "end_pos": 139156,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 518
    }
  },
  "1022": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_313",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "bility to understand and answer complex questions. ARC-Easy 342 subset of the ARC dataset. ARC- Easy. contains questions that are answered correctly by either retrieval-based algorithm or word co-occurrence algorithm. 28",
    "chunk_index": 313,
    "start_pos": 139157,
    "end_pos": 139335,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 229,
      "word_count": 34,
      "optimized_for_embedding": true,
      "original_content_length": 229,
      "optimized_content_length": 220
    }
  },
  "1023": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_314",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ed algorithm or word co-occurrence algorithm. 28 --- Page 29 --- Table 9. Categorized evaluation datasets used in evaluating LLMs. Type Datasets Benchmarks Multi-Task MMLU 307 SuperGLUE BIG-bench 308 GLUE 309 BBH 308 CUGE 310 Zero- CLUE 311 FewCLUE 312 Blended Skill Talk 313 HELM 314 KLUE-STS 315 Language Understanding CoQA 316 WiC 317 Wikitext103 318 PG19 319 LCQMC 320 QQP 321 WinoGender 322 CB 323 FinRE 324 SanWen 325 AFQMC 311 BQ Corpus 326 CNSS 327 CKBQA 13 328",
    "chunk_index": 314,
    "start_pos": 139337,
    "end_pos": 139834,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 469
    }
  },
  "1024": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_315",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "311 BQ Corpus 326 CNSS 327 CKBQA 13 328 CLUENER 311 Weibo 329 AQuA 330 OntoNotes 331 HeadQA 332 Twitter Dataset 333 Story Cloze and Sentence CompletionStoryCloze 334 LAMBADA 335 LCSTS 336 AdGen 337 E2E 338 CHID 339 CHID- FC 312 Physical Knowledge and World UnderstandingPIQA 340 TriviaQA 341 ARC 342 ARC-Easy 342 ARC-Challenge 342 PROST 343 Open- BookQA 344 WebNLG 345 DogWhistle Insider Outsider 346 Contextual Language",
    "chunk_index": 315,
    "start_pos": 139835,
    "end_pos": 140280,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 420
    }
  },
  "1025": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_316",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "istle Insider Outsider 346 Contextual Language UnderstandingRACE 347 RACE-Middle 347 RACE-High 347 QuAC 348 StrategyQA 349 Quiz Bowl 350 cMedQA 351 .cMedQA2 352 MATINF-QA 353 Commonsense Reasoning WinoGrande 354 HellaSwag 355 COPA 356 WSC 357 CSQA 358 SIQA 359 C3 360 CLUEWSC2020 311 CLUEWSC 311 CLUEWSC-FC 312 ReCoRD 361 Reading Comprehension SQuAD 362 BoolQ 363 SQUADv2 364 DROP 365 RTE 366 WebQA 367 CMRC2017 368",
    "chunk_index": 316,
    "start_pos": 140281,
    "end_pos": 140725,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 415
    }
  },
  "1026": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_317",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ROP 365 RTE 366 WebQA 367 CMRC2017 368 CMRC2018 369 CMRC2019 370 COTE-BD 371 COTE-DP 371 COTE-MFW 371 Mul- tiRC 372 Natural Questions 373 CNSE 327 DRCD 374 DuReader 375 Dureader robust 376 DuReader-QG 375 SciQ 377 Sogou-log 378 Dureader robust-QG 376 QA4MRE 379 KorQuAD 1.0 380 CAIL2018-Task1 Task2 381 Mathematical Reasoning MATH 382 Math23k 383 GSM8K 384 MathQA 385 MGSM 386 MultiArith 387 AS- Div 388 MAWPS 389 SV AMP 390",
    "chunk_index": 317,
    "start_pos": 140726,
    "end_pos": 141192,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 424
    }
  },
  "1027": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_318",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "th 387 AS- Div 388 MAWPS 389 SV AMP 390 Problem Solving HumanEval 141 DS-1000 391 MBPP 392 APPS 382 CodeContests 142 Natural Language Inference Logical ReasoningANLI 393 MNLI-m 394 MNLI-mm 394 .QNLI 362 WNLI 357 OCNLI 311 CMNLI 311 ANLI R1 393 ANLI R2 393 ANLI R3 393 HANS 395 OCNLI-FC 312 LogiQA 396 Strate- gyQA 349 Cross-Lingual Understanding MLQA 397 XNLI 398 PAWS-X 399 XSum 400 XCOPA 401 XWinograd 402 TyDiQA- GoldP 403 MLSum 404",
    "chunk_index": 318,
    "start_pos": 141193,
    "end_pos": 141666,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 435
    }
  },
  "1028": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_319",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "XWinograd 402 TyDiQA- GoldP 403 MLSum 404 Truthfulness and Fact Checking TruthfulQA 405 MultiFC 406 Fact Checking on Fever 407 Biases and Ethics in AI ETHOS 408 StereoSet 409 BBQ 410 Winobias 411 CrowS-Pairs 412 Toxicity RealToxicityPrompts 413 CivilComments toxicity classification 414 Language Translation WMT 415 WMT20 416 WMT20-enzh 416 EPRSTMT 312 CCPM 417 Scientific Knowledge AminoProbe 148 BioLAMA 148 Chemical Reactions 148 Galaxy Clusters 148 Mineral Groups 148",
    "chunk_index": 319,
    "start_pos": 141667,
    "end_pos": 142151,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 471
    }
  },
  "1029": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_320",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "148 Galaxy Clusters 148 Mineral Groups 148 Dialogue Wizard of Wikipedia 418 Empathetic Dialogues 419 DPC-generated 96 dialogues. ConvAI2 420 KdConv 421 Topic Classification TNEWS-FC 312 YNAT 315 KLUE-TC 315 CSL 311 CSL-FC 312 IFLYTEK 422 It is great starting point for models beginning to explore ad- vanced question-answering. ARC-Challenge 342 rigorous question-answering dataset. ARC-Challenge includes complex. grade-school level questions that demand reasoning beyond simple retrieval. test-",
    "chunk_index": 320,
    "start_pos": 142152,
    "end_pos": 142642,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 496
    }
  },
  "1030": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_321",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "at demand reasoning beyond simple retrieval. test- ing the true comprehension capabilities of models. 5.2.5. Contextual Language Understanding RACE 347 The RACE dataset is reading comprehension dataset collected from English examinations in China. which benchmarks AI models for understanding and answering ques- tions on long and complex passages. simulating the challenge of real-world examination. RACE-Middle 347 Another subset of the RACE 347 dataset. RACE-Middle. contains middle school-level English",
    "chunk_index": 321,
    "start_pos": 142643,
    "end_pos": 143110,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 506
    }
  },
  "1031": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_322",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "RACE-Middle. contains middle school-level English exam questions. It ffers slightly less challenging but academ- ically oriented evaluation of model comprehension skills. RACE-High 347 subset of the RACE 347 dataset. RACE-High consists of high school-level English exam ques-tions. It is designed to evaluate the comprehension ability of models in more academic and challenging context. QuAC 348 This dataset simulates an information-seeking dialog between students and teachers using hidden Wikipedia",
    "chunk_index": 322,
    "start_pos": 143111,
    "end_pos": 143582,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 501
    }
  },
  "1032": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_323",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tween students and teachers using hidden Wikipedia text. It introduces unique challenges not found in machine com- prehension datasets. making it valuable resource for advanc- ing dialog systems. 5.2.6. Commonsense Reasoning HellaSwag 355 dataset that challenges models to pick the best ending to context uses Adversarial Filtering to create Goldilocks zone of complexity. where generated text is absurd to humans but often misclassified by models. COPA 401 This dataset evaluates model progress in",
    "chunk_index": 323,
    "start_pos": 143583,
    "end_pos": 144050,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 498
    }
  },
  "1033": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_324",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "401 This dataset evaluates model progress in open-domain commonsense causal reasoning. Each question comprises premise and two alternatives. and the model must select the more plausible alternative. testing model ability to understand and reason about cause and ffect. WSC 357 The Winograd Schema Challenge WSC is 29",
    "chunk_index": 324,
    "start_pos": 144051,
    "end_pos": 144337,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 337,
      "word_count": 52,
      "optimized_for_embedding": true,
      "original_content_length": 337,
      "optimized_content_length": 316
    }
  },
  "1034": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_325",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "357 The Winograd Schema Challenge WSC is 29 --- Page 30 --- Table 10. An illustration of training datasets and evaluation tasks employed by pre-trained LLMs. Here. QA is question-answering. Clf is classification. NLI is natural language inference. MT is machine translation. RC is reading comprehension. CR is commonsense reasoning. MR is mathematical reasoning. Mem. is memorization. Benchmark Models Training DatasetBIG- benchMMLUSuper GLUEQA Clf NLI MTCloze CompletionRC CR MR CodingTruthful Bias Toxicity Mem.",
    "chunk_index": 325,
    "start_pos": 144339,
    "end_pos": 144832,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 513
    }
  },
  "1035": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_326",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "onRC CR MR CodingTruthful Bias Toxicity Mem. T5 C4 10 GPT-3 Common Crawl. WebText. Books Cor- pora. Wikipedia mT5 mC4 11 PanGu-\u03b1 1.1TB Chinese Text Corpus CPM-2 WuDaoCorpus 109 Codex 54 million public repositories from Github ERNIE-3.0 Chinese text corpora. Baidu Search. Web text. QA-long. QA-short. Poetry and Cou- plet Domain-specific data from medical. law. and financial area Baidu knowledge graph with more than 50 million facts",
    "chunk_index": 326,
    "start_pos": 144833,
    "end_pos": 145284,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 434
    }
  },
  "1036": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_327",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "graph with more than 50 million facts Jurassic-1 Wikipedia. OWT. Books. C4. Pile 301 arXiv. GitHub HyperCLOV Korean blogs. Community sites. News. KiN Korean Wikipedia. Wikipedia En- glish and Japanese Modu-Corpus. Mes- senger. News. Spoken and written lan- guage corpus. Web corpus Yuan 1.0 Common Crawl. SogouT. Sogou News. Baidu Baike. Wikipedia. Books Gopher subsets of MassiveWeb Books. C4. News. GitHub and Wikipedia samples from Mas- siveText ERNIE-3.0 TITAN Same as ERNIE 3.0 and ERNIE 3.0 ad-",
    "chunk_index": 327,
    "start_pos": 145285,
    "end_pos": 145780,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 500
    }
  },
  "1037": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_328",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "RNIE-3.0 TITAN Same as ERNIE 3.0 and ERNIE 3.0 ad- versarial dataset. ERNIE 3.0 controllable dataset GPT-NeoX-20B Pile 301 OPT RoBERTa 299 Pile 301 PushShift.io Reddit 423 BLOOM ROOTs 13 Galactica arXiv. PMC. Semantic Scholar. Wikipedia. StackExchange. LibreText. Open Text- books. RefSeq Genome. OEIS. LIPID MAPS. NASAExoplanet. Common Crawl. ScientificCC. AcademicCC. GitHub repos- itories Khan Problems. GSM8K. OneS- mallStep GLaM Filtered Webpages. Social media conversa-",
    "chunk_index": 328,
    "start_pos": 145781,
    "end_pos": 146261,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 475
    }
  },
  "1038": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_329",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "GLaM Filtered Webpages. Social media conversa- tions Wikipedia. Forums. Books. News LaMDA Infiniset Public documents. Dialogs. Ut- terances MT-NLG Two snapshots of Common Crawl and Books3. OpenWebText2. Stack Exchange. PubMed Abstracts. Wikipedia. PG-19 242 BookCorpus2. NIH ExPorter. Pile. CC-Stories. RealNews AlphaCode Selected GitHub repositories. CodeCon- tests. Codeforces. Description2Code. Co- deNet Chinchilla MassiveWeb. MassiveText Books. C4. News. GitHub. Wikipedia",
    "chunk_index": 329,
    "start_pos": 146262,
    "end_pos": 146725,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 477
    }
  },
  "1039": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_330",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "eText Books. C4. News. GitHub. Wikipedia PaLM webpages. books. Wikipedia. news. arti- cles. source code. social media conversa- tions AlexaTM Wikipedia. mC4 U-PaLM Same as PaLM UL2 GLM-130B CodeGen Pile. BigQuery. BigPython LLaMA CommonCrawl. C4. Github. Wikipedia. Books. arXiv. StackExchange PanGu- WuDaoCorpora. CLUE. Pile. C4. Python code BloombergGPT inPile. Pile. C4. Wikipedia CodeT5 CodeSearchNet. Github Code",
    "chunk_index": 330,
    "start_pos": 146726,
    "end_pos": 147197,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 101,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 417
    }
  },
  "1040": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_331",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "CodeT5 CodeSearchNet. Github Code StarCoder The Stack v1.2 LLaMA-2 PaLM-2 Web documents. Code. Books. Maths. Conversation 30",
    "chunk_index": 331,
    "start_pos": 147198,
    "end_pos": 147324,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 177,
      "word_count": 39,
      "optimized_for_embedding": true,
      "original_content_length": 177,
      "optimized_content_length": 124
    }
  },
  "1041": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_332",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ode. Books. Maths. Conversation 30 --- Page 31 --- Table 11. An illustration of training datasets and evaluation benchmarks used in fine-tuned LLMs. SNI is short of Super-NaturalInsturctions. Models Training DatasetBIG- benchMMLU BBH RAFT FLAN SNI PromptSource TyDiQA HumanEval MBPPTruthful Bias Toxicity T0 Pool of Prompts WebGPT ELI5 424 ELI5 fact- check 166 TriviaQA 341 ARC-Challenge 342 ARC- Easy 342 Hand-written data. Demonstrations of humans. Com- parisons between model-generated answers",
    "chunk_index": 332,
    "start_pos": 147326,
    "end_pos": 147813,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 496
    }
  },
  "1042": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_333",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ns. Com- parisons between model-generated answers Tk-INSTRUCT SNI 18 mT0 xP3 154 OPT-IML PromptSource 17 FLAN 16 SNI 425 UnifiedSKG 426 CrossFit 427 ExMix 428 T5 10 Reasoning Flan Muffin. T0-SF. NIv2. CoT WizardCoder Code Alpaca reading comprehension task in which system must resolve references in text. often requiring world knowledge and rea- soning about the text. CSQA 358 The CommonsenseQA is question-answering dataset that requires commonsense knowledge to evaluate the",
    "chunk_index": 333,
    "start_pos": 147814,
    "end_pos": 148298,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 477
    }
  },
  "1043": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_334",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "hat requires commonsense knowledge to evaluate the ability of AI models to understand and answer questions. 5.2.7. Reading Comprehension BoolQ 363 dataset derived from Google search queries. BoolQ challenges models to answer binary yes no questions. The questions are naturally occurring and are paired with paragraph from Wikipedia article containing the answer. It is test of reading comprehension and reasoning. SQUADv2 364 The Stanford Question Answering Dataset SQuAD 362 is collection of questions posed by crowd",
    "chunk_index": 334,
    "start_pos": 148299,
    "end_pos": 148789,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 518
    }
  },
  "1044": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_335",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "362 is collection of questions posed by crowd workers on set of Wikipedia articles. where the answer to ev- ery question is segment of text from the corresponding reading passage. SQuADv2 combines the original SQuAD1.1 dataset with over 50.000 unanswerable questions. The aim is to evalu- ate model ability to understand and answer questions based on given context and to determine when question is unan- swerable. DROP 365 DROP. or Discrete Reasoning Over the con- tent of Paragraphs. is designed to test model ability to un-",
    "chunk_index": 335,
    "start_pos": 148790,
    "end_pos": 149289,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 526
    }
  },
  "1045": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_336",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "aphs. is designed to test model ability to un- derstand wide variety of reading phenomena. It encourages comprehensive and reliable evaluation of reading comprehen- sion capabilities. RTE 366 The Recognizing Textual Entailment RTE datasets come from series of annual competitions on textual entailment. predicting whether given sentence logically fol- lows from another and evaluating model understanding of logical relationships in text. WebQA 367 dataset for open-domain question answering.",
    "chunk_index": 336,
    "start_pos": 149290,
    "end_pos": 149757,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 492
    }
  },
  "1046": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_337",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "67 dataset for open-domain question answering. WebQA ffers large collection of web-based question-answer pairs. It is designed to assess the ability of AI models to under- stand and answer questions based on web content. CMRC2018 369 This dataset is test of Chinese language models ability to reason comprehensively and is designed with challenging span-extraction format that pushes the boundariesof machine performance. 5.2.8. Mathematical Reasoning MATH 382 This dataset is platform for evaluating the",
    "chunk_index": 337,
    "start_pos": 149758,
    "end_pos": 150232,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 504
    }
  },
  "1047": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_338",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "82 This dataset is platform for evaluating the mathematical problem-solving abilities of AI models. It con- tains diverse set of math problems. ranging from arithmetic to calculus. and is designed to test the model ability to under- stand and solve complex mathematical problems. Math23k 383 This one challenges model ability to un- derstand and solve mathematical word problems. It contains 23.000 Chinese arithmetic word problems that require models to perform reasoning and computation based on the problem description.",
    "chunk_index": 338,
    "start_pos": 150233,
    "end_pos": 150719,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 522
    }
  },
  "1048": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_339",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "and computation based on the problem description. GSM8K 384 dataset of diverse grade school math word problems. testing model ability to perform multi-step math- ematical reasoning. 5.2.9. Problem Solving and Logical Reasoning ANLI 393 large-scale dataset designed to test the robust- ness of machine learning models in Natural Language Inference NLI is created through an iterative. adversarial process where humans try to generate examples that models cannot correctly classify.",
    "chunk_index": 339,
    "start_pos": 150720,
    "end_pos": 151166,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 480
    }
  },
  "1049": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_340",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "te examples that models cannot correctly classify. HumanEval 141 dataset for evaluating the problem- solving ability of AI models. which includes diverse set of tasks that require various cognitive abilities. making it com- prehensive tool for assessing general intelligence in AI. StrategyQA 349 question-answering dataset that re- quires reasoning over multiple pieces of evidence to evaluate the strategic reasoning ability of AI models. pushing the bound- aries of what machines can understand and answer.",
    "chunk_index": 340,
    "start_pos": 151167,
    "end_pos": 151639,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 509
    }
  },
  "1050": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_341",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "aries of what machines can understand and answer. 5.2.10. Cross-Lingual Understanding XNLI 398 cross-lingual benchmark. XNLI extends the MultiNLI 429 corpus to 15 languages. including low-resource ones like Urdu. It tests models on cross-lingual sentence under- standing. with 112.500 annotated pairs across three categories. entailment. contradiction. and neutral. PAWS-X 399 PAWS-X. or Cross-lingual Paraphrase Adver- saries from Word Scrambling. is multilingual version of the 31",
    "chunk_index": 341,
    "start_pos": 151640,
    "end_pos": 152084,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 482
    }
  },
  "1051": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_342",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rd Scrambling. is multilingual version of the 31 --- Page 32 --- PAWS 430 dataset for paraphrase identification. It includes examples in seven languages and is designed to evaluate the performance of cross-lingual paraphrase identification models. 5.2.11. Truthfulness Truthful-QA 405 unique benchmark that measures language model truthfulness when generating answers. The dataset includes questions across various categories like health. law. and politics. some designed to test the model against com- mon human misconceptions.",
    "chunk_index": 342,
    "start_pos": 152086,
    "end_pos": 152576,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 528
    }
  },
  "1052": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_343",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "the model against com- mon human misconceptions. 5.2.12. Biases and Ethics in AI ETHOS 408 ETHOS is hate speech detection dataset built from YouTube and Reddit comments. It is tool in the fight against online hate speech. ffering binary and multi-label variants for robust content moderation. StereoSet 409 StereoSet is comprehensive dataset de- signed to measure and evaluate the presence of stereotypical biases in language models. It focuses on four key domains. gender. profession. race. and religion. Contrasting stereotypi-",
    "chunk_index": 343,
    "start_pos": 152577,
    "end_pos": 153071,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 529
    }
  },
  "1053": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_344",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ssion. race. and religion. Contrasting stereotypi- cal bias against language modeling ability provides valuable tool for understanding and mitigating biases in large language models. 6. Applications Applying Large Language Models LLMs to variety of downstream tasks has become popular trend in both AI- related research communities and industries. with many emerg- ing uses being discovered and explored daily. LLMs. which are capable of understanding and generating human-like text. have",
    "chunk_index": 344,
    "start_pos": 153072,
    "end_pos": 153517,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 488
    }
  },
  "1054": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_345",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "understanding and generating human-like text. have found meaningful applications across variety of fields. This section provides an overview of LLM applications in medicine. education. science. mathematics. law. finance. robotics. and coding. While each of these domains pose di fferent challenges. LLMs open up opportunities to make significant contributions to these domains through their generalizability. General Purpose. LLMs are being widely considered as general-purpose tools for wide variety of tasks 431 This",
    "chunk_index": 345,
    "start_pos": 153518,
    "end_pos": 153992,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 518
    }
  },
  "1055": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_346",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "pose tools for wide variety of tasks 431 This is due to their inherent ability to understand. generate. and manipulate human-like text in contextually relevant man- ner. This allows them to perform tasks ranging from simple language translation and question-answering to more complex tasks like summarization. text generation. and even program- ming help 432 The utility of LLMs is further enhanced by their ability to adapt to the specific style and tone of the text they are processing. making the outputs more user-friendly and",
    "chunk_index": 346,
    "start_pos": 153993,
    "end_pos": 154482,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 530
    }
  },
  "1056": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_347",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "cessing. making the outputs more user-friendly and context-aware. In everyday applications. LLMs can be used as personal assistants. helping users draft emails or schedule appointments 433 they can also be deployed in customer ser- vice to handle common questions or applied to generate content for digital platforms like websites by creating human-like text based on given prompts 434 Moreover. LLMs play cru- cial role in data analysis. where they can filter large volumes of text data. summarize key points. and find patterns that would",
    "chunk_index": 347,
    "start_pos": 154483,
    "end_pos": 154979,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 539
    }
  },
  "1057": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_348",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "summarize key points. and find patterns that would take humans much longer to identify 435 Despite their wide- ranging applications. it is essential to remember that LLMs.similar to any AI system. are only as good as the data they have been trained on. Medicine. The application of LLMs in the field of medicine is reshaping healthcare delivery and research. For example. LLMs are increasingly used in clinical decision support systems to provide physicians with evidence-based treatment recommen-",
    "chunk_index": 348,
    "start_pos": 154980,
    "end_pos": 155429,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 497
    }
  },
  "1058": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_349",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "physicians with evidence-based treatment recommen- dations 436. 437. 438 By analyzing patient data and medical literature. they can help identify potential diagnoses. suggest appropriate tests. and recommend optimal treatment strategies. Moreover. LLMs can also enhance patient interactions with healthcare systems. e.g. they can be used in chatbot applica- tions 439. 440. 441 to answer patient queries about symptoms or medications. schedule appointments. and even provide es- sential health advice. For medical research. LLMs are used to",
    "chunk_index": 349,
    "start_pos": 155430,
    "end_pos": 155925,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 540
    }
  },
  "1059": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_350",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "lth advice. For medical research. LLMs are used to extract and filter information from considerable amount of medical literature. identify relevant studies. summarize find- ings. and even predict future research trends 442. 443. 444 For medical education. LLMs can help create training mate- rials. generate exam questions. provide detailed explanations of complex medical topics. and ffer personalized feedback to students 445. 446. 447. 448 They can also simulate patient interactions. enabling students to practice and improve their",
    "chunk_index": 350,
    "start_pos": 155926,
    "end_pos": 156420,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 535
    }
  },
  "1060": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_351",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "s. enabling students to practice and improve their clinical skills. At broader level. LLMs can assist in public health initiatives by analyzing media data to detect disease out- breaks. monitor public sentiment towards health policies. and disseminate health information in clear and understandable manner 449 LLMs can be employed to support public health initiatives. addressing related issues such as data privacy. the necessity for explainability. and the potential risk of propagat- ing biases 450. 451",
    "chunk_index": 351,
    "start_pos": 156421,
    "end_pos": 156886,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 506
    }
  },
  "1061": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_352",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "potential risk of propagat- ing biases 450. 451 Education. The integration of LLMs into the educational sec- tor offers opportunities to enhance learning experiences. teacher support. and educational content development. For students. by analyzing their learning styles. performance. and preferences. LLMs can provide customized study materials and practice questions to develop personalized learning experiences 452 For teachers. LLMs can help to create lesson plans and grade assignments and generate diverse and inclusive educational",
    "chunk_index": 352,
    "start_pos": 156887,
    "end_pos": 157378,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 536
    }
  },
  "1062": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_353",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nts and generate diverse and inclusive educational content. significantly saving more time for teaching and student interaction 453. 454 In language learning. LLMs serve as advanced conversational partners capable of simulating conver- sations in multiple languages. correcting grammar. enhancing vocabulary. and aiding pronunciation for the needs of fluency in practice 455 Furthermore. LLMs improve accessibility in education by providing support for students with disabili- ties. They can generate real-time transcriptions for the hear-",
    "chunk_index": 353,
    "start_pos": 157379,
    "end_pos": 157873,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 539
    }
  },
  "1063": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_354",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "an generate real-time transcriptions for the hear- ing impaired. ffer reading assistance for the visually impaired. and simplify complex texts for those with learning disabili- ties 451 As LLMs continue to evolve. their applications in education can benefit more students and teachers from di fferent perspectives in practice. Science. Similar to medical applications. LLMs can expedite the research process by quickly analyzing and summarizing sci- entific literature. By briefing comprehensible and accessible re-",
    "chunk_index": 354,
    "start_pos": 157874,
    "end_pos": 158343,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 515
    }
  },
  "1064": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_355",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ure. By briefing comprehensible and accessible re- search summaries. LLMs can assist researchers in staying up- to-date with the latest findings. even in fields outside their area of expertise 456. 457 In addition. LLMs can aid scientists 32",
    "chunk_index": 355,
    "start_pos": 158344,
    "end_pos": 158537,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 244,
      "word_count": 38,
      "optimized_for_embedding": true,
      "original_content_length": 244,
      "optimized_content_length": 241
    }
  },
  "1065": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_356",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "456. 457 In addition. LLMs can aid scientists 32 --- Page 33 --- Table 12. Performance comparison of top performing LLMs across various NLU and NLG tasks. Here. N-Shots indicate the number of example prompts provided to the model during the evaluation. representing its capability in few-shot or zero-shot learning settings. represents the fine-tuned version. and represents the benchmark. Task Dataset BenchmarkTop-1 Top-2 Top-3 Model Size Score N-shots Model Size Score N-shots Model Size Score N-shots",
    "chunk_index": 356,
    "start_pos": 158539,
    "end_pos": 159017,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 504
    }
  },
  "1066": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_357",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Size Score N-shots Model Size Score N-shots Multi-TaskBIG-bench Chinchilla 70B 65.1 5-shot Gopher 280B 53.97 5-shot PaLM 540B 53.7 5-shot MMLU GPT-4 86.4 5-shot Gemini Ultra 83.7 5-shot Flan-PaLM-2 Large 81.2 5-shot Language Understanding SuperGLUE ERNIE 3.0 12B 90.6 PaLM 540B 90.4 T5 11B 88.9 Story Comprehension and GenerationHellaSwag GPT-4 95.3 10-shot Gemini Ultra 87.8 10-shot PaLM-2 Large 86.8 one shot",
    "chunk_index": 357,
    "start_pos": 159018,
    "end_pos": 159460,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 410
    }
  },
  "1067": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_358",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tra 87.8 10-shot PaLM-2 Large 86.8 one shot StoryCloze GPT3 175B 87.7 few shot PaLM-2 Large 87.4 one shot OPT 175B 79.82 Physical Knowledge and World UnderstandingPIQA PaLM-2 Large 85.0 one shot LLaMa 65B 82.8 zero shot MT-NLG 530B 81.99 zero shot TriviaQA PaLM-2 Large 86.1 one shot LLaMA-2 70B 85.0 one shot PaLM 540B 81.4 one shot Contextual Language UnderstandingLAMBADA PaLM 540B 89.7 few shot MT-NLG 530B 87.15 few shot PaLM-2 Large 86.9 one shot",
    "chunk_index": 358,
    "start_pos": 159461,
    "end_pos": 159919,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 452
    }
  },
  "1068": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_359",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "87.15 few shot PaLM-2 Large 86.9 one shot Commonsense ReasoningWinoGrande GPT-4 87.5 5-shot PaLM-2 Large 83.0 one shot PaLM 540B 81.1 zero shot SIQA LLaMA 65B 52.3 zero shot Chinchilla 70B 51.3 zero shot Gopher 280B 50.6 zero shot Reading Comprehension BoolQ PaLM 540B 92.2 T5 11B 91.2 PaLM-2 Large 90.9 one shot Truthfulness Truthful-QA LLaMA 65B 57 Mathematical ReasoningMATH Gemini Ultra 53.2 4-shot PaLM-2 Large 34.3 4-shot LLaMa-2 65B 13.5 4-shot",
    "chunk_index": 359,
    "start_pos": 159920,
    "end_pos": 160392,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 451
    }
  },
  "1069": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_360",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Large 34.3 4-shot LLaMa-2 65B 13.5 4-shot GSM8K GPT-4 92.0 5-shot PaLM-2 Large 80.7 8-shot U-PaLM 540B 58.5 Problem Solving and Logical ReasoningHumanEval Gemini Ultra 74.4 zero shot GPT-4 67.0 zero shot Code Llama 34B 48.8 zero shot in formulating new hypotheses and research questions since their ability to process large-scale datasets allows them to un- veil insights that might not be immediately apparent to human researchers 458 Moreover. for scientific writing. LLMs can",
    "chunk_index": 360,
    "start_pos": 160393,
    "end_pos": 160865,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 478
    }
  },
  "1070": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_361",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "458 Moreover. for scientific writing. LLMs can help researchers draft documents. suggest improvements. and ensure adherence to specific formatting guidelines 459. 460 This not only saves time but also improves the clarity of scien- tific communication. enabling interdisciplinary teams to work together more ffectively. Maths. In addition to providing mathematical research and education support. LLMs can assist in solving mathematical problems by giving step-by-step explanations and guiding users",
    "chunk_index": 361,
    "start_pos": 160866,
    "end_pos": 161323,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 499
    }
  },
  "1071": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_362",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "giving step-by-step explanations and guiding users through complex proofs and calculations. They can help iden- tify errors in reasoning or computation and suggest corrections. serving as an invaluable tool for both learning and verification purposes 461. 462 LLMs can be employed to check the valid- ity of mathematical proofs. ffering preliminary filter before human review. While they are not substitute for the meticu- lous work of mathematicians. they can help simplify the process",
    "chunk_index": 362,
    "start_pos": 161324,
    "end_pos": 161768,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 486
    }
  },
  "1072": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_363",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "mathematicians. they can help simplify the process of proof verification 463. 464 Moreover. LLMs enhance ac- cessibility to mathematics by translating complex concepts and findings into understandable language for non-specialists 465 where the gap between theoretical mathematics and applied contexts such as physics. engineering. and economics can be bridged. Law. LLMs can assist with the thematic analysis of legal doc- uments. including generating initial coding for datasets. iden-",
    "chunk_index": 363,
    "start_pos": 161769,
    "end_pos": 162210,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 486
    }
  },
  "1073": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_364",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ding generating initial coding for datasets. iden- tifying themes. and classifying data according to these themes. This collaborative ffort between legal experts and LLMs has proved to be ffective in analyzing legal texts such as court opinions on theft. improving both the fficiency and quality of the research 466 Additionally. LLMs have been evaluated for their ability to generate explanations of legal terms. focusing on improving factual accuracy and relevance by incorporating",
    "chunk_index": 364,
    "start_pos": 162211,
    "end_pos": 162652,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 483
    }
  },
  "1074": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_365",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng factual accuracy and relevance by incorporating sentences from case law. By feeding relevant case law into the LLM. the augmented models can generate higher-quality expla- nations with less factually incorrect information 467 More- over. LLMs can be trained with specialized domain knowledgeto perform legal reasoning tasks 468 and answer legal ques- tions 469 Finance. LLMs like BloombergGPT 151 trained on exten- sive proprietary financial datasets. exhibit superior performance",
    "chunk_index": 365,
    "start_pos": 162653,
    "end_pos": 163096,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 483
    }
  },
  "1075": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_366",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "financial datasets. exhibit superior performance on financial tasks. This indicates the value of domain-specific training in creating LLMs that can more accurately understand and process industry-specific language and concepts. The intro- duction of FinGPT 470 as an open-source model ffers trans- parent and accessible resources to develop novel applications such as robo-advising. algorithmic trading. and low-code so- lutions. ultimately expanding the capabilities of financial ser- vices. Both BloombergGPT and FinGPT show the adaptabil-",
    "chunk_index": 366,
    "start_pos": 163097,
    "end_pos": 163593,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 541
    }
  },
  "1076": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_367",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Both BloombergGPT and FinGPT show the adaptabil- ity of LLMs to the financial domain. with the former showing the power of custom datasets and the latter emphasizing data- centric approach and low-rank adaptation techniques for cus- tomization. Moreover. LLMs demonstrate an ability to break down complex financial tasks into actionable plans. enabling end-to-end solutions that were previously unfeasible with sin- gle model 471 Robotics. In robotics research. LLMs have promising appli-",
    "chunk_index": 367,
    "start_pos": 163594,
    "end_pos": 164040,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 488
    }
  },
  "1077": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_368",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "In robotics research. LLMs have promising appli- cations. such as enhancing human-robot interaction 28. 472. 473. 474 task planning 237 motion planning 246 nav- igation 246. 475 object manipulation 236 personalized robots 476 etc. LLMs enable robots to understand the en- vironment ffectively and generate plans to complete tasks col- laboratively 240. 26 They can facilitate continuous learning by allowing robots to access and integrate information from wide range of sources. helping robots acquire new skills. adapt",
    "chunk_index": 368,
    "start_pos": 164041,
    "end_pos": 164536,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 519
    }
  },
  "1078": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_369",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "sources. helping robots acquire new skills. adapt to changes. and refine their paths 224. 233. 234 7. Challenges and Future Directions LLMs such as GPT-4 and its predecessors have significantly advanced natural language processing. Nevertheless. they also bring along set of challenges. The computational cost. ad- versarial robustness. and interpretability are among the tech- nical challenges that are intrinsic to these models. Further- more. as these models are scaled up to handle more complex 33",
    "chunk_index": 369,
    "start_pos": 164537,
    "end_pos": 164993,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 501
    }
  },
  "1079": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_370",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ese models are scaled up to handle more complex 33 --- Page 34 --- tasks or to operate in more complex or dynamic environments. new challenges in scalability. privacy. and real-time processing emerge. On the frontier of foundational research. integrating multi-modality and the ffectiveness of transfer learning are be- ing keenly explored. Additionally. the continuous learning as- pect of these models. which aims to have models that can adapt to new information over time. presents fresh set of challenges.",
    "chunk_index": 370,
    "start_pos": 164995,
    "end_pos": 165457,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 509
    }
  },
  "1080": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_371",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ion over time. presents fresh set of challenges. These challenges not only underscore the technical intricacies involved but also highlight the broader impact and the future trajectory of LLMs in real-world applications. The following sections delve into these challenges. shedding light on the on- going and potential fforts to address them. Computational Cost. Training LLMs require extensive compu- tational resources. which increases production costs and raises environmental concerns due to substantial energy consump-",
    "chunk_index": 371,
    "start_pos": 165458,
    "end_pos": 165934,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 523
    }
  },
  "1081": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_372",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "mental concerns due to substantial energy consump- tion during large-scale training. Improved performance occurs as computational resources increase. but the rate of improve- ment gradually decreases when both the model and dataset size remain fixed. following the power law of diminishing re- turns 477 Bias and Fairness. LLMs can inherit and amplify societal bi- ases in their training data. These biases can manifest in the model outputs. leading to potential ethical and fairness is- sues 478",
    "chunk_index": 372,
    "start_pos": 165935,
    "end_pos": 166388,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 496
    }
  },
  "1082": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_373",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "to potential ethical and fairness is- sues 478 Overfitting. Although LLMs possess substantial learning ca- pabilities. they are susceptible to overfitting noisy and peculiar patterns within their extensive training data. Consequently. this may cause them to generate illogical responses 479 The de- bate about Memorization vs. Generalization in LLMs is about finding the right balance. Memorization allows the model to remember specific details from its training data. ensuring it can",
    "chunk_index": 373,
    "start_pos": 166389,
    "end_pos": 166829,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 484
    }
  },
  "1083": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_374",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ic details from its training data. ensuring it can provide accurate answers to precise questions. However. gen- eralization enables the model to make inferences and produce responses for inputs it has not seen before. which is essential for handling various real-world tasks. Striking the right bal- ance is the challenge. too much memorization can lead to over- fitting. making the model inflexible and struggling with new inputs 480 Economic and Research Inequality. The high cost of train-",
    "chunk_index": 374,
    "start_pos": 166830,
    "end_pos": 167274,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 492
    }
  },
  "1084": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_375",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "and Research Inequality. The high cost of train- ing and deploying LLMs may make their development concen- trated within well-funded organizations. potentially worsening economic and research inequalities in AI 481 Reasoning and Planning. Some reasoning and planning tasks. even as seemingly simple as common-sense planning. which humans find easy. remain well beyond the current capabilities of LLMs evaluated using an assessment framework. This is not entirely unexpected. considering that LLMs primarily generate",
    "chunk_index": 375,
    "start_pos": 167275,
    "end_pos": 167744,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 515
    }
  },
  "1085": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_376",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "expected. considering that LLMs primarily generate text completions based on likelihood and ffer no solid guaran- tees in terms of reasoning abilities 482 Hallucinations. LLMs exhibit hallucinations where they generate responses that. while sounding plausible. are incorrect or do not align with the provided information 483 Hallucina- tions can be categorized into three categories. Input-conflicting hallucination. wherein LLMs produce content that diverges from the input given by users.",
    "chunk_index": 376,
    "start_pos": 167745,
    "end_pos": 168196,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 490
    }
  },
  "1086": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_377",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ntent that diverges from the input given by users. Context-conflicting hallucination. where LLMs generatecontent that contradicts information they have generated earlier. Fact-conflicting hallucination involves LLM generation of content that does not align with established world knowledge. Prompt Engineering. Prompts serve as inputs to LLMs. and their syntax and semantics play crucial role in determining the model output. The prompt variations. sometimes counter- intuitive to humans. can result in significant changes in model",
    "chunk_index": 377,
    "start_pos": 168197,
    "end_pos": 168685,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 531
    }
  },
  "1087": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_378",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "humans. can result in significant changes in model output and are addressed through prompt engineering. which involves designing natural language queries to guide LLMs responses ffectively 484. 32 Limited Knowledge. Information acquired during pretraining is limited and may become obsolete after some time. Re- training the model using updated data is costly. To generate factually accurate responses. people use retrieval augmen- tation pipeline 198 However. pre-trained models are not",
    "chunk_index": 378,
    "start_pos": 168686,
    "end_pos": 169132,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 487
    }
  },
  "1088": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_379",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ipeline 198 However. pre-trained models are not trained with retrieval augmentation generation RAG 6. 21 hence. adapting the training pipeline is necessary 193. 25 Safety and Controllability. Using LLMs comes with the risk of generating harmful. misleading. or inappropriate content. whether by accident or when given specific prompts. Ensuring these models are safely utilized is significant concern 485 Security and Privacy. LLMs are prone to leaking personal information and generating false. unethical. misaligned re-",
    "chunk_index": 379,
    "start_pos": 169133,
    "end_pos": 169619,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 521
    }
  },
  "1089": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_380",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "on and generating false. unethical. misaligned re- sponses. Researchers have explored various security attacks. i.e. backdoor attacks. jailbreaking. prompt injection. and data poisoning. that lead to breaking LLMs security. Therefore. developing better defense mechanisms is essential to ensure LLMs are safe. reliable. and trustworthy for complex AI applications 486 Multi-Modality. Multi-modal learning. where LLMs are trained on diverse data like text. images. and videos. aims to create models with richer understanding but faces challenges",
    "chunk_index": 380,
    "start_pos": 169620,
    "end_pos": 170117,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 544
    }
  },
  "1090": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_381",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "els with richer understanding but faces challenges in data alignment. fusion strategies. and higher computational demands. Catastrophic Forgetting. LLMs are often pre-trained on large datasets and then fine-tuned on domain-specific data. reducing training resources. However. they face issues like domain adaptation and catastrophic forgetting. which hinder the retention of original knowledge when learning new tasks. Adversarial Robustness. Large Language Models LLMs have shown great capabilities in various tasks but are vul-",
    "chunk_index": 381,
    "start_pos": 170118,
    "end_pos": 170598,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 529
    }
  },
  "1091": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_382",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "great capabilities in various tasks but are vul- nerable to adversarial attacks. where slight. deliberate input alterations can mislead them. Especially with models like BERT. adversarial fine-tuning can enhance robustness. al- though it sometimes compromises generalization 487 As LLMs integrate more into complex systems. examining their security properties becomes crucial. given the emerging field of adversarial attacks on LLMs within trustworthy ML 488 This vulnerability is notable in safety-critical domains. ne-",
    "chunk_index": 382,
    "start_pos": 170599,
    "end_pos": 171076,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 520
    }
  },
  "1092": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_383",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ability is notable in safety-critical domains. ne- cessitating robust adversarial evaluation tools to ensure LLM reliability 489 Interpretability and Explainability. The black-box nature of LLMs poses challenges in understanding their decision- making. which is crucial for broader acceptance and trust. 34",
    "chunk_index": 383,
    "start_pos": 171077,
    "end_pos": 171337,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 311,
      "word_count": 41,
      "optimized_for_embedding": true,
      "original_content_length": 311,
      "optimized_content_length": 306
    }
  },
  "1093": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_384",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ch is crucial for broader acceptance and trust. 34 --- Page 35 --- especially in sensitive domains. Despite their advanced capabilities. the lack of insight into their operation limits their effectiveness and trustworthiness 490. 491 fforts are being made to make LLMs more explainable to promote user trust and to ensure responsible AI usage. Understanding the logic behind LLMs responses is essential for fostering trust and ensuring they align with human values and legal standards. Privacy Concerns. Privacy concerns in Large Language",
    "chunk_index": 384,
    "start_pos": 171339,
    "end_pos": 171832,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 538
    }
  },
  "1094": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_385",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ivacy Concerns. Privacy concerns in Large Language Models LLMs have escalated with their growth in complexity and size. particularly around data sharing and potential misuse. There is risk of malicious content creation. filter bypass. and data privacy issues. especially in e-commerce. where protecting customer privacy is crucial. If models are trained on private data. additional concerns arise if such models are made publicly available. LLMs tend to memorize phrases from their training sets. which an adversary could exploit to extract",
    "chunk_index": 385,
    "start_pos": 171833,
    "end_pos": 172326,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 540
    }
  },
  "1095": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_386",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "sets. which an adversary could exploit to extract sensitive data. posing threat to personal privacy 492. 493 Real-Time Processing. Real-time processing in Large Lan- guage Models LLMs is pivotal for various applications. especially with the rising popularity of mobile AI applications and concerns regarding information security and privacy. However. LLMs often have hundreds of layers and millions of parameters. which impede real-time processing due to the high computational demands and limited weight storage on",
    "chunk_index": 386,
    "start_pos": 172327,
    "end_pos": 172799,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 515
    }
  },
  "1096": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_387",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "omputational demands and limited weight storage on hardware platforms. particularly in edge computing environ- ments 494 While certain fforts like MobileBERT aim to reduce memory requirements. they still face substantial execution overhead due to the large number of model layers. leading to high inference latency. Long-Term Dependencies. Large Language Models have shown considerable progress in understanding and generating text. yet they often struggle with preserving context and handling long-term dependencies. particularly in complex.",
    "chunk_index": 387,
    "start_pos": 172800,
    "end_pos": 173296,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 542
    }
  },
  "1097": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_388",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "long-term dependencies. particularly in complex. multi-turn conversations or long documents. This limitation can lead to incoherent or irrelevant responses. Hardware Acceleration. The growth of LLMs presents signif- icant hardware challenges due to the increasing computational and memory demands associated with training and deploying these models. GPUs have played crucial role in meeting the hardware requirements for training LLMs. with the networking industry also evolving to optimize hardware for training",
    "chunk_index": 388,
    "start_pos": 173297,
    "end_pos": 173762,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 512
    }
  },
  "1098": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_389",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ry also evolving to optimize hardware for training workloads. However. the growing size of LLMs. which has been outpacing hardware progress. makes model inference in- creasingly costly. Model quantization is promising approach to bridge the widening gap between LLM size and hardware capacity 495 Although specialized hardware acceleration like GPUs or TPUs can significantly reduce the computational cost. making real-time applications more feasible. they may not fully resolve all limitations. necessitating further advancements",
    "chunk_index": 389,
    "start_pos": 173763,
    "end_pos": 174247,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 530
    }
  },
  "1099": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_390",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ll limitations. necessitating further advancements in hardware technology. Regulatory and Ethical Frameworks. The rapid advancements in artificial intelligence have given rise to sophisticated Large Language Models LLMs like OpenAI GPT-4 157 and Google Bard. These developments underscore the imperative for regulatory oversight to manage the ethical and social challenges accompanying LLMs widespread use 496 For instance. LLMs can generate content that can be used posi-tively or negatively. emphasizing the need for proactive ethical",
    "chunk_index": 390,
    "start_pos": 174248,
    "end_pos": 174745,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 536
    }
  },
  "1100": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_391",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tively. emphasizing the need for proactive ethical frameworks and policy measures to guide their responsible use and assign accountability for their outputs 497 Auditing is identified as promising governance mechanism to ensure that AI systems. including LLMs. are designed and deployed ethically. legally. and technically robust 498 8. Conclusion This article has comprehensively reviewed the develop- ments in LLMs. It contributes to summarizing significant findings of LLMs in the existing literature and provides",
    "chunk_index": 391,
    "start_pos": 174746,
    "end_pos": 175221,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 516
    }
  },
  "1101": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_392",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "of LLMs in the existing literature and provides detailed analysis of the design aspects. including architec- tures. datasets. and training pipelines. We identified crucial architectural components and training strategies employed by different LLMs. These aspects are presented as summaries and discussions throughout the article. Moreover. we have discussed the performance di fferences of LLMs in zero-shot and few-shot settings. explored the impact of fine-tuning. and compared supervised and generalized models and encoder vs.",
    "chunk_index": 392,
    "start_pos": 175222,
    "end_pos": 175703,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 529
    }
  },
  "1102": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_393",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "supervised and generalized models and encoder vs. decoder vs. encoder-decoder architectures. comprehensive review of multi-modal LLMs. retrieval augmented LLMs. LLMs-powered agents. fficient LLMs. datasets. evaluation. applications. and challenges is also provided. This article is anticipated to serve as valuable resource for researchers. offering insights into the recent advancements in LLMs and providing fundamental concepts and details to develop better LLMs. Acknowledgement. The author would like to acknowl-",
    "chunk_index": 393,
    "start_pos": 175704,
    "end_pos": 176180,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 517
    }
  },
  "1103": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_394",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nowledgement. The author would like to acknowl- edge the support received from Saudi Data and AI Authority SDAIA and King Fahd University of Petroleum and Miner- als KFUPM under SDAIA-KFUPM Joint Research Center for Artificial Intelligence Grant No. JRC-AI-RFP-11. References A. Chernyavskiy. D. Ilvovsky. P. Nakov. Transformers. the end of his- tory for natural language processing. in. Machine Learning and Knowledge Discovery in Databases. Research Track. European Con- ference. ECML PKDD 2021. Bilbao. Spain. September 13 17. 2021.",
    "chunk_index": 394,
    "start_pos": 176181,
    "end_pos": 176678,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 535
    }
  },
  "1104": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_395",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "PKDD 2021. Bilbao. Spain. September 13 17. 2021. Proceedings. Part III 21. Springer. 2021. pp. 677 693. A. Wang. Pruksachatkun. N. Nangia. A. Singh. J. Michael. F. Hill. O. Levy. S. Bowman. Superglue. stickier benchmark for general- purpose language understanding systems. Advances in neural informa- tion processing systems 32 2019 1. 26. 29 D. Adiwardana. M.-T. Luong. D. R. So. J. Hall. N. Fiedel. R. Thoppilan. Z. Yang. A. Kulshreshtha. G. Nemade. Lu. et al. Towards human-",
    "chunk_index": 395,
    "start_pos": 176679,
    "end_pos": 177133,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 477
    }
  },
  "1105": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_396",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "shtha. G. Nemade. Lu. et al. Towards human- like open-domain chatbot. arXiv preprint arXiv.2001.09977 2020 B. A. Arcas. Do large language models understand us. Daedalus 151 2022 183 197. A. Radford. J. Wu. R. Child. D. Luan. D. Amodei. I. Sutskever. et al. Language models are unsupervised multitask learners. OpenAI blog 2019 9. 2. T. Brown. B. Mann. N. Ryder. M. Subbiah. J. D. Kaplan. P. Dhariwal. A. Neelakantan. P. Shyam. G. Sastry. A. Askell. et al. Language models",
    "chunk_index": 396,
    "start_pos": 177134,
    "end_pos": 177601,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 471
    }
  },
  "1106": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_397",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "yam. G. Sastry. A. Askell. et al. Language models are few-shot learners. Advances in neural information processing sys- tems 33 2020 1877 1901. 2. 6. 7. 8. 9. 16. 18. 23. 24. 25. 34 J. Devlin. M.-W. Chang. K. Lee. K. Toutanova. Bert. Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv.1810.04805 2018 2. 18. 24 35",
    "chunk_index": 397,
    "start_pos": 177602,
    "end_pos": 177920,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 369,
      "word_count": 59,
      "optimized_for_embedding": true,
      "original_content_length": 369,
      "optimized_content_length": 359
    }
  },
  "1107": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_398",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Xiv preprint arXiv.1810.04805 2018 2. 18. 24 35 --- Page 36 --- M. E. Peters. M. Neumann. M. Iyyer. M. Gardner. C. Clark. K. Lee. L. Zettlemoyer. Deep contextualized word representations. in. NAACL- HLT. Association for Computational Linguistics. 2018. pp. 2227 2237. M. Lewis. Liu. N. Goyal. M. Ghazvininejad. A. Mohamed. O. Levy. Stoyanov. L. Zettlemoyer. Bart. Denoising sequence-to-sequence pre- training for natural language generation. translation. and comprehen- sion. arXiv preprint arXiv.1910.13461 2019",
    "chunk_index": 398,
    "start_pos": 177922,
    "end_pos": 178409,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 512
    }
  },
  "1108": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_399",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "n- sion. arXiv preprint arXiv.1910.13461 2019 10 C. Ra ffel. N. Shazeer. A. Roberts. K. Lee. S. Narang. M. Matena. Zhou. W. Li. P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Re- search 21 2020 5485 5551. 2. 7. 8. 18. 19. 24. 25. 28. 30. 31 11 L. Xue. N. Constant. A. Roberts. M. Kale. R. Al-Rfou. A. Siddhant. A. Barua. C. Ra ffel. mt5. massively multilingual pre-trained text-to-",
    "chunk_index": 399,
    "start_pos": 178410,
    "end_pos": 178840,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 481,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 458
    }
  },
  "1109": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_400",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "mt5. massively multilingual pre-trained text-to- text transformer. arXiv preprint arXiv.2010.11934 2020 2. 7. 8. 24. 25. 28. 30 12 Z. Zhang. Gu. X. Han. S. Chen. C. Xiao. Z. Sun. Yao. F. Qi. J. Guan. P. Ke. et al. Cpm-2. Large-scale cost-e ffective pre-trained lan- guage models. AI Open 2021 216 224. 2. 8. 25 13 T. L. Scao. A. Fan. C. Akiki. E. Pavlick. S. Ili c. D. Hesslow. R. Castagne A. S. Luccioni. F. Yvon. M. Galle et al. Bloom. 176b- parameter open-access multilingual language model. arXiv preprint",
    "chunk_index": 400,
    "start_pos": 178841,
    "end_pos": 179327,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 96,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 509
    }
  },
  "1110": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_401",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "access multilingual language model. arXiv preprint arXiv.2211.05100 2022 2. 4. 9. 11. 23. 24. 25. 30 14 S. Zhang. S. Roller. N. Goyal. M. Artetxe. M. Chen. S. Chen. C. Dewan. M. Diab. X. Li. X. Lin. et al. Opt. Open pre-trained transformer language models. arXiv preprint arXiv.2205.01068 2022 2. 9. 11. 24. 25 15 A. Chowdhery. S. Narang. J. Devlin. M. Bosma. G. Mishra. A. Roberts. P. Barham. H. W. Chung. C. Sutton. S. Gehrmann. et al. Palm. Scal- ing language modeling with pathways. arXiv preprint arXiv.2204.02311",
    "chunk_index": 401,
    "start_pos": 179328,
    "end_pos": 179811,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 518
    }
  },
  "1111": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_402",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ing with pathways. arXiv preprint arXiv.2204.02311 2022 2. 6. 9. 11. 23. 24. 25 16 H. W. Chung. L. Hou. S. Longpre. B. Zoph. Tay. W. Fedus. E. Li. X. Wang. M. Dehghani. S. Brahma. et al. Scaling instruction-finetuned language models. arXiv preprint arXiv.2210.11416 2022 2. 7. 11. 16. 17. 22. 24. 25. 28. 31 17 Sanh. A. Webson. C. Ra ffel. S. H. Bach. L. Sutawika. Z. Alyafeai. A. Cha ffin. A. Stiegler. T. L. Scao. A. Raja. et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint",
    "chunk_index": 402,
    "start_pos": 179812,
    "end_pos": 180294,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 513
    }
  },
  "1112": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_403",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "bles zero-shot task generalization. arXiv preprint arXiv.2110.08207 2021 2. 11. 16. 25. 28. 31 18 Wang. S. Mishra. P. Alipoormolabashi. Kordi. A. Mirzaei. A. Naik. A. Ashok. A. S. Dhanasekaran. A. Arunkumar. D. Stap. et al. Super-naturalinstructions. Generalization via declarative instructions on 1600 nlp tasks. in. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022. pp. 5085 5109. 2. 7. 11. 16. 17. 24. 25. 28. 31 19 Wang. Kordi. S. Mishra. A. Liu. N. A. Smith. D. Khashabi. H. Ha-",
    "chunk_index": 403,
    "start_pos": 180295,
    "end_pos": 180795,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 527
    }
  },
  "1113": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_404",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Mishra. A. Liu. N. A. Smith. D. Khashabi. H. Ha- jishirzi. Self-instruct. Aligning language model with self generated in- structions. arXiv preprint arXiv.2212.10560 2022 2. 16. 19. 22. 28 20 L. Ouyang. J. Wu. X. Jiang. D. Almeida. C. Wainwright. P. Mishkin. C. Zhang. S. Agarwal. K. Slama. A. Ray. et al. Training language mod- els to follow instructions with human feedback. Advances in Neural In- formation Processing Systems 35 2022 27730 27744. 2. 7. 11. 16. 22 21 H. Touvron. L. Martin. K. Stone. P. Albert. A. Almahairi. Babaei.",
    "chunk_index": 404,
    "start_pos": 180796,
    "end_pos": 181296,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 535
    }
  },
  "1114": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_405",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "in. K. Stone. P. Albert. A. Almahairi. Babaei. N. Bashlykov. S. Batra. P. Bhargava. S. Bhosale. et al. Llama 2. Open foundation and fine-tuned chat models. arXiv preprint arXiv.2307.09288 2023 2. 7. 10. 16. 25. 34 22 J. Wei. Tay. R. Bommasani. C. Ra ffel. B. Zoph. S. Borgeaud. D. Yo- gatama. M. Bosma. D. Zhou. D. Metzler. et al. Emergent abilities of large language models. arXiv preprint arXiv.2206.07682 2022 23 T. Webb. K. J. Holyoak. H. Lu. Emergent analogical reasoning in large",
    "chunk_index": 405,
    "start_pos": 181297,
    "end_pos": 181753,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 485
    }
  },
  "1115": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_406",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "oak. H. Lu. Emergent analogical reasoning in large language models. Nature Human Behaviour 2023 1526 1541. 24 D. A. Boiko. R. MacKnight. G. Gomes. Emergent autonomous sci- entific research capabilities of large language models. arXiv preprint arXiv.2304.05332 2023 25 G. Izacard. P. Lewis. M. Lomeli. L. Hosseini. F. Petroni. T. Schick. J. Dwivedi-Yu. A. Joulin. S. Riedel. E. Grave. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv.2208.03299 2022 2. 18. 19. 34",
    "chunk_index": 406,
    "start_pos": 181754,
    "end_pos": 182219,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 494
    }
  },
  "1116": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_407",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "iv preprint arXiv.2208.03299 2022 2. 18. 19. 34 26 D. Driess. F. Xia. M. S. Sajjadi. C. Lynch. A. Chowdhery. B. Ichter. A. Wahid. J. Tompson. Q. Vuong. T. Yu. et al. Palm-e. An embodiedmultimodal language model. arXiv preprint arXiv.2303.03378 2023 2. 20. 22. 33 27 A. Parisi. Zhao. N. Fiedel. Talm. Tool augmented language models. arXiv preprint arXiv.2205.12255 2022 2. 19. 20 28 B. Zhang. H. Soh. Large language models as zero-shot human models for human-robot interaction. arXiv preprint arXiv.2303.03548 2023 2. 33",
    "chunk_index": 407,
    "start_pos": 182220,
    "end_pos": 182711,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 519
    }
  },
  "1117": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_408",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ion. arXiv preprint arXiv.2303.03548 2023 2. 33 29 Q. Ye. H. Xu. G. Xu. J. Ye. M. Yan. Zhou. J. Wang. A. Hu. P. Shi. Shi. et al. mplug-owl. Modularization empowers large language models with multimodality. arXiv preprint arXiv.2304.14178 2023 2. 22 30 W. Wang. Z. Chen. X. Chen. J. Wu. X. Zhu. G. Zeng. P. Luo. T. Lu. J. Zhou. Qiao. et al. Visionllm. Large language model is also an open-ended decoder for vision-centric tasks. arXiv preprint arXiv.2305.11175 2023 2. 22",
    "chunk_index": 408,
    "start_pos": 182712,
    "end_pos": 183158,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 470
    }
  },
  "1118": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_409",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "sks. arXiv preprint arXiv.2305.11175 2023 2. 22 31 R. Yang. L. Song. Li. S. Zhao. Ge. X. Li. Shan. Gpt4tools. Teaching large language model to use tools via self-instruction. arXiv preprint arXiv.2305.18752 2023 2. 19. 22. 23 32 E. Saravia. Prompt Engineering Guide. https. github.com dair- ai Prompt-Engineering-Guide 12 2022 2. 7. 18. 34 33 A. Zeng. X. Liu. Z. Du. Z. Wang. H. Lai. M. Ding. Z. Yang. Xu. W. Zheng. X. Xia. et al. Glm-130b. An open bilingual pre-trained",
    "chunk_index": 409,
    "start_pos": 183159,
    "end_pos": 183613,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 505,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 470
    }
  },
  "1119": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_410",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "a. et al. Glm-130b. An open bilingual pre-trained model. arXiv preprint arXiv.2210.02414 2022 2. 10. 23. 24. 25 34 Wang. H. Le. A. D. Gotmare. N. D. Bui. J. Li. S. C. Hoi. Codet5 Open code large language models for code understanding and genera- tion. arXiv preprint arXiv.2305.07922 2023 2. 11. 24. 25 35 S. Wang. Sun. Xiang. Z. Wu. S. Ding. W. Gong. S. Feng. J. Shang. Zhao. C. Pang. et al. Ernie 3.0 titan. Exploring larger-scale knowl- edge enhanced pre-training for language understanding and generation.",
    "chunk_index": 410,
    "start_pos": 183614,
    "end_pos": 184103,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 96,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 509
    }
  },
  "1120": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_411",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "raining for language understanding and generation. arXiv preprint arXiv.2112.12731 2021 2. 8. 24. 25 36 J. Rasley. S. Rajbhandari. O. Ruwase. He. Deepspeed. System op- timizations enable training deep learning models with over 100 billion parameters. in. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery Data Mining. 2020. pp. 3505 3506. 2. 37 S. Rajbhandari. J. Rasley. O. Ruwase. He. Zero. Memory optimiza- tions toward training trillion parameter models. in. SC20. International",
    "chunk_index": 411,
    "start_pos": 184104,
    "end_pos": 184588,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 515
    }
  },
  "1121": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_412",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "trillion parameter models. in. SC20. International Conference for High Performance Computing. Networking. Storage and Analysis. IEEE. 2020. pp. 16. 2. 4. 24 38 J. He. C. Zhou. X. Ma. T. Berg-Kirkpatrick. G. Neubig. Towards unified view of parameter-e fficient transfer learning. arXiv preprint arXiv.2110.04366 2021 2. 20. 21 39 Z. Hu. Lan. L. Wang. W. Xu. E.-P. Lim. R. K.-W. Lee. L. Bing. S. Po- ria. Llm-adapters. An adapter family for parameter-e fficient fine-tuning",
    "chunk_index": 412,
    "start_pos": 184589,
    "end_pos": 185024,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 486,
      "optimized_content_length": 471
    }
  },
  "1122": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_413",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "dapter family for parameter-e fficient fine-tuning of large language models. arXiv preprint arXiv.2304.01933 2023 2. 20 40 B. Lester. R. Al-Rfou. N. Constant. The power of scale for parameter- efficient prompt tuning. arXiv preprint arXiv.2104.08691 2021 2. 8. 20. 21 41 X. L. Li. P. Liang. Prefix-tuning. Optimizing continuous prompts for generation. arXiv preprint arXiv.2101.00190 2021 2. 20. 21 42 X. Ma. G. Fang. X. Wang. Llm-pruner. On the structural pruning of large language models. arXiv preprint arXiv.2305.11627 2023 2. 22",
    "chunk_index": 413,
    "start_pos": 185025,
    "end_pos": 185525,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 533
    }
  },
  "1123": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_414",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "els. arXiv preprint arXiv.2305.11627 2023 2. 22 43 R. Xu. F. Luo. C. Wang. B. Chang. J. Huang. S. Huang. F. Huang. From dense to sparse. Contrastive pruning for better pre-trained lan- guage model compression. in. Proceedings of the AAAI Conference on Artificial Intelligence. ol. 36. 2022. pp. 11547 11555. 2. 22 44 G. Xiao. J. Lin. M. Seznec. H. Wu. J. Demouth. S. Han. Smoothquant. Accurate and fficient post-training quantization for large language models. in. ICML. ol. 202 of Proceedings of Machine Learning Re-",
    "chunk_index": 414,
    "start_pos": 185526,
    "end_pos": 186005,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 517
    }
  },
  "1124": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_415",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ol. 202 of Proceedings of Machine Learning Re- search. PMLR. 2023. pp. 38087 38099. 2. 21 45 C. Tao. L. Hou. W. Zhang. L. Shang. X. Jiang. Q. Liu. P. Luo. N. Wong. Compression of generative pre-trained language models via quantiza- tion. arXiv preprint arXiv.2203.10705 2022 2. 21 46 A. Pal. D. Karkhanis. M. Roberts. S. Dooley. A. Sundararajan. S. Naidu. Giraffe. Adventures in expanding context lengths in llms. arXiv preprint arXiv.2308.10882 2023 2. 17 47 B. Peng. J. Quesnelle. H. Fan. E. Shippole. Yarn. fficient con-",
    "chunk_index": 415,
    "start_pos": 186006,
    "end_pos": 186496,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 523
    }
  },
  "1125": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_416",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "snelle. H. Fan. E. Shippole. Yarn. fficient con- text window extension of large language models. arXiv preprint arXiv.2309.00071 2023 2. 17 48 M. Guo. J. Ainslie. D. Uthus. S. Ontanon. J. Ni. .-H. Sung. Yang. 36",
    "chunk_index": 416,
    "start_pos": 186497,
    "end_pos": 186670,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 224,
      "word_count": 40,
      "optimized_for_embedding": true,
      "original_content_length": 224,
      "optimized_content_length": 211
    }
  },
  "1126": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_417",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "thus. S. Ontanon. J. Ni. .-H. Sung. Yang. 36 --- Page 37 --- Longt5. fficient text-to-text transformer for long sequences. arXiv preprint arXiv.2112.07916 2021 2. 18 49 S. Chen. S. Wong. L. Chen. Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv.2306.15595 2023 2. 17 50 W. X. Zhao. K. Zhou. J. Li. T. Tang. X. Wang. Hou. Min. B. Zhang. J. Zhang. Z. Dong. et al. survey of large language models. arXiv preprint arXiv.2303.18223 2023 2. 3.",
    "chunk_index": 417,
    "start_pos": 186672,
    "end_pos": 187154,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 95,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 495
    }
  },
  "1127": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_418",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "s. arXiv preprint arXiv.2303.18223 2023 2. 3. 51 U. Naseem. I. Razzak. S. K. Khan. M. Prasad. comprehensive sur- vey on word representation models. From classical to state-of-the-art word representation language models. Transactions on Asian and Low- Resource Language Information Processing 20 2021 35. 2. 52 B. Min. H. Ross. E. Sulem. A. P. B. Veyseh. T. H. Nguyen. O. Sainz. E. Agirre. I. Heinz. D. Roth. Recent advances in natural language pro- cessing via large pre-trained language models. survey. arXiv preprint",
    "chunk_index": 418,
    "start_pos": 187155,
    "end_pos": 187645,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 518
    }
  },
  "1128": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_419",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "-trained language models. survey. arXiv preprint arXiv.2111.01243 2021 2. 53 C. Zhou. Q. Li. C. Li. J. Yu. Liu. G. Wang. K. Zhang. C. Ji. Q. Yan. L. He. et al. comprehensive survey on pretrained foundation models. history from bert to chatgpt. arXiv preprint arXiv.2302.09419 2023 2. 54 Q. Dong. L. Li. D. Dai. C. Zheng. Z. Wu. B. Chang. X. Sun. J. Xu. Z. Sui. survey for in-context learning. arXiv preprint arXiv.2301.00234 2022 2. 7. 18 55 J. Huang. K. C.-C. Chang. Towards reasoning in large language models.",
    "chunk_index": 419,
    "start_pos": 187646,
    "end_pos": 188138,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 97,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 511
    }
  },
  "1129": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_420",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Chang. Towards reasoning in large language models. survey. arXiv preprint arXiv.2212.10403 2022 2. 7. 18 56 Wang. W. Zhong. L. Li. F. Mi. X. Zeng. W. Huang. L. Shang. X. Jiang. Q. Liu. Aligning large language models with human. survey. arXiv preprint arXiv.2307.12966 2023 57 X. Zhu. J. Li. Liu. C. Ma. W. Wang. survey on model compression for large language models. arXiv preprint arXiv.2308.07633 2023 58 S. Yin. C. Fu. S. Zhao. K. Li. X. Sun. T. Xu. E. Chen. survey on multi-",
    "chunk_index": 420,
    "start_pos": 188139,
    "end_pos": 188601,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 478
    }
  },
  "1130": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_421",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "K. Li. X. Sun. T. Xu. E. Chen. survey on multi- modal large language models. arXiv preprint arXiv.2306.13549 2023 2. 22. 23 59 J. J. Webster. C. Kit. Tokenization as the initial phase in nlp. in. COL- ING 1992 volume 4. The 14th international conference on computa- tional linguistics. 1992. 60 T. Kudo. Subword regularization. Improving neural network translation models with multiple subword candidates. in. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ol-",
    "chunk_index": 421,
    "start_pos": 188602,
    "end_pos": 189067,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 501
    }
  },
  "1131": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_422",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Association for Computational Linguistics ol- ume 1. Long Papers 2018. pp. 66 75. 61 R. Sennrich. B. Haddow. A. Birch. Neural machine translation of rare words with subword units. in. Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics olume 1. Long Papers 2016. pp. 1715 1725. 62 M. Schuster. K. Nakajima. Japanese and korean voice search. in. 2012 IEEE international conference on acoustics. speech and signal process- ing ICASSP IEEE. 2012. pp. 5149 5152.",
    "chunk_index": 422,
    "start_pos": 189068,
    "end_pos": 189538,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 496
    }
  },
  "1132": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_423",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rocess- ing ICASSP IEEE. 2012. pp. 5149 5152. 63 S. J. Mielke. Z. Alyafeai. E. Salesky. C. Ra ffel. M. Dey. M. Galle A. Raja. C. Si. W. Lee. B. Sagot. et al. Between words and char- acters. brief history of open-vocabulary modeling and tokenization in nlp. arXiv preprint arXiv.2112.10508 2021 64 A. Vaswani. N. Shazeer. N. Parmar. J. Uszkoreit. L. Jones. A. N. Gomez. \u0141. Kaiser. I. Polosukhin. Attention is all you need. Advances in neural information processing systems 30 2017 4.",
    "chunk_index": 423,
    "start_pos": 189539,
    "end_pos": 189997,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 482
    }
  },
  "1133": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_424",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ral information processing systems 30 2017 4. 65 O. Press. N. Smith. M. Lewis. Train short. test long. Attention with linear biases enables input length extrapolation. in. International Con- ference on Learning Representations. 2022. URL https. openreview.net forum.id R8sQPpGCv0 4. 17 66 J. Su. Lu. S. Pan. A. Murtadha. B. Wen. Liu. Roformer. En- hanced transformer with rotary position embedding. arXiv preprint arXiv.2104.09864 2021 4. 9. 17 67 R. Child. S. Gray. A. Radford. I. Sutskever. Generating long sequences",
    "chunk_index": 424,
    "start_pos": 189998,
    "end_pos": 190488,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 518
    }
  },
  "1134": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_425",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Radford. I. Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv.1904.10509 2019 4. 7. 23 68 T. Dao. D. Fu. S. Ermon. A. Rudra. C. Re Flashattention. Fast and memory-e fficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 2022 16344 16359. 69 K. Hornik. M. Stinchcombe. H. White. Multilayer feedforward networks are universal approximators. Neural networks 1989 359 366. 70 Nair. G. E. Hinton. Rectified linear units improve restricted boltz-",
    "chunk_index": 425,
    "start_pos": 190489,
    "end_pos": 190984,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 516
    }
  },
  "1135": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_426",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Rectified linear units improve restricted boltz- mann machines. in. Proceedings of the 27th international conference onmachine learning ICML-10 2010. pp. 807 814. 71 D. Hendrycks. K. Gimpel. Gaussian error linear units gelus arXiv preprint arXiv.1606.08415 2016 72 N. Srivastava. G. Hinton. A. Krizhevsky. I. Sutskever. R. Salakhutdinov. Dropout. simple way to prevent neural networks from overfitting. The journal of machine learning research 15 2014 1929 1958.",
    "chunk_index": 426,
    "start_pos": 190985,
    "end_pos": 191425,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 462
    }
  },
  "1136": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_427",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "chine learning research 15 2014 1929 1958. 73 D. Krueger. T. Maharaj. J. Krama r. M. Pezeshki. N. Ballas. N. R. Ke. A. Goyal. Bengio. A. Courville. C. Pal. Zoneout. Regular- izing rnns by randomly preserving hidden activations. arXiv preprint arXiv.1606.01305 2016 74 N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv.2002.05202 2020 75 N. Dauphin. A. Fan. M. Auli. D. Grangier. Language modeling with gated convolutional networks. in. International conference on machine",
    "chunk_index": 427,
    "start_pos": 191426,
    "end_pos": 191894,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 488
    }
  },
  "1137": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_428",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "networks. in. International conference on machine learning. PMLR. 2017. pp. 933 941. 76 J. L. Ba. J. R. Kiros. G. E. Hinton. Layer normalization. arXiv preprint arXiv.1607.06450 2016 77 B. Zhang. R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems 32 2019 78 A. Baevski. M. Auli. Adaptive input representations for neural language modeling. arXiv preprint arXiv.1809.10853 2018 79 H. Wang. S. Ma. L. Dong. S. Huang. D. Zhang. F. Wei. Deepnet. Scaling",
    "chunk_index": 428,
    "start_pos": 191895,
    "end_pos": 192369,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 499
    }
  },
  "1138": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_429",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Dong. S. Huang. D. Zhang. F. Wei. Deepnet. Scaling transformers to 1.000 layers. arXiv preprint arXiv.2203.00555 2022 80 M. Shoeybi. M. Patwary. R. Puri. P. LeGresley. J. Casper. B. Catanzaro. Megatron-lm. Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv.1909.08053 2019 4. 81 bmtrain. fficient training for big models. URL https. github.com OpenBMB BMTrain 4. 82 T. Wolf. L. Debut. Sanh. J. Chaumond. C. Delangue. A. Moi. P. Cis-",
    "chunk_index": 429,
    "start_pos": 192370,
    "end_pos": 192823,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 476
    }
  },
  "1139": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_430",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Sanh. J. Chaumond. C. Delangue. A. Moi. P. Cis- tac. T. Rault. R. Louf. M. Funtowicz. et al. Transformers. State-of-the- art natural language processing. in. Proceedings of the 2020 conference on empirical methods in natural language processing. system demon- strations. 2020. pp. 38 45. 83 J. Bradbury. R. Frostig. P. Hawkins. M. J. Johnson. C. Leary. D. Maclau- rin. G. Necula. A. Paszke. J. VanderPlas. S. Wanderman-Milne. et al. Jax. composable transformations of python numpy programs 2018",
    "chunk_index": 430,
    "start_pos": 192824,
    "end_pos": 193282,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 494
    }
  },
  "1140": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_431",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ransformations of python numpy programs 2018 84 S. Li. J. Fang. Z. Bian. H. Liu. Liu. H. Huang. B. Wang. You. Colossal-ai. unified deep learning system for large-scale parallel train- ing. arXiv preprint arXiv.2110.14883 2021 85 J. He. J. Qiu. A. Zeng. Z. Yang. J. Zhai. J. Tang. Fastmoe. fast mixture-of-expert training system. arXiv preprint arXiv.2103.13262 2021 86 L. Huawei Technologies Co. Huawei mindspore ai development frame- work. in. Artificial Intelligence Technology. Springer. 2022. pp. 137 162.",
    "chunk_index": 431,
    "start_pos": 193283,
    "end_pos": 193779,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 509
    }
  },
  "1141": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_432",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "igence Technology. Springer. 2022. pp. 137 162. 87 A. Paszke. S. Gross. F. Massa. A. Lerer. J. Bradbury. G. Chanan. T. Killeen. Z. Lin. N. Gimelshein. L. Antiga. et al. Pytorch. An imper- ative style. high-performance deep learning library. Advances in neural information processing systems 32 2019 88 M. Abadi. P. Barham. J. Chen. Z. Chen. A. Davis. J. Dean. M. Devin. S. Ghemawat. G. Irving. M. Isard. et al. Tensorflow. system for large- scale machine learning. in. Osdi. ol. 16. Savannah. GA. USA. 2016. pp. 265 283.",
    "chunk_index": 432,
    "start_pos": 193780,
    "end_pos": 194270,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 520
    }
  },
  "1142": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_433",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ol. 16. Savannah. GA. USA. 2016. pp. 265 283. 89 T. Chen. M. Li. Li. M. Lin. N. Wang. M. Wang. T. Xiao. B. Xu. C. Zhang. Z. Zhang. Mxnet. flexible and fficient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv.1512.01274 2015 90 W. Fedus. B. Zoph. N. Shazeer. Switch transformers. Scaling to tril- lion parameter models with simple and fficient sparsity. The Journal of Machine Learning Research 23 2022 5232 5270. 5.",
    "chunk_index": 433,
    "start_pos": 194271,
    "end_pos": 194704,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 484,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 483,
      "optimized_content_length": 452
    }
  },
  "1143": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_434",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ne Learning Research 23 2022 5232 5270. 5. 91 N. Du. Huang. A. M. Dai. S. Tong. D. Lepikhin. Xu. M. Krikun. Zhou. A. W. Yu. O. Firat. et al. Glam. fficient scaling of language models with mixture-of-experts. in. International Conference on Ma- chine Learning. PMLR. 2022. pp. 5547 5569. 5. 9. 23. 24. 25 92 X. Ren. P. Zhou. X. Meng. X. Huang. Wang. W. Wang. P. Li. X. Zhang. A. Podolskiy. G. Arshinov. et al. Pangu-P. Towards trillion parameter language model with sparse heterogeneous computing. arXiv",
    "chunk_index": 434,
    "start_pos": 194705,
    "end_pos": 195188,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 97,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 502
    }
  },
  "1144": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_435",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "model with sparse heterogeneous computing. arXiv preprint arXiv.2303.10845 2023 5. 10. 16. 23. 24. 25 93 T. Wang. A. Roberts. D. Hesslow. T. Le Scao. H. W. Chung. I. Beltagy. J. Launay. C. Ra ffel. What language model architecture and pretrain- 37",
    "chunk_index": 435,
    "start_pos": 195189,
    "end_pos": 195392,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 254,
      "word_count": 43,
      "optimized_for_embedding": true,
      "original_content_length": 254,
      "optimized_content_length": 247
    }
  },
  "1145": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_436",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "What language model architecture and pretrain- 37 --- Page 38 --- ing objective works best for zero-shot generalization. in. International Conference on Machine Learning. PMLR. 2022. pp. 22964 22984. 94 L. Dong. N. Yang. W. Wang. F. Wei. X. Liu. Wang. J. Gao. M. Zhou. H.-W. Hon. Unified language model pre-training for natural language understanding and generation. Advances in neural information process- ing systems 32 2019 95 J. Kaplan. S. McCandlish. T. Henighan. T. B. Brown. B. Chess. R. Child.",
    "chunk_index": 436,
    "start_pos": 195394,
    "end_pos": 195861,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 501
    }
  },
  "1146": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_437",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ish. T. Henighan. T. B. Brown. B. Chess. R. Child. S. Gray. A. Radford. J. Wu. D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv.2001.08361 2020 96 J. Ho ffmann. S. Borgeaud. A. Mensch. E. Buchatskaya. T. Cai. E. Rutherford. D. d. L. Casas. L. A. Hendricks. J. Welbl. A. Clark. et al. Training compute-optimal large language models. arXiv preprint arXiv.2203.15556 2022 6. 9. 25. 29 97 S. Iyer. X. Lin. R. Pasunuru. T. Mihaylov. D. Simig. P. Yu. K. Shuster.",
    "chunk_index": 437,
    "start_pos": 195862,
    "end_pos": 196308,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 480
    }
  },
  "1147": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_438",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "asunuru. T. Mihaylov. D. Simig. P. Yu. K. Shuster. T. Wang. Q. Liu. P. S. Koura. et al. Opt-iml. Scaling language model in- struction meta learning through the lens of generalization. arXiv preprint arXiv.2212.12017 2022 7. 11. 16. 17. 22. 25. 28 98 Z. Sun. Shen. Q. Zhou. H. Zhang. Z. Chen. D. Cox. Yang. C. Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv.2305.03047 2023 7. 17 99 A. Askell. Bai. A. Chen. D. Drain. D. Ganguli. T. Henighan. A. Jones.",
    "chunk_index": 438,
    "start_pos": 196309,
    "end_pos": 196806,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 525
    }
  },
  "1148": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_439",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Chen. D. Drain. D. Ganguli. T. Henighan. A. Jones. N. Joseph. B. Mann. N. DasSarma. et al. general language assistant as laboratory for alignment. arXiv preprint arXiv.2112.00861 2021 100 D. M. Ziegler. N. Stiennon. J. Wu. T. B. Brown. A. Radford. D. Amodei. P. Christiano. G. Irving. Fine-tuning language models from human pref- erences. arXiv preprint arXiv.1909.08593 2019 101 S. Kim. S. J. Joo. D. Kim. J. Jang. S. Ye. J. Shin. M. Seo. The cot collec- tion. Improving zero-shot and few-shot learning of language models via",
    "chunk_index": 439,
    "start_pos": 196807,
    "end_pos": 197301,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 526
    }
  },
  "1149": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_440",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "-shot and few-shot learning of language models via chain-of-thought fine-tuning. arXiv preprint arXiv.2305.14045 2023 7. 16 102 Q. Liu. F. Zhou. Z. Jiang. L. Dou. M. Lin. From zero to hero. Exam- ining the power of symbolic tasks in instruction tuning. arXiv preprint arXiv.2304.07995 2023 7. 16 103 J. Wei. X. Wang. D. Schuurmans. M. Bosma. F. Xia. E. Chi. Q. Le. D. Zhou. et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems 35 2022 24824 24837. 7. 20. 23",
    "chunk_index": 440,
    "start_pos": 197302,
    "end_pos": 197800,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 532
    }
  },
  "1150": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_441",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rocessing Systems 35 2022 24824 24837. 7. 20. 23 104 X. Wang. J. Wei. D. Schuurmans. Q. Le. E. Chi. S. Narang. A. Chowd- hery. D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv.2203.11171 2022 7. 20 105 S. Yao. D. Yu. J. Zhao. I. Shafran. T. L. Gri ffiths. Cao. K. Narasimhan. Tree of thoughts. Deliberate problem solving with large language mod- els. arXiv preprint arXiv.2305.10601 2023 7. 20 106 N. Houlsby. A. Giurgiu. S. Jastrzebski. B. Morrone. Q. De Laroussilhe.",
    "chunk_index": 441,
    "start_pos": 197801,
    "end_pos": 198290,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 522
    }
  },
  "1151": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_442",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "iu. S. Jastrzebski. B. Morrone. Q. De Laroussilhe. A. Gesmundo. M. Attariyan. S. Gelly. Parameter-e fficient transfer learn- ing for nlp. in. International Conference on Machine Learning. PMLR. 2019. pp. 2790 2799. 7. 20 107 S. McCandlish. J. Kaplan. D. Amodei. O. D. Team. An empirical model of large-batch training. arXiv preprint arXiv.1812.06162 2018 108 W. Zeng. X. Ren. T. Su. H. Wang. Liao. Z. Wang. X. Jiang. Z. Yang. K. Wang. X. Zhang. et al. Pangu- \u03b1. Large-scale autoregressive pre-",
    "chunk_index": 442,
    "start_pos": 198291,
    "end_pos": 198747,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 493
    }
  },
  "1152": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_443",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "et al. Pangu- \u03b1. Large-scale autoregressive pre- trained chinese language models with auto-parallel computation. arXiv preprint arXiv.2104.12369 2021 8. 23. 24. 25 109 S. Yuan. H. Zhao. Z. Du. M. Ding. X. Liu. Cen. X. Zou. Z. Yang. J. Tang. Wudaocorpora. super large-scale chinese corpora for pre- training language models. AI Open 2021 65 68. 8. 30 110 Sun. S. Wang. S. Feng. S. Ding. C. Pang. J. Shang. J. Liu. X. Chen. Zhao. Lu. et al. Ernie 3.0. Large-scale knowledge enhanced",
    "chunk_index": 443,
    "start_pos": 198748,
    "end_pos": 199209,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 480
    }
  },
  "1153": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_444",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "et al. Ernie 3.0. Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv.2107.02137 2021 8. 25 111 Z. Dai. Z. Yang. Yang. J. Carbonell. Q. Le. R. Salakhutdinov. Transformer-xl. Attentive language models beyond fixed-length con- text. arXiv preprint arXiv.1901.02860 2019 112 O. Lieber. O. Sharir. B. Lenz. Shoham. Jurassic-1. Technical details and evaluation. White Paper. AI21 Labs 2021 8. 24. 25",
    "chunk_index": 444,
    "start_pos": 199210,
    "end_pos": 199642,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 483,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 482,
      "optimized_content_length": 450
    }
  },
  "1154": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_445",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "uation. White Paper. AI21 Labs 2021 8. 24. 25 113 Levine. N. Wies. O. Sharir. H. Bata. A. Shashua. Limits to depth ef- ficiencies of self-attention. Advances in Neural Information Processing Systems 33 2020 22640 22651. 8. 11 114 B. Kim. H. Kim. S.-W. Lee. G. Lee. D. Kwak. D. H. Jeon. S. Park.S. Kim. S. Kim. D. Seo. et al. What changes can large-scale language models bring. intensive study on hyperclova. Billions-scale korean generative pretrained transformers. arXiv preprint arXiv.2109.04650 2021 8. 25",
    "chunk_index": 445,
    "start_pos": 199643,
    "end_pos": 200119,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 508
    }
  },
  "1155": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_446",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ers. arXiv preprint arXiv.2109.04650 2021 8. 25 115 S. Wu. X. Zhao. T. Yu. R. Zhang. C. Shen. H. Liu. F. Li. H. Zhu. J. Luo. L. Xu. et al. Yuan 1.0. Large-scale pre-trained language model in zero- shot and few-shot learning. arXiv preprint arXiv.2110.04725 2021 8. 24. 25 116 J. W. Rae. S. Borgeaud. T. Cai. K. Millican. J. Ho ffmann. F. Song. J. Aslanides. S. Henderson. R. Ring. S. Young. et al. Scaling lan- guage models. Methods. analysis insights from training gopher. arXiv preprint arXiv.2112.11446 2021 8. 9. 25. 28",
    "chunk_index": 446,
    "start_pos": 200120,
    "end_pos": 200609,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 523
    }
  },
  "1156": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_447",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Xiv preprint arXiv.2112.11446 2021 8. 9. 25. 28 117 S. Smith. M. Patwary. B. Norick. P. LeGresley. S. Rajbhandari. J. Casper. Z. Liu. S. Prabhumoye. G. Zerveas. Korthikanti. et al. Using deepspeed and megatron to train megatron-turing nlg 530b. large-scale generative language model. arXiv preprint arXiv.2201.11990 2022 8. 9. 24. 25 118 S. Black. S. Biderman. E. Hallahan. Q. Anthony. L. Gao. L. Golding. H. He. C. Leahy. K. McDonell. J. Phang. et al. Gpt-neox-20b. An open-",
    "chunk_index": 447,
    "start_pos": 200610,
    "end_pos": 201052,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 475
    }
  },
  "1157": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_448",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "McDonell. J. Phang. et al. Gpt-neox-20b. An open- source autoregressive language model. arXiv preprint arXiv.2204.06745 2022 9. 23. 24. 25 119 W. Ben. K. Aran. Gpt-j-6b. billion parameter autoregressive lan- guage model 2021 120 P. Micikevicius. S. Narang. J. Alben. G. Diamos. E. Elsen. D. Garcia. B. Ginsburg. M. Houston. O. Kuchaiev. G. Venkatesh. et al. Mixed pre- cision training. arXiv preprint arXiv.1710.03740 2017 9. 23 121 N. Shazeer. A. Mirhoseini. K. Maziarz. A. Davis. Q. Le. G. Hin-",
    "chunk_index": 448,
    "start_pos": 201053,
    "end_pos": 201521,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 496
    }
  },
  "1158": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_449",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Mirhoseini. K. Maziarz. A. Davis. Q. Le. G. Hin- ton. J. Dean. Outrageously large neural networks. The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv.1701.06538 2017 9. 23 122 S. Soltan. S. Ananthakrishnan. J. FitzGerald. R. Gupta. W. Hamza. H. Khan. C. Peris. S. Rawls. A. Rosenbaum. A. Rumshisky. et al. Alex- atm 20b. Few-shot learning using large-scale multilingual seq2seq model. arXiv preprint arXiv.2208.01448 2022 9. 23. 24. 25 123 R. Anil. A. M. Dai. O. Firat. M. Johnson. D. Lepikhin. A. Passos.",
    "chunk_index": 449,
    "start_pos": 201522,
    "end_pos": 202006,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 520
    }
  },
  "1159": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_450",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Dai. O. Firat. M. Johnson. D. Lepikhin. A. Passos. S. Shakeri. E. Taropa. P. Bailey. Z. Chen. et al. Palm technical report. arXiv preprint arXiv.2305.10403 2023 9. 25 124 Tay. J. Wei. H. W. Chung. Q. Tran. D. R. So. S. Shakeri. X. Garcia. H. S. Zheng. J. Rao. A. Chowdhery. et al. Transcending scaling laws with 0.1 extra compute. arXiv preprint arXiv.2210.11399 2022 9. 24. 25 125 Tay. M. Dehghani. Q. Tran. X. Garcia. J. Wei. X. Wang. H. W. Chung. D. Bahri. T. Schuster. S. Zheng. et al. Ul2. Unifying lan-",
    "chunk_index": 450,
    "start_pos": 202007,
    "end_pos": 202496,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 102,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 508
    }
  },
  "1160": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_451",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "T. Schuster. S. Zheng. et al. Ul2. Unifying lan- guage learning paradigms. in. The Eleventh International Conference on Learning Representations. 2022. 9. 10. 24. 25 126 Z. Du. Qian. X. Liu. M. Ding. J. Qiu. Z. Yang. J. Tang. Glm. Gen- eral language model pretraining with autoregressive blank infilling. in. Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics olume 1. Long Papers 2022. pp. 320 335. 10 127 H. Touvron. T. Lavril. G. Izacard. X. Martinet. M.-A. Lachaux.",
    "chunk_index": 451,
    "start_pos": 202497,
    "end_pos": 202969,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 508
    }
  },
  "1161": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_452",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "T. Lavril. G. Izacard. X. Martinet. M.-A. Lachaux. T. Lacroix. B. Rozie re. N. Goyal. E. Hambro. F. Azhar. et al. Llama. Open and fficient foundation language models. arXiv preprint arXiv.2302.13971 2023 10. 23. 25 128 M. N. Rabe. C. Staats. Self-attention does not need n2 memory. arXiv preprint arXiv.2112.05682 2021 10 129 A. Korthikanti. J. Casper. S. Lym. L. McAfee. M. Andersch. M. Shoeybi. B. Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems 2023 10",
    "chunk_index": 452,
    "start_pos": 202970,
    "end_pos": 203468,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 525
    }
  },
  "1162": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_453",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "dings of Machine Learning and Systems 2023 10 130 A. Dubey. A. Jauhri. A. Pandey. A. Kadian. A. Al-Dahle. A. Letman. A. Mathur. A. Schelten. A. Yang. A. Fan. et al. The llama herd of models. arXiv preprint arXiv.2407.21783 2024 10. 25 131 https. mistral.ai news mixtral-8x22b 10. 25 132 https. github.com Snowflake-Labs snowflake-arctic 10. 25 133 https. github.com xai-org grok-1 10 134 https. x.ai blog grok-1.5 10 135 G. Team. R. Anil. S. Borgeaud. Wu. J.-B. Alayrac. J. Yu. R. Soricut.",
    "chunk_index": 453,
    "start_pos": 203469,
    "end_pos": 203947,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 489
    }
  },
  "1163": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_454",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "orgeaud. Wu. J.-B. Alayrac. J. Yu. R. Soricut. J. Schalkwyk. A. M. Dai. A. Hauth. et al. Gemini. family of highly capable multimodal models. arXiv preprint arXiv.2312.11805 2023 10 136 M. Reid. N. Savinov. D. Teplyashin. D. Lepikhin. T. Lillicrap. J.-b. 38",
    "chunk_index": 454,
    "start_pos": 203948,
    "end_pos": 204165,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 268,
      "word_count": 45,
      "optimized_for_embedding": true,
      "original_content_length": 268,
      "optimized_content_length": 256
    }
  },
  "1164": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_455",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "D. Teplyashin. D. Lepikhin. T. Lillicrap. J.-b. 38 --- Page 39 --- Alayrac. R. Soricut. A. Lazaridou. O. Firat. J. Schrittwieser. et al. Gem- ini 1.5. Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv.2403.05530 2024 10 137 B. Adler. N. Agarwal. A. Aithal. D. H. Anh. P. Bhattacharya. A. Brun- dyn. J. Casper. B. Catanzaro. S. Clay. J. Cohen. et al. Nemotron-4 340b technical report. arXiv preprint arXiv.2406.11704 2024 10. 25",
    "chunk_index": 455,
    "start_pos": 204167,
    "end_pos": 204597,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 481,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 471
    }
  },
  "1165": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_456",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rt. arXiv preprint arXiv.2406.11704 2024 10. 25 138 X. Bi. D. Chen. G. Chen. S. Chen. D. Dai. C. Deng. H. Ding. K. Dong. Q. Du. Z. Fu. et al. Deepseek llm. Scaling open-source language models with longtermism. arXiv preprint arXiv.2401.02954 2024 10. 25 139 DeepSeek-AI. A. Liu. B. Feng. B. Wang. B. Wang. B. Liu. C. Zhao. C. Deng. C. Ruan. D. Dai. D. Guo. D. Yang. D. Chen. D. Ji. E. Li. F. Lin. F. Luo. G. Hao. G. Chen. G. Li. H. Zhang. H. Xu. H. Yang. H. Zhang. H. Ding. H. Xin. H. Gao. H. Li. H. Qu. J. L. Cai. J. Liang.",
    "chunk_index": 456,
    "start_pos": 204598,
    "end_pos": 205082,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 107,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 524
    }
  },
  "1166": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_457",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "H. Xin. H. Gao. H. Li. H. Qu. J. L. Cai. J. Liang. J. Guo. J. Ni. J. Li. J. Chen. J. Yuan. J. Qiu. J. Song. K. Dong. K. Gao. K. Guan. L. Wang. L. Zhang. L. Xu. L. Xia. L. Zhao. L. Zhang. M. Li. M. Wang. M. Zhang. M. Zhang. M. Tang. M. Li. N. Tian. P. Huang. P. Wang. P. Zhang. Q. Zhu. Q. Chen. Q. Du. R. J. Chen. R. L. Jin. R. Ge. R. Pan. R. Xu. R. Chen. S. S. Li. S. Lu. S. Zhou. S. Chen. S. Wu. S. Ye. S. Ma. S. Wang. S. Zhou. S. Yu. S. Zhou. S. Zheng. T. Wang. T. Pei. T. Yuan. T. Sun. W. L. Xiao. W. Zeng. W. An. W. Liu. W. Liang. W. Gao.",
    "chunk_index": 457,
    "start_pos": 205083,
    "end_pos": 205574,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 131,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 542
    }
  },
  "1167": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_458",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "L. Xiao. W. Zeng. W. An. W. Liu. W. Liang. W. Gao. W. Zhang. X. Q. Li. X. Jin. X. Wang. X. Bi. X. Liu. X. Wang. X. Shen. X. Chen. X. Chen. X. Nie. X. Sun. Deepseek-v2. strong. economical. and fficient mixture-of-experts language model. CoRR abs 2405.04434 2024 10. 25 140 E. Nijkamp. B. Pang. H. Hayashi. L. Tu. H. Wang. Zhou. S. Savarese. C. Xiong. Codegen. An open large language model for code with multi- turn program synthesis. arXiv preprint arXiv.2203.13474 2022 11. 23. 25. 28",
    "chunk_index": 458,
    "start_pos": 205575,
    "end_pos": 206025,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 484
    }
  },
  "1168": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_459",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "preprint arXiv.2203.13474 2022 11. 23. 25. 28 141 M. Chen. J. Tworek. H. Jun. Q. Yuan. H. P. d. O. Pinto. J. Kaplan. H. Ed- wards. Burda. N. Joseph. G. Brockman. et al. Evaluating large lan- guage models trained on code. arXiv preprint arXiv.2107.03374 2021 11. 25. 29. 31 142 Li. D. Choi. J. Chung. N. Kushman. J. Schrittwieser. R. Leblond. T. Eccles. J. Keeling. F. Gimeno. A. Dal Lago. et al. Competition-level code generation with alphacode. Science 378 6624 2022 1092 1097. 11. 23. 25. 29",
    "chunk_index": 459,
    "start_pos": 206026,
    "end_pos": 206494,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 493
    }
  },
  "1169": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_460",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "cience 378 6624 2022 1092 1097. 11. 23. 25. 29 143 N. Shazeer. Fast transformer decoding. One write-head is all you need. arXiv preprint arXiv.1911.02150 2019 11 144 R. Pang. H. He. Text generation by learning from demonstrations. arXiv preprint arXiv.2009.07839 2020 11 145 R. Dabre. A. Fujita. Softmax tempering for training neural machine translation models. arXiv preprint arXiv.2009.09372 2020 11 146 Wang. W. Wang. S. Joty. S. C. Hoi. Codet5. Identifier-aware unified",
    "chunk_index": 460,
    "start_pos": 206495,
    "end_pos": 206946,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 473
    }
  },
  "1170": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_461",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Joty. S. C. Hoi. Codet5. Identifier-aware unified pre-trained encoder-decoder models for code understanding and genera- tion. arXiv preprint arXiv.2109.00859 2021 11 147 R. Li. L. B. Allal. Zi. N. Muennigho ff. D. Kocetkov. C. Mou. M. Marone. C. Akiki. J. Li. J. Chim. et al. Starcoder. may the source be with you. arXiv preprint arXiv.2305.06161 2023 11. 25 148 R. Taylor. M. Kardas. G. Cucurull. T. Scialom. A. Hartshorn. E. Saravia. A. Poulton. Kerkez. R. Stojnic. Galactica. large language model for",
    "chunk_index": 461,
    "start_pos": 206947,
    "end_pos": 207422,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 503
    }
  },
  "1171": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_462",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "R. Stojnic. Galactica. large language model for science. arXiv preprint arXiv.2211.09085 2022 11. 24. 25. 29 149 FairScale authors. Fairscale. general purpose modular pytorch library for high performance and large scale training. https. github.com facebookresearch fairscale 2021 11 150 R. Thoppilan. D. De Freitas. J. Hall. N. Shazeer. A. Kulshreshtha. H.-T. Cheng. A. Jin. T. Bos. L. Baker. Du. et al. Lamda. Language models for dialog applications. arXiv preprint arXiv.2201.08239 2022 11. 25",
    "chunk_index": 462,
    "start_pos": 207423,
    "end_pos": 207892,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 495
    }
  },
  "1172": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_463",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ns. arXiv preprint arXiv.2201.08239 2022 11. 25 151 S. Wu. O. Irsoy. S. Lu. Dabravolski. M. Dredze. S. Gehrmann. P. Kambadur. D. Rosenberg. G. Mann. Bloomberggpt. large language model for finance. arXiv preprint arXiv.2303.17564 2023 11. 25. 33 152 X. Zhang. Q. Yang. D. Xu. Xuanyuan 2.0. large chinese finan- cial chat model with hundreds of billions parameters. arXiv preprint arXiv.2305.12002 2023 11. 17. 25 153 W. Ben. Mesh-transformer-jax. Model-parallel implementation of trans-",
    "chunk_index": 463,
    "start_pos": 207893,
    "end_pos": 208350,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 485
    }
  },
  "1173": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_464",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ormer-jax. Model-parallel implementation of trans- former language model with jax 2021 12. 24 154 N. Muennigho ff. T. Wang. L. Sutawika. A. Roberts. S. Biderman. T. L. Scao. M. S. Bari. S. Shen. Z.-X. Yong. H. Schoelkopf. et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv.2211.01786 2022 16. 25. 28. 31 155 D. Yin. X. Liu. F. Yin. M. Zhong. H. Bansal. J. Han. K.-W. Chang. Dynosaur. dynamic growth paradigm for instruction-tuning data cu-ration. arXiv preprint arXiv.2305.14327 2023 16",
    "chunk_index": 464,
    "start_pos": 208351,
    "end_pos": 208837,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 521
    }
  },
  "1174": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_465",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ration. arXiv preprint arXiv.2305.14327 2023 16 156 P. Gao. J. Han. R. Zhang. Z. Lin. S. Geng. A. Zhou. W. Zhang. P. Lu. C. He. X. Yue. et al. Llama-adapter v2. Parameter-e fficient visual in- struction model. arXiv preprint arXiv.2304.15010 2023 16. 24 157 Openai. gpt-4 technical report 2023 16. 35 158 R. Taori. I. Gulrajani. T. Zhang. Dubois. X. Li. C. Guestrin. P. Liang. T. B. Hashimoto. Stanford alpaca. An instruction-following llama model. https. github.com tatsu-lab stanford_alpaca 2023 16. 25. 28",
    "chunk_index": 465,
    "start_pos": 208838,
    "end_pos": 209319,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 508
    }
  },
  "1175": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_466",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "b.com tatsu-lab stanford_alpaca 2023 16. 25. 28 159 W.-L. Chiang. Z. Li. Z. Lin. Sheng. Z. Wu. H. Zhang. L. Zheng. S. Zhuang. Zhuang. J. E. Gonzalez. I. Stoica. E. P. Xing. Vicuna. An open-source chatbot impressing gpt-4 with 90 chatgpt quality March 2023 URL https. lmsys.org blog 2023-03-30-vicuna 16. 22. 25. 28 160 B. Peng. C. Li. P. He. M. Galley. J. Gao. Instruction tuning with gpt-4. arXiv preprint arXiv.2304.03277 2023 16. 28 161 T. Liu. B. K. H. Low. Goat. Fine-tuned llama outperforms gpt-4 on",
    "chunk_index": 466,
    "start_pos": 209320,
    "end_pos": 209801,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 505
    }
  },
  "1176": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_467",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Low. Goat. Fine-tuned llama outperforms gpt-4 on arithmetic tasks. arXiv preprint arXiv.2305.14201 2023 16 162 H. Wang. C. Liu. N. Xi. Z. Qiang. S. Zhao. B. Qin. T. Liu. Huatuo. Tuning llama model with chinese medical knowledge. arXiv preprint arXiv.2304.06975 2023 16 163 C. Xu. Q. Sun. K. Zheng. X. Geng. P. Zhao. J. Feng. C. Tao. D. Jiang. Wizardlm. Empowering large language models to follow complex in- structions. arXiv preprint arXiv.2304.12244 2023 16 164 Z. Luo. C. Xu. P. Zhao. Q. Sun. X. Geng. W. Hu. C. Tao. J. Ma. Q. Lin.",
    "chunk_index": 467,
    "start_pos": 209802,
    "end_pos": 210302,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 534
    }
  },
  "1177": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_468",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ao. Q. Sun. X. Geng. W. Hu. C. Tao. J. Ma. Q. Lin. D. Jiang. Wizardcoder. Empowering code large language models with evol-instruct. arXiv preprint arXiv.2306.08568 2023 16. 25 165 J. Menick. M. Trebacz. Mikulik. J. Aslanides. F. Song. M. Chadwick. M. Glaese. S. Young. L. Campbell-Gillingham. G. Irving. et al. Teach- ing language models to support answers with verified quotes. arXiv preprint arXiv.2203.11147 2022 17 166 R. Nakano. J. Hilton. S. Balaji. J. Wu. L. Ouyang. C. Kim.",
    "chunk_index": 468,
    "start_pos": 210303,
    "end_pos": 210748,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 481
    }
  },
  "1178": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_469",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "o. J. Hilton. S. Balaji. J. Wu. L. Ouyang. C. Kim. C. Hesse. S. Jain. Kosaraju. W. Saunders. et al. Webgpt. Browser- assisted question-answering with human feedback. arXiv preprint arXiv.2112.09332 2021 17. 19. 20. 25. 31 167 A. Glaese. N. McAleese. M. Tr ebacz. J. Aslanides. Firoiu. T. Ewalds. M. Rauh. L. Weidinger. M. Chadwick. P. Thacker. et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv.2209.14375 2022 17. 20. 25",
    "chunk_index": 469,
    "start_pos": 210749,
    "end_pos": 211185,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 487,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 487,
      "optimized_content_length": 468
    }
  },
  "1179": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_470",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "arXiv preprint arXiv.2209.14375 2022 17. 20. 25 168 R. Rafailov. A. Sharma. E. Mitchell. S. Ermon. C. D. Manning. C. Finn. Direct preference optimization. Your language model is secretly re- ward model. arXiv preprint arXiv.2305.18290 2023 17 169 H. Dong. W. Xiong. D. Goyal. R. Pan. S. Diao. J. Zhang. K. Shum. T. Zhang. Raft. Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv.2304.06767 2023 17 170 Z. Yuan. H. Yuan. C. Tan. W. Wang. S. Huang. F. Huang. Rrhf. Rank",
    "chunk_index": 470,
    "start_pos": 211186,
    "end_pos": 211657,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 505
    }
  },
  "1180": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_471",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "n. C. Tan. W. Wang. S. Huang. F. Huang. Rrhf. Rank responses to align language models with human feedback without tears. arXiv preprint arXiv.2304.05302 2023 17 171 F. Song. B. Yu. M. Li. H. Yu. F. Huang. Li. H. Wang. Preference rank- ing optimization for human alignment. arXiv preprint arXiv.2306.17492 2023 17 172 H. Liu. C. Sferrazza. P. Abbeel. Languages are rewards. Hindsight fine- tuning using human feedback. arXiv preprint arXiv.2302.02676 2023 17",
    "chunk_index": 471,
    "start_pos": 211658,
    "end_pos": 212081,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 474,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 474,
      "optimized_content_length": 457
    }
  },
  "1181": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_472",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "edback. arXiv preprint arXiv.2302.02676 2023 17 173 Bai. S. Kadavath. S. Kundu. A. Askell. J. Kernion. A. Jones. A. Chen. A. Goldie. A. Mirhoseini. C. McKinnon. et al. Constitutional ai. Harm- lessness from ai feedback. arXiv preprint arXiv.2212.08073 2022 17 174 Dubois. X. Li. R. Taori. T. Zhang. I. Gulrajani. J. Ba. C. Guestrin. P. Liang. T. B. Hashimoto. Alpacafarm. simulation frame- work for methods that learn from human feedback. arXiv preprint arXiv.2305.14387 2023 17",
    "chunk_index": 472,
    "start_pos": 212082,
    "end_pos": 212533,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 478
    }
  },
  "1182": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_473",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "edback. arXiv preprint arXiv.2305.14387 2023 17 175 C. Si. Z. Gan. Z. Yang. S. Wang. J. Wang. J. Boyd-Graber. L. Wang. Prompting gpt-3 to be reliable. arXiv preprint arXiv.2210.09150 2022 17 176 D. Ganguli. A. Askell. N. Schiefer. T. Liao. K. Lukos ut e. A. Chen. A. Goldie. A. Mirhoseini. C. Olsson. D. Hernandez. et al. The capac- ity for moral self-correction in large language models. arXiv preprint arXiv.2302.07459 2023 17 177 A. Wei. N. Haghtalab. J. Steinhardt. Jailbroken. How does llm safety",
    "chunk_index": 473,
    "start_pos": 212534,
    "end_pos": 213002,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 501
    }
  },
  "1183": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_474",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ab. J. Steinhardt. Jailbroken. How does llm safety training fail. arXiv preprint arXiv.2307.02483 2023 17 39",
    "chunk_index": 474,
    "start_pos": 213003,
    "end_pos": 213064,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 112,
      "word_count": 16,
      "optimized_for_embedding": true,
      "original_content_length": 112,
      "optimized_content_length": 108
    }
  },
  "1184": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_475",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "il. arXiv preprint arXiv.2307.02483 2023 17 39 --- Page 40 --- 178 D. Ganguli. L. Lovitt. J. Kernion. A. Askell. Bai. S. Kadavath. B. Mann. E. Perez. N. Schiefer. K. Ndousse. et al. Red teaming lan- guage models to reduce harms. Methods. scaling behaviors. and lessons learned. arXiv preprint arXiv.2209.07858 2022 17. 28 179 S. Casper. J. Lin. J. Kwon. G. Culp. D. Hadfield-Menell. Explore. estab- lish. exploit. Red teaming language models from scratch. arXiv preprint arXiv.2306.09442 2023 17",
    "chunk_index": 475,
    "start_pos": 213066,
    "end_pos": 213529,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 495
    }
  },
  "1185": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_476",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "cratch. arXiv preprint arXiv.2306.09442 2023 17 180 E. Perez. S. Huang. F. Song. T. Cai. R. Ring. J. Aslanides. A. Glaese. N. McAleese. G. Irving. Red teaming language models with language models. arXiv preprint arXiv.2202.03286 2022 17 181 T. Scialom. T. Chakrabarty. S. Muresan. Fine-tuned language models are continual learners. in. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022. pp. 6107 6122. 17 182 Z. Shi. A. Lipani. Don stop pretraining. make prompt-based fine-",
    "chunk_index": 476,
    "start_pos": 213530,
    "end_pos": 214009,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 516
    }
  },
  "1186": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_477",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "i. Don stop pretraining. make prompt-based fine- tuning powerful learner. arXiv preprint arXiv.2305.01711 2023 17 183 H. Gupta. S. A. Sawant. S. Mishra. M. Nakamura. A. Mitra. S. Mashetty. C. Baral. Instruction tuned models are quick learners. arXiv preprint arXiv.2306.05539 2023 17 184 H. Chen. Zhang. Q. Zhang. H. Yang. X. Hu. X. Ma. Yanggong. J. Zhao. Maybe only 0.5 data is needed. preliminary exploration of low training data instruction tuning. arXiv preprint arXiv.2305.09246 2023 17",
    "chunk_index": 477,
    "start_pos": 214010,
    "end_pos": 214476,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 491
    }
  },
  "1187": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_478",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tuning. arXiv preprint arXiv.2305.09246 2023 17 185 C. Zhou. P. Liu. P. Xu. S. Iyer. J. Sun. Mao. X. Ma. A. Efrat. P. Yu. L. Yu. et al. Lima. Less is more for alignment. arXiv preprint arXiv.2305.11206 2023 17. 25. 28 186 C. Han. Q. Wang. W. Xiong. Chen. H. Ji. S. Wang. Lm-infinite. Sim- ple on-the-fly length generalization for large language models. arXiv preprint arXiv.2308.16137 2023 17. 18 187 J. Ainslie. T. Lei. M. de Jong. S. Ontan n. S. Brahma. Zemlyan-",
    "chunk_index": 478,
    "start_pos": 214477,
    "end_pos": 214918,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 464
    }
  },
  "1188": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_479",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "i. M. de Jong. S. Ontan n. S. Brahma. Zemlyan- skiy. D. Uthus. M. Guo. J. Lee-Thorp. Tay. et al. Colt5. Faster long-range transformers with conditional computation. arXiv preprint arXiv.2303.09752 2023 18 188 J. Ding. S. Ma. L. Dong. X. Zhang. S. Huang. W. Wang. F. Wei. Longnet. Scaling transformers to 1.000.000.000 tokens. arXiv preprint arXiv.2307.02486 2023 18 189 Chen. S. Qian. H. Tang. X. Lai. Z. Liu. S. Han. J. Jia. Longlora. ffi- cient fine-tuning of long-context large language models. arXiv preprint",
    "chunk_index": 479,
    "start_pos": 214919,
    "end_pos": 215405,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 512
    }
  },
  "1189": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_480",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "long-context large language models. arXiv preprint arXiv.2309.12307 2023 18 190 N. Ratner. Levine. Belinkov. O. Ram. I. Magar. O. Abend. E. Karpas. A. Shashua. K. Leyton-Brown. Shoham. Parallel context windows for large language models. in. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics olume 1. Long Papers 2023. pp. 6383 6402. 18 191 W. Wang. L. Dong. H. Cheng. X. Liu. X. Yan. J. Gao. F. Wei. Augmenting language models with long-term memory. arXiv preprint",
    "chunk_index": 480,
    "start_pos": 215406,
    "end_pos": 215881,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 502
    }
  },
  "1190": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_481",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "guage models with long-term memory. arXiv preprint arXiv.2306.07174 2023 18 192 X. Xu. Z. Gou. W. Wu. Z.-Y Niu. H. Wu. H. Wang. S. Wang. Long time no see. open-domain conversation with long-term persona memory. arXiv preprint arXiv.2203.05797 2022 18 193 S. Borgeaud. A. Mensch. J. Ho ffmann. T. Cai. E. Rutherford. K. Milli- can. G. B. Van Den Driessche. J.-B. Lespiau. B. Damoc. A. Clark. et al. Improving language models by retrieving from trillions of tokens. in. International conference on machine learning. PMLR. 2022. pp. 2206",
    "chunk_index": 481,
    "start_pos": 215882,
    "end_pos": 216379,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 548,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 534
    }
  },
  "1191": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_482",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ference on machine learning. PMLR. 2022. pp. 2206 2240. 18. 19. 34 194 W. Zhong. L. Guo. Q. Gao. Wang. Memorybank. Enhanc- ing large language models with long-term memory. arXiv preprint arXiv.2305.10250 2023 18 195 N. Shinn. F. Cassano. B. Labash. A. Gopinath. K. Narasimhan. S. Yao. Reflexion. Language agents with verbal reinforcement learning. arXiv preprint arXiv.2303.11366 14 2023 18. 20 196 C. Hu. J. Fu. C. Du. S. Luo. J. Zhao. H. Zhao. Chatdb. Augment- ing llms with databases as their symbolic memory. arXiv preprint",
    "chunk_index": 482,
    "start_pos": 216380,
    "end_pos": 216873,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 527
    }
  },
  "1192": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_483",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "databases as their symbolic memory. arXiv preprint arXiv.2306.03901 2023 18 197 Z. Jiang. F. F. Xu. L. Gao. Z. Sun. Q. Liu. J. Dwivedi-Yu. Yang. J. Callan. G. Neubig. Active retrieval augmented generation. arXiv preprint arXiv.2305.06983 2023 18 198 O. Ram. Levine. I. Dalmedigos. D. Muhlgay. A. Shashua. K. Leyton- Brown. Shoham. In-context retrieval-augmented language models. arXiv preprint arXiv.2302.00083 2023 18. 34 199 X. Li. X. Qiu. Mot. Pre-thinking and recalling enable chatgpt to self-",
    "chunk_index": 483,
    "start_pos": 216874,
    "end_pos": 217347,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 497
    }
  },
  "1193": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_484",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Pre-thinking and recalling enable chatgpt to self- improve with memory-of-thoughts. arXiv preprint arXiv.2305.05181 2023 18 200 D. Schuurmans. Memory augmented large language models are compu- tationally universal. arXiv preprint arXiv.2301.04589 2023 18 201 A. Modarressi. A. Imani. M. Fayyaz. H. Schu tze. Ret-llm. Towards general read-write memory for large language models. arXiv preprint arXiv.2305.14322 2023 18 202 S. Robertson. H. Zaragoza. et al. The probabilistic relevance frame-",
    "chunk_index": 484,
    "start_pos": 217348,
    "end_pos": 217803,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 490
    }
  },
  "1194": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_485",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ragoza. et al. The probabilistic relevance frame- work. Bm25 and beyond. Foundations and Trends in Information Re- trieval 2009 333 389. 18 203 X. Wang. J. Wei. D. Schuurmans. Q. Le. E. Chi. D. Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv.2207.00747 2022 18 204 F. Zhang. B. Chen. Zhang. J. Liu. D. Zan. Mao. J.-G. Lou. W. Chen. Repocoder. Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv.2303.12570 2023 18",
    "chunk_index": 485,
    "start_pos": 217804,
    "end_pos": 218269,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 488
    }
  },
  "1195": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_486",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ration. arXiv preprint arXiv.2303.12570 2023 18 205 B. Wang. W. Ping. P. Xu. L. McAfee. Z. Liu. M. Shoeybi. Dong. O. Kuchaiev. B. Li. C. Xiao. et al. Shall we pretrain autoregressive language models with retrieval. comprehensive study. arXiv preprint arXiv.2304.06762 2023 19 206 L. Wang. N. Yang. F. Wei. Learning to retrieve in-context examples for large language models. arXiv preprint arXiv.2307.07164 2023 19 207 J. Liu. D. Shen. Zhang. B. Dolan. L. Carin. W. Chen. What makes",
    "chunk_index": 486,
    "start_pos": 218270,
    "end_pos": 218726,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 507,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 507,
      "optimized_content_length": 481
    }
  },
  "1196": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_487",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Zhang. B. Dolan. L. Carin. W. Chen. What makes good in-context examples for gpt-3. arXiv preprint arXiv.2101.06804 2021 19 208 O. Rubin. J. Herzig. J. Berant. Learning to retrieve prompts for in- context learning. arXiv preprint arXiv.2112.08633 2021 19 209 W. Shi. S. Min. M. Yasunaga. M. Seo. R. James. M. Lewis. L. Zettle- moyer. W.-t. Yih. Replug. Retrieval-augmented black-box language models. arXiv preprint arXiv.2301.12652 2023 19 210 O. Rubin. J. Berant. Long-range language modeling with self-retrieval.",
    "chunk_index": 487,
    "start_pos": 218727,
    "end_pos": 219209,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 513
    }
  },
  "1197": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_488",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Long-range language modeling with self-retrieval. arXiv preprint arXiv.2306.13421 2023 19 211 K. Guu. K. Lee. Z. Tung. P. Pasupat. M. Chang. Retrieval augmented language model pre-training. in. International conference on machine learning. PMLR. 2020. pp. 3929 3938. 19 212 S. Hofsta tter. J. Chen. K. Raman. H. Zamani. Fid-light. fficient and ef- fective retrieval-augmented text generation. in. Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2023. pp. 1437 1447. 19",
    "chunk_index": 488,
    "start_pos": 219210,
    "end_pos": 219701,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 533
    }
  },
  "1198": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_489",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "in Information Retrieval. 2023. pp. 1437 1447. 19 213 M. Komeili. K. Shuster. J. Weston. Internet-augmented dialogue gener- ation. arXiv preprint arXiv.2107.07566 2021 19 214 A. Lazaridou. E. Gribovskaya. W. Stokowiec. N. Grigorev. Internet- augmented language models through few-shot prompting for open- domain question answering. arXiv preprint arXiv.2203.05115 2022 19 215 D. Gao. L. Ji. L. Zhou. K. Q. Lin. J. Chen. Z. Fan. M. Z. Shou. Assist- gpt. general multi-modal assistant that can plan. execute. inspect. and",
    "chunk_index": 489,
    "start_pos": 219702,
    "end_pos": 220185,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 519
    }
  },
  "1199": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_490",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "dal assistant that can plan. execute. inspect. and learn. arXiv preprint arXiv.2306.08640 2023 19 216 P. Lu. B. Peng. H. Cheng. M. Galley. K.-W. Chang. N. Wu. S.-C. Zhu. J. Gao. Chameleon. Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv.2304.09842 2023 19. 20. 23 217 B. Paranjape. S. Lundberg. S. Singh. H. Hajishirzi. L. Zettlemoyer. M. T. Ribeiro. Art. Automatic multi-step reasoning and tool-use for large lan- guage models. arXiv preprint arXiv.2303.09014 2023 19",
    "chunk_index": 490,
    "start_pos": 220186,
    "end_pos": 220659,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 507
    }
  },
  "1200": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_491",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2303.09014 2023 19 218 C.-Y Hsieh. S.-A. Chen. C.-L. Li. Fujii. A. Ratner. C.-Y Lee. R. Kr- ishna. T. Pfister. Tool documentation enables zero-shot tool-usage with large language models. arXiv preprint arXiv.2308.00675 2023 19 219 Song. W. Xiong. D. Zhu. C. Li. K. Wang. Tian. S. Li. Restgpt. Connecting large language models with real-world applications via rest- ful apis. arXiv preprint arXiv.2306.06624 2023 19 220 S. Hao. T. Liu. Z. Wang. Z. Hu. Toolkengpt. Augmenting frozen lan-",
    "chunk_index": 491,
    "start_pos": 220660,
    "end_pos": 221154,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 514
    }
  },
  "1201": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_492",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Z. Wang. Z. Hu. Toolkengpt. Augmenting frozen lan- guage models with massive tools via tool embeddings. arXiv preprint arXiv.2305.11554 2023 19 221 S. G. Patil. T. Zhang. X. Wang. J. E. Gonzalez. Gorilla. Large language model connected with massive apis. arXiv preprint arXiv.2305.15334 2023 19 222 Q. Xu. F. Hong. B. Li. C. Hu. Z. Chen. J. Zhang. On the tool manipu- lation capability of open-source large language models. arXiv preprint arXiv.2305.16504 2023 19",
    "chunk_index": 492,
    "start_pos": 221155,
    "end_pos": 221580,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 476,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 476,
      "optimized_content_length": 463
    }
  },
  "1202": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_493",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2305.16504 2023 19 223 Qin. S. Liang. Ye. K. Zhu. L. Yan. Lu. Lin. X. Cong. X. Tang. B. Qian. et al. Toolllm. Facilitating large language models to master 16000 real-world apis. arXiv preprint arXiv.2307.16789 2023 19. 40",
    "chunk_index": 493,
    "start_pos": 221581,
    "end_pos": 221806,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 276,
      "word_count": 49,
      "optimized_for_embedding": true,
      "original_content_length": 276,
      "optimized_content_length": 250
    }
  },
  "1203": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_494",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "is. arXiv preprint arXiv.2307.16789 2023 19. 40 --- Page 41 --- 20 224 Shen. K. Song. X. Tan. D. Li. W. Lu. Zhuang. Hugginggpt. Solv- ing ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv.2303.17580 2023 19. 20. 33 225 Liang. C. Wu. T. Song. W. Wu. Xia. Liu. Ou. S. Lu. L. Ji. S. Mao. et al. Taskmatrix. ai. Completing tasks by connecting foun- dation models with millions of apis. arXiv preprint arXiv.2303.16434 2023 19",
    "chunk_index": 494,
    "start_pos": 221808,
    "end_pos": 222241,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 484,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 484,
      "optimized_content_length": 446
    }
  },
  "1204": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_495",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "apis. arXiv preprint arXiv.2303.16434 2023 19 226 D. Suri s. S. Menon. C. ondrick. Vipergpt. Visual inference via python execution for reasoning. arXiv preprint arXiv.2303.08128 2023 20 227 A. Maedche. S. Morana. S. Schacht. D. Werth. J. Krumeich. Advanced user assistance systems. Business Information Systems Engineering 58 2016 367 370. 20 228 M. Campbell. A. J. Hoane Jr. F.-h. Hsu. Deep blue. Artificial intelligence 134 1-2 2002 57 83. 20 229 S. Hong. X. Zheng. J. Chen. Cheng. J. Wang. C. Zhang. Z. Wang.",
    "chunk_index": 495,
    "start_pos": 222242,
    "end_pos": 222731,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 511
    }
  },
  "1205": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_496",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "g. J. Chen. Cheng. J. Wang. C. Zhang. Z. Wang. S. K. S. Yau. Z. Lin. L. Zhou. et al. Metagpt. Meta programming for multi-agent collaborative framework. arXiv preprint arXiv.2308.00352 2023 20 230 Z. Xi. W. Chen. X. Guo. W. He. Ding. B. Hong. M. Zhang. J. Wang. S. Jin. E. Zhou. et al. The rise and potential of large language model based agents. survey. arXiv preprint arXiv.2309.07864 2023 20 231 L. Wang. C. Ma. X. Feng. Z. Zhang. H. Yang. J. Zhang. Z. Chen. J. Tang.",
    "chunk_index": 496,
    "start_pos": 222732,
    "end_pos": 223172,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 469
    }
  },
  "1206": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_497",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng. Z. Zhang. H. Yang. J. Zhang. Z. Chen. J. Tang. X. Chen. Lin. et al. survey on large language model based au- tonomous agents. arXiv preprint arXiv.2308.11432 2023 20 232 W. Huang. P. Abbeel. D. Pathak. I. Mordatch. Language models as zero- shot planners. Extracting actionable knowledge for embodied agents. in. International Conference on Machine Learning. PMLR. 2022. pp. 9118 9147. 20 233 S. Hao. Gu. H. Ma. J. J. Hong. Z. Wang. D. Z. Wang. Z. Hu. Reason- ing with language model is planning with world model. arXiv preprint",
    "chunk_index": 497,
    "start_pos": 223173,
    "end_pos": 223671,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 95,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 531
    }
  },
  "1207": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_498",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "model is planning with world model. arXiv preprint arXiv.2305.14992 2023 20. 33 234 W. Yao. S. Heinecke. J. C. Niebles. Z. Liu. Feng. L. Xue. R. Murthy. Z. Chen. J. Zhang. D. Arpit. et al. Retroformer. Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv.2308.02151 2023 20. 33 235 W. Huang. F. Xia. T. Xiao. H. Chan. J. Liang. P. Florence. A. Zeng. J. Tompson. I. Mordatch. Chebotar. P. Sermanet. T. Jackson. N. Brown. L. Luu. S. Levine. K. Hausman. brian ichter. Inner mono-",
    "chunk_index": 498,
    "start_pos": 223672,
    "end_pos": 224155,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 515
    }
  },
  "1208": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_499",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "S. Levine. K. Hausman. brian ichter. Inner mono- logue. Embodied reasoning through planning with language models. in. 6th Annual Conference on Robot Learning. 2022. URL https. openreview.net forum.id 3R3Pz5i0tye 20 236 C. Jin. W. Tan. J. Yang. B. Liu. R. Song. L. Wang. J. Fu. Alphablock. Embodied finetuning for vision-language reasoning in robot manipula- tion. arXiv preprint arXiv.2305.18898 2023 20. 33 237 I. Singh. Blukis. A. Mousavian. A. Goyal. D. Xu. J. Tremblay. D. Fox.",
    "chunk_index": 499,
    "start_pos": 224156,
    "end_pos": 224600,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 481
    }
  },
  "1209": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_500",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Mousavian. A. Goyal. D. Xu. J. Tremblay. D. Fox. J. Thomason. A. Garg. Progprompt. Generating situated robot task plans using large language models. in. 2023 IEEE International Conference on Robotics and Automation ICRA IEEE. 2023. pp. 11523 11530. 20. 33 238 W. Yu. N. Gileadi. C. Fu. S. Kirmani. K.-H. Lee. M. G. Arenas. H.-T. L. Chiang. T. Erez. L. Hasenclever. J. Humplik. et al. Language to rewards for robotic skill synthesis. arXiv preprint arXiv.2306.08647 2023 20",
    "chunk_index": 500,
    "start_pos": 224601,
    "end_pos": 225033,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 483,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 483,
      "optimized_content_length": 472
    }
  },
  "1210": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_501",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "thesis. arXiv preprint arXiv.2306.08647 2023 20 239 X. Tang. A. Zou. Z. Zhang. Zhao. X. Zhang. A. Cohan. M. Gerstein. Medagents. Large language models as collaborators for zero-shot med- ical reasoning. arXiv preprint arXiv.2311.10537 2023 20 240 A. Brohan. Chebotar. C. Finn. K. Hausman. A. Herzog. D. Ho. J. Ibarz. A. Irpan. E. Jang. R. Julian. et al. Do as can. not as say. Grounding language in robotic ffordances. in. Conference on Robot Learning. PMLR. 2023. pp. 287 318. 20. 33",
    "chunk_index": 501,
    "start_pos": 225034,
    "end_pos": 225492,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 484
    }
  },
  "1211": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_502",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "on Robot Learning. PMLR. 2023. pp. 287 318. 20. 33 241 H. Ha. P. Florence. S. Song. Scaling up and distilling down. Language- guided robot skill acquisition. arXiv preprint arXiv.2307.14535 2023 20 242 A. Rajvanshi. K. Sikka. X. Lin. B. Lee. H.-P. Chiu. A. Velasquez. Say- nav. Grounding large language models for dynamic planning to navi- gation in new environments. arXiv preprint arXiv.2309.04077 2023 20 243 C. H. Song. J. Wu. C. Washington. B. M. Sadler. W.-L. Chao. Su.",
    "chunk_index": 502,
    "start_pos": 225493,
    "end_pos": 225933,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 475
    }
  },
  "1212": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_503",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "C. Washington. B. M. Sadler. W.-L. Chao. Su. Llm-planner. Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv.2212.04088 2022 20 244 S. Dorbala. J. F. Mullen Jr. D. Manocha. Can an embodied agent findyour cat-shaped mug llm-based zero-shot object navigation. arXiv preprint arXiv.2303.03480 2023 20 245 C. Huang. O. Mees. A. Zeng. W. Burgard. Visual language maps for robot navigation. in. 2023 IEEE International Conference on Robotics",
    "chunk_index": 503,
    "start_pos": 225934,
    "end_pos": 226386,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 503,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 503,
      "optimized_content_length": 480
    }
  },
  "1213": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_504",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "in. 2023 IEEE International Conference on Robotics and Automation ICRA IEEE. 2023. pp. 10608 10615. 20 246 Ding. X. Zhang. C. Paxton. S. Zhang. Task and motion planning with large language models for object rearrangement. arXiv preprint arXiv.2303.06247 2023 20. 33 247 X. Liu. Zheng. Z. Du. M. Ding. Qian. Z. Yang. J. Tang. Gpt under- stands. too. arXiv preprint arXiv.2103.10385 2021 20. 21 248 G. Chen. F. Liu. Z. Meng. S. Liang. Revisiting parameter-e fficient tun-",
    "chunk_index": 504,
    "start_pos": 226387,
    "end_pos": 226832,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 469
    }
  },
  "1214": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_505",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng. S. Liang. Revisiting parameter-e fficient tun- ing. Are we really there yet. arXiv preprint arXiv.2202.07962 2022 20 249 Wang. S. Mukherjee. X. Liu. J. Gao. A. H. Awadallah. J. Gao. Adamix. Mixture-of-adapter for parameter-e fficient tuning of large lan- guage models. arXiv preprint arXiv.2205.12410 2022 4. 20 250 E. J. Hu. Shen. P. Wallis. Z. Allen-Zhu. Li. S. Wang. L. Wang. W. Chen. Lora. Low-rank adaptation of large language models. arXiv preprint arXiv.2106.09685 2021 21. 22. 23",
    "chunk_index": 505,
    "start_pos": 226833,
    "end_pos": 227304,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 491
    }
  },
  "1215": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_506",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "arXiv preprint arXiv.2106.09685 2021 21. 22. 23 251 X. Liu. K. Ji. Fu. W. Tam. Z. Du. Z. Yang. J. Tang. P-tuning. Prompt tuning can be comparable to fine-tuning across scales and tasks. in. Pro- ceedings of the 60th Annual Meeting of the Association for Computa- tional Linguistics olume 2. Short Papers 2022. pp. 61 68. 21 252 A. Razdaibiedina. Mao. R. Hou. M. Khabsa. M. Lewis. A. Almahairi. Progressive prompts. Continual learning for language models. arXiv preprint arXiv.2301.12314 2023 21",
    "chunk_index": 506,
    "start_pos": 227305,
    "end_pos": 227771,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 494
    }
  },
  "1216": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_507",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2301.12314 2023 21 253 Z.-R. Zhang. C. Tan. H. Xu. C. Wang. J. Huang. S. Huang. To- wards adaptive prefix tuning for parameter-e fficient language model fine-tuning. arXiv preprint arXiv.2305.15212 2023 21 254 E. B. Zaken. S. Ravfogel. Goldberg. Bitfit. Simple parameter- efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv.2106.10199 2021 21 255 T. Dettmers. M. Lewis. Belkada. L. Zettlemoyer. Llm. int8",
    "chunk_index": 507,
    "start_pos": 227772,
    "end_pos": 228218,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 497,
      "optimized_content_length": 470
    }
  },
  "1217": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_508",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Lewis. Belkada. L. Zettlemoyer. Llm. int8 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv.2208.07339 2022 21. 22 256 E. Frantar. S. Ashkboos. T. Hoefler. D. Alistarh. Gptq. Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv.2210.17323 2022 21 257 X. Wei. Zhang. Li. X. Zhang. R. Gong. J. Guo. X. Liu. Outlier sup- pression Accurate quantization of large language models by equiva-",
    "chunk_index": 508,
    "start_pos": 228219,
    "end_pos": 228653,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 484,
      "optimized_content_length": 455
    }
  },
  "1218": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_509",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "quantization of large language models by equiva- lent and optimal shifting and scaling. arXiv preprint arXiv.2304.09145 2023 21 258 E. Frantar. D. Alistarh. Optimal brain compression. framework for accurate post-training quantization and pruning. Advances in Neural In- formation Processing Systems 35 2022 4475 4488. 21 259 C. Lee. J. Jin. T. Kim. H. Kim. E. Park. Owq. Lessons learned from ac- tivation outliers for weight quantization in large language models. arXiv preprint arXiv.2306.02272 2023 21",
    "chunk_index": 509,
    "start_pos": 228654,
    "end_pos": 229122,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 503
    }
  },
  "1219": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_510",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2306.02272 2023 21 260 S. J. Kwon. J. Kim. J. Bae. K. M. Yoo. J.-H. Kim. B. Park. B. Kim. J.- W. Ha. N. Sung. D. Lee. Alphatuning. Quantization-aware parameter- efficient adaptation of large-scale pre-trained language models. arXiv preprint arXiv.2210.03858 2022 21 261 T. Dettmers. A. Pagnoni. A. Holtzman. L. Zettlemoyer. Qlora. fficient finetuning of quantized llms. arXiv preprint arXiv.2305.14314 2023 21. 22 262 Z. Liu. B. Oguz. C. Zhao. E. Chang. P. Stock. Mehdad. Shi. R. Kr-",
    "chunk_index": 510,
    "start_pos": 229123,
    "end_pos": 229609,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 512
    }
  },
  "1220": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_511",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "o. E. Chang. P. Stock. Mehdad. Shi. R. Kr- ishnamoorthi. Chandra. Llm-qat. Data-free quantization aware train- ing for large language models. arXiv preprint arXiv.2305.17888 2023 21. 22 263 Guo. A. Yao. H. Zhao. Chen. Network sketching. Exploiting bi- nary structure in deep cnns. in. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. pp. 5955 5963. 21 264 J. Kim. J. H. Lee. S. Kim. J. Park. K. M. Yoo. S. J. Kwon. D. Lee.",
    "chunk_index": 511,
    "start_pos": 229610,
    "end_pos": 230044,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 458
    }
  },
  "1221": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_512",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "e. S. Kim. J. Park. K. M. Yoo. S. J. Kwon. D. Lee. Memory-e fficient fine-tuning of compressed large language models via sub-4-bit integer quantization. arXiv preprint arXiv.2305.14152 2023 22 265 M. Sun. Z. Liu. A. Bair. J. Z. Kolter. simple and ffective pruning approach for large language models. arXiv preprint arXiv.2306.11695 2023 22 266 Z. Wang. J. Wohlwend. T. Lei. Structured pruning of large language models. arXiv preprint arXiv.1910.04732 2019 22 41",
    "chunk_index": 512,
    "start_pos": 230045,
    "end_pos": 230472,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 478,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 478,
      "optimized_content_length": 461
    }
  },
  "1222": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_513",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "els. arXiv preprint arXiv.1910.04732 2019 22 41 --- Page 42 --- 267 L. Yin. Wu. Z. Zhang. C.-Y Hsieh. Wang. Jia. M. Pechenizkiy. Liang. Z. Wang. S. Liu. Outlier weighed layerwise sparsity owl missing secret sauce for pruning llms to high sparsity. arXiv preprint arXiv.2310.05175 2023 22 268 C. Tao. L. Hou. H. Bai. J. Wei. X. Jiang. Q. Liu. P. Luo. N. Wong. Structured pruning for fficient generative pre-trained language models. in. Findings of the Association for Computational Linguistics. ACL",
    "chunk_index": 513,
    "start_pos": 230474,
    "end_pos": 230955,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 497
    }
  },
  "1223": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_514",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "the Association for Computational Linguistics. ACL 2023. 2023. pp. 10880 10895. 22 269 J.-B. Alayrac. J. Donahue. P. Luc. A. Miech. I. Barr. Hasson. K. Lenc. A. Mensch. K. Millican. M. Reynolds. et al. Flamingo. visual lan- guage model for few-shot learning. Advances in Neural Information Pro- cessing Systems 35 2022 23716 23736. 22 270 J. Li. D. Li. S. Savarese. S. Hoi. Blip-2. Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv.2301.12597 2023 22",
    "chunk_index": 514,
    "start_pos": 230956,
    "end_pos": 231438,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 517
    }
  },
  "1224": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_515",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2301.12597 2023 22 271 H. Liu. C. Li. Q. Wu. J. Lee. Visual instruction tuning. arXiv preprint arXiv.2304.08485 2023 22 272 K. Li. He. Wang. Li. W. Wang. P. Luo. Wang. L. Wang. Qiao. Videochat. Chat-centric video understanding. arXiv preprint arXiv.2305.06355 2023 22 273 M. Maaz. H. Rasheed. S. Khan. F. S. Khan. Video-chatgpt. Towards de- tailed video understanding via large vision and language models. arXiv preprint arXiv.2306.05424 2023 22",
    "chunk_index": 515,
    "start_pos": 231439,
    "end_pos": 231904,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 474
    }
  },
  "1225": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_516",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2306.05424 2023 22 274 H. Zhang. X. Li. L. Bing. Video-llama. An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv.2306.02858 2023 22 275 X. Mei. C. Meng. H. Liu. Q. Kong. T. Ko. C. Zhao. M. D. Plumbley. Zou. W. Wang. Wavcaps. chatgpt-assisted weakly-labelled au- dio captioning dataset for audio-language multimodal research. arXiv preprint arXiv.2303.17395 2023 22 276 C. Lyu. M. Wu. L. Wang. X. Huang. B. Liu. Z. Du. S. Shi. Z. Tu. Macaw-",
    "chunk_index": 516,
    "start_pos": 231905,
    "end_pos": 232387,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 512
    }
  },
  "1226": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_517",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ng. X. Huang. B. Liu. Z. Du. S. Shi. Z. Tu. Macaw- llm. Multi-modal language modeling with image. audio. video. and text integration. arXiv preprint arXiv.2306.09093 2023 22 277 D. Zhu. J. Chen. X. Shen. X. Li. M. Elhoseiny. Minigpt-4. Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv.2304.10592 2023 22 278 A. Dosovitskiy. L. Beyer. A. Kolesnikov. D. Weissenborn. X. Zhai. T. Unterthiner. M. Dehghani. M. Minderer. G. Heigold. S. Gelly. et al.",
    "chunk_index": 517,
    "start_pos": 232388,
    "end_pos": 232841,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 493
    }
  },
  "1227": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_518",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "hghani. M. Minderer. G. Heigold. S. Gelly. et al. An image is worth 16x16 words. Transformers for image recognition at scale. arXiv preprint arXiv.2010.11929 2020 22 279 W. Dai. J. Li. D. Li. A. M. H. Tiong. J. Zhao. W. Wang. B. Li. P. Fung. S. Hoi. Instructblip. Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv.2305.06500 2023 22 280 Z. Xu. Shen. L. Huang. Multiinstruct. Improving multi-modal zero- shot learning via instruction tuning. arXiv preprint arXiv.2212.10773 2022 22",
    "chunk_index": 518,
    "start_pos": 232842,
    "end_pos": 233332,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 523
    }
  },
  "1228": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_519",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tuning. arXiv preprint arXiv.2212.10773 2022 22 281 Z. Zhao. L. Guo. T. Yue. S. Chen. S. Shao. X. Zhu. Z. Yuan. J. Liu. Chatbridge. Bridging modalities with large language model as lan- guage catalyst. arXiv preprint arXiv.2305.16103 2023 22 282 L. Li. Yin. S. Li. L. Chen. P. Wang. S. Ren. M. Li. Yang. J. Xu. X. Sun. et al. M3 it. large-scale dataset towards multi-modal multi- lingual instruction tuning. arXiv preprint arXiv.2306.04387 2023 22 283 R. Pi. J. Gao. S. Diao. R. Pan. H. Dong. J. Zhang. L. Yao. J. Han.",
    "chunk_index": 519,
    "start_pos": 233333,
    "end_pos": 233828,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 98,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 518
    }
  },
  "1229": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_520",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Diao. R. Pan. H. Dong. J. Zhang. L. Yao. J. Han. H. Xu. L. K. T. Zhang. Detgpt. Detect what you need via reasoning. arXiv preprint arXiv.2305.14167 2023 22 284 G. Luo. Zhou. T. Ren. S. Chen. X. Sun. R. Ji. Cheap and quick. Efficient vision-language instruction tuning for large language models. arXiv preprint arXiv.2305.15023 2023 22 285 R. Zhang. J. Han. A. Zhou. X. Hu. S. Yan. P. Lu. H. Li. P. Gao. Qiao. Llama-adapter. fficient fine-tuning of language models with zero-init",
    "chunk_index": 520,
    "start_pos": 233829,
    "end_pos": 234278,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 500,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 478
    }
  },
  "1230": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_521",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ient fine-tuning of language models with zero-init attention. arXiv preprint arXiv.2303.16199 2023 22 286 A. Radford. J. W. Kim. T. Xu. G. Brockman. C. McLeavey. I. Sutskever. Robust speech recognition via large-scale weak supervision. in. Inter- national Conference on Machine Learning. PMLR. 2023. pp. 28492 28518. 22 287 Z. Zhang. A. Zhang. M. Li. H. Zhao. G. Karypis. A. Smola. Multi- modal chain-of-thought reasoning in language models. arXiv preprint arXiv.2302.00923 2023 23",
    "chunk_index": 521,
    "start_pos": 234279,
    "end_pos": 234720,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 481
    }
  },
  "1231": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_522",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2302.00923 2023 23 288 J. Ge. H. Luo. S. Qian. Gan. J. Fu. S. Zhan. Chain of thought prompt tuning in vision language models. arXiv preprint arXiv.2304.07919 2023 23 289 C. Wu. S. Yin. W. Qi. X. Wang. Z. Tang. N. Duan. Visual chatgpt. Talk- ing. drawing and editing with visual foundation models. arXiv preprint arXiv.2303.04671 2023 23 290 Z. Yang. L. Li. J. Wang. K. Lin. E. Azarnasab. F. Ahmed. Z. Liu. C. Liu. M. Zeng. L. Wang. Mm-react. Prompting chatgpt for multimodal rea-",
    "chunk_index": 522,
    "start_pos": 234721,
    "end_pos": 235196,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 508
    }
  },
  "1232": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_523",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "g. Mm-react. Prompting chatgpt for multimodal rea- soning and action. arXiv preprint arXiv.2303.11381 2023 23 291 T. Wang. J. Zhang. J. Fei. Ge. H. Zheng. Tang. Z. Li. M. Gao. S. Zhao. Shan. et al. Caption anything. Interactive image descrip- tion with diverse multimodal controls. arXiv preprint arXiv.2305.02677 2023 23 292 X. Zhu. R. Zhang. B. He. Z. Zeng. S. Zhang. P. Gao. Pointclip v2. Adapting clip for powerful 3d open-world learning. arXiv preprint arXiv.2211.11682 2022 23",
    "chunk_index": 523,
    "start_pos": 235197,
    "end_pos": 235654,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 482
    }
  },
  "1233": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_524",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "arning. arXiv preprint arXiv.2211.11682 2022 23 293 T. Gupta. A. Kembhavi. Visual programming. Compositional visual rea- soning without training. in. Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. 2023. pp. 14953 14962. 23 294 P. Gao. Z. Jiang. H. You. P. Lu. S. C. Hoi. X. Wang. H. Li. Dynamic fusion with intra-and inter-modality attention flow for visual question answering. in. Proceedings of the IEEE CVF conference on computer vision and pattern recognition. 2019. pp. 6639 6648. 23",
    "chunk_index": 524,
    "start_pos": 235655,
    "end_pos": 236137,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 524
    }
  },
  "1234": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_525",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "and pattern recognition. 2019. pp. 6639 6648. 23 295 Z. Yu. J. Yu. Cui. D. Tao. Q. Tian. Deep modular co-attention net- works for visual question answering. in. Proceedings of the IEEE CVF conference on computer vision and pattern recognition. 2019. pp. 6281 6290. 23 296 H. You. R. Sun. Z. Wang. L. Chen. G. Wang. H. A. Ayyubi. K.- W. Chang. S.-F. Chang. Idealgpt. Iteratively decomposing vision and language reasoning via large language models. arXiv preprint arXiv.2305.14985 2023 23",
    "chunk_index": 525,
    "start_pos": 236138,
    "end_pos": 236588,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 486
    }
  },
  "1235": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_526",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2305.14985 2023 23 297 R. Zhang. X. Hu. B. Li. S. Huang. H. Deng. Qiao. P. Gao. H. Li. Prompt. generate. then cache. Cascade of foundation models makes strong few-shot learners. in. Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. 2023. pp. 15211 15222. 23 298 T. Q. Nguyen. J. Salazar. Transformers without tears. Improving the normalization of self-attention. CoRR abs 1910.05895 2019 24 299 Liu. M. Ott. N. Goyal. J. Du. M. Joshi. D. Chen. O. Levy. M. Lewis.",
    "chunk_index": 526,
    "start_pos": 236589,
    "end_pos": 237084,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 524
    }
  },
  "1236": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_527",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "oyal. J. Du. M. Joshi. D. Chen. O. Levy. M. Lewis. L. Zettlemoyer. Stoyanov. Roberta. robustly optimized bert pre- training approach. arXiv preprint arXiv.1907.11692 2019 24. 30 300 X. Geng. A. Gudibande. H. Liu. E. Wallace. P. Abbeel. S. Levine. D. Song. Koala. dialogue model for academic research. Blog post April 2023 URL https. bair.berkeley.edu blog 2023 04 03 koala 25 301 L. Gao. S. Biderman. S. Black. L. Golding. T. Hoppe. C. Foster. J. Phang. H. He. A. Thite. N. Nabeshima. et al. The pile. An",
    "chunk_index": 527,
    "start_pos": 237085,
    "end_pos": 237559,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 525,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 504
    }
  },
  "1237": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_528",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "He. A. Thite. N. Nabeshima. et al. The pile. An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv.2101.00027 2020 28. 30 302 H. Laurenc on. L. Saulnier. T. Wang. C. Akiki. A. Villanova del Moral. T. Le Scao. L. on Werra. C. Mou. E. Gonza lez Ponferrada. H. Nguyen. et al. The bigscience roots corpus. 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems 35 2022 31809 31826. 28 303 Wikipedia. URL https. en.wikipedia.org wiki Main_Page 28",
    "chunk_index": 528,
    "start_pos": 237560,
    "end_pos": 238023,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 498
    }
  },
  "1238": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_529",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ia. URL https. en.wikipedia.org wiki Main_Page 28 304 Together Computer. Redpajama. An open source recipe to reproduce llama training dataset Apr. 2023 URL https. github.com togethercomputer RedPajama-Data 28 305 O. Honovich. T. Scialom. O. Levy. T. Schick. Unnatural instructions. Tuning language models with almost no human labor. arXiv preprint arXiv.2212.09689 2022 28 306 Bai. A. Jones. K. Ndousse. A. Askell. A. Chen. N. DasSarma. D. Drain. S. Fort. D. Ganguli. T. Henighan. et al. Training helpful and",
    "chunk_index": 529,
    "start_pos": 238024,
    "end_pos": 238505,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 508
    }
  },
  "1239": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_530",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nguli. T. Henighan. et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv.2204.05862 2022 28 307 D. Hendrycks. C. Burns. S. Basart. A. Zou. M. Mazeika. D. Song. J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv.2009.03300 2020 26. 29 308 A. Srivastava. A. Rastogi. A. Rao. A. A. M. Shoeb. A. Abid. A. Fisch. A. R. Brown. A. Santoro. A. Gupta. A. Garriga-Alonso. et al. Beyond 42",
    "chunk_index": 530,
    "start_pos": 238506,
    "end_pos": 238948,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 493,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 493,
      "optimized_content_length": 479
    }
  },
  "1240": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_531",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ro. A. Gupta. A. Garriga-Alonso. et al. Beyond 42 --- Page 43 --- the imitation game. Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv.2206.04615 2022 26. 29 309 A. Wang. A. Singh. J. Michael. F. Hill. O. Levy. S. R. Bowman. Glue. multi-task benchmark and analysis platform for natural language un- derstanding. arXiv preprint arXiv.1804.07461 2018 26. 29 310 Yao. Q. Dong. J. Guan. B. Cao. Z. Zhang. C. Xiao. X. Wang. F. Qi.",
    "chunk_index": 531,
    "start_pos": 238950,
    "end_pos": 239380,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 481,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 464
    }
  },
  "1241": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_532",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Guan. B. Cao. Z. Zhang. C. Xiao. X. Wang. F. Qi. J. Bao. J. Nie. et al. Cuge. chinese language understanding and gen- eration evaluation benchmark. arXiv preprint arXiv.2112.13610 2021 29 311 L. Xu. H. Hu. X. Zhang. L. Li. C. Cao. Li. Xu. K. Sun. D. Yu. C. Yu. et al. Clue. chinese language understanding evaluation bench- mark. arXiv preprint arXiv.2004.05986 2020 29 312 L. Xu. X. Lu. C. Yuan. X. Zhang. H. Xu. H. Yuan. G. Wei. X. Pan. X. Tian. L. Qin. et al. Fewclue. chinese few-shot learning evaluation",
    "chunk_index": 532,
    "start_pos": 239381,
    "end_pos": 239866,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 100,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 507
    }
  },
  "1242": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_533",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Fewclue. chinese few-shot learning evaluation benchmark. arXiv preprint arXiv.2107.07498 2021 29 313 E. M. Smith. M. Williamson. K. Shuster. J. Weston. .-L. Boureau. Can you put it all together. Evaluating conversational agents ability to blend skills. arXiv preprint arXiv.2004.08449 2020 29 314 P. Liang. R. Bommasani. T. Lee. D. Tsipras. D. Soylu. M. Yasunaga. Zhang. D. Narayanan. Wu. A. Kumar. et al. Holistic evaluation of language models. arXiv preprint arXiv.2211.09110 2022 29",
    "chunk_index": 533,
    "start_pos": 239867,
    "end_pos": 240331,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 515,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 485
    }
  },
  "1243": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_534",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2211.09110 2022 29 315 S. Park. J. Moon. S. Kim. W. I. Cho. J. Han. J. Park. C. Song. J. Kim. Song. T. Oh. et al. Klue. Korean language understanding evaluation. arXiv preprint arXiv.2105.09680 2021 29 316 S. Reddy. D. Chen. C. D. Manning. Coqa. conversational question answering challenge. Transactions of the Association for Computational Linguistics 2019 249 266. 27. 29 317 M. T. Pilehvar. J. Camacho-Collados. Wic. 10.000 example",
    "chunk_index": 534,
    "start_pos": 240332,
    "end_pos": 240767,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 486,
      "optimized_content_length": 463
    }
  },
  "1244": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_535",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Pilehvar. J. Camacho-Collados. Wic. 10.000 example pairs for evaluating context-sensitive representations. arXiv preprint arXiv.1808.09121 2018 27. 29 318 S. Merity. C. Xiong. J. Bradbury. R. Socher. Pointer sentinel mixture models. arXiv preprint arXiv.1609.07843 2016 28. 29 319 J. W. Rae. A. Potapenko. S. M. Jayakumar. T. P. Lillicrap. Compres- sive transformers for long-range sequence modelling. arXiv preprint arXiv.1911.05507 2019 28. 29 320 X. Liu. Q. Chen. C. Deng. H. Zeng. J. Chen. D. Li. B. Tang. Lcqmc.",
    "chunk_index": 535,
    "start_pos": 240768,
    "end_pos": 241252,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 516
    }
  },
  "1245": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_536",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Deng. H. Zeng. J. Chen. D. Li. B. Tang. Lcqmc. large-scale chinese question matching corpus. in. Proceedings of the 27th international conference on computational linguistics. 2018. pp. 1952 1962. 28. 29 321 S. Iyer. N. Dandekar. K. Csernai. First quora dataset re- lease. Question pairs. https. quoradata.quora.com First-Quora-Dataset-Release-Question-Pairs 29 322 R. Rudinger. J. Naradowsky. B. Leonard. B. Van Durme. Gender bias in coreference resolution. arXiv preprint arXiv.1804.09301 2018 29",
    "chunk_index": 536,
    "start_pos": 241253,
    "end_pos": 241715,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 498
    }
  },
  "1246": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_537",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "lution. arXiv preprint arXiv.1804.09301 2018 29 323 M.-C. De Marne ffe. M. Simons. J. Tonhauser. The commitmentbank. In- vestigating projection in naturally occurring discourse. in. proceedings of Sinn und Bedeutung. ol. 23. 2019. pp. 107 124. 29 324 Z. Li. N. Ding. Z. Liu. H. Zheng. Shen. Chinese relation extraction with multi-grained information and external linguistic knowledge. in. Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics. 2019. pp. 4377 4386. 29",
    "chunk_index": 537,
    "start_pos": 241716,
    "end_pos": 242181,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 503
    }
  },
  "1247": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_538",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "mpu- tational Linguistics. 2019. pp. 4377 4386. 29 325 J. Xu. J. Wen. X. Sun. Q. Su. discourse-level named entity recognition and relation extraction dataset for chinese literature text. arXiv preprint arXiv.1711.07010 2017 29 326 J. Chen. Q. Chen. X. Liu. H. Yang. D. Lu. B. Tang. The bq corpus. large-scale domain-specific chinese corpus for sentence semantic equiv- alence identification. in. Proceedings of the 2018 conference on empiri- cal methods in natural language processing. 2018. pp. 4946 4951. 29",
    "chunk_index": 538,
    "start_pos": 242182,
    "end_pos": 242651,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 509
    }
  },
  "1248": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_539",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tural language processing. 2018. pp. 4946 4951. 29 327 B. Liu. D. Niu. H. Wei. J. Lin. He. K. Lai. Xu. Matching arti- cle pairs with graphical decomposition and convolutions. arXiv preprint arXiv.1802.07459 2018 29 328 P. Li. W. Li. Z. He. X. Wang. Cao. J. Zhou. W. Xu. Dataset and neu- ral recurrent sequence labeling model for open-domain factoid question answering. arXiv preprint arXiv.1607.06275 2016 29 329 N. Peng. M. Dredze. Named entity recognition for chinese social media",
    "chunk_index": 539,
    "start_pos": 242652,
    "end_pos": 243107,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 482
    }
  },
  "1249": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_540",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Named entity recognition for chinese social media with jointly trained embeddings. in. Proceedings of the 2015 conference on empirical methods in natural language processing. 2015. pp. 548 554. 29 330 W. Ling. D. Yogatama. C. Dyer. P. Blunsom. Program induction by ratio- nale generation. Learning to solve and explain algebraic word problems. arXiv preprint arXiv.1705.04146 2017 29 331 R. Weischedel. S. Pradhan. L. Ramshaw. M. Palmer. N. Xue. M. Mar- cus. A. Taylor. C. Greenberg. E. Hovy. R. Belvin. et al. Ontonotes re-",
    "chunk_index": 540,
    "start_pos": 243108,
    "end_pos": 243590,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 524
    }
  },
  "1250": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_541",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "eenberg. E. Hovy. R. Belvin. et al. Ontonotes re- lease 4.0. LDC2011T03. Philadelphia. Penn. Linguistic Data Consor- tium 2011 29 332 D. Vilares. C. Go mez-Rodri guez. Head-qa. healthcare dataset for complex reasoning. arXiv preprint arXiv.1906.04701 2019 29 333 S. L. Blodgett. L. Green. B. Connor. Demographic dialectal variation in social media. case study of african-american english. arXiv preprint arXiv.1608.08868 2016 29 334 N. Mostafazadeh. N. Chambers. X. He. D. Parikh. D. Batra. L. Van-",
    "chunk_index": 541,
    "start_pos": 243591,
    "end_pos": 244059,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 498
    }
  },
  "1251": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_542",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "N. Chambers. X. He. D. Parikh. D. Batra. L. Van- derwende. P. Kohli. J. Allen. corpus and evaluation framework for deeper understanding of commonsense stories. arXiv preprint arXiv.1604.01696 2016 28. 29 335 D. Paperno. G. Kruszewski. A. Lazaridou. Q. N. Pham. R. Bernardi. S. Pezzelle. M. Baroni. G. Boleda. R. Ferna ndez. The lambada dataset. Word prediction requiring broad discourse context. arXiv preprint arXiv.1606.06031 2016 28. 29 336 B. Hu. Q. Chen. F. Zhu. Lcsts. large scale chinese short text summa-",
    "chunk_index": 542,
    "start_pos": 244060,
    "end_pos": 244538,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 512
    }
  },
  "1252": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_543",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "hu. Lcsts. large scale chinese short text summa- rization dataset. arXiv preprint arXiv.1506.05865 2015 29 337 Z. Shao. M. Huang. J. Wen. W. Xu. X. Zhu. Long and diverse text gener- ation with planning-based hierarchical variational model. arXiv preprint arXiv.1908.06605 2019 29 338 J. Novikova. O. Dus ek. Rieser. The e2e dataset. New challenges for end-to-end generation. arXiv preprint arXiv.1706.09254 2017 29 339 C. Zheng. M. Huang. A. Sun. Chid. large-scale chinese idiom dataset",
    "chunk_index": 543,
    "start_pos": 244539,
    "end_pos": 244996,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 486
    }
  },
  "1253": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_544",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "A. Sun. Chid. large-scale chinese idiom dataset for cloze test. arXiv preprint arXiv.1906.01265 2019 29 340 Bisk. R. Zellers. J. Gao. Choi. et al. Piqa. Reasoning about phys- ical commonsense in natural language. in. Proceedings of the AAAI conference on artificial intelligence. ol. 34. 2020. pp. 7432 7439. 28. 29 341 M. Joshi. E. Choi. D. S. Weld. L. Zettlemoyer. Triviaqa. large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv.1705.03551 2017 28. 29. 31",
    "chunk_index": 544,
    "start_pos": 244997,
    "end_pos": 245474,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 502
    }
  },
  "1254": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_545",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "arXiv preprint arXiv.1705.03551 2017 28. 29. 31 342 P. Clark. I. Cowhey. O. Etzioni. T. Khot. A. Sabharwal. C. Schoenick. O. Tafjord. Think you have solved question answering. try arc. the ai2 reasoning challenge. arXiv preprint arXiv.1803.05457 2018 28. 29. 31 343 S. Aroca-Ouellette. C. Paik. A. Roncone. K. Kann. Prost. Phys- ical reasoning of objects through space and time. arXiv preprint arXiv.2106.03634 2021 29 344 T. Mihaylov. P. Clark. T. Khot. A. Sabharwal. Can suit of armor con-",
    "chunk_index": 545,
    "start_pos": 245475,
    "end_pos": 245932,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 491
    }
  },
  "1255": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_546",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "k. T. Khot. A. Sabharwal. Can suit of armor con- duct electricity. new dataset for open book question answering. arXiv preprint arXiv.1809.02789 2018 29 345 T. C. Ferreira. C. Gardent. N. Ilinykh. C. Van Der Lee. S. Mille. D. Moussallem. A. Shimorina. The 2020 bilingual. bi-directional webnlg shared task overview and evaluation results webnlg 2020 in. Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web WebNLG 2020. 29",
    "chunk_index": 546,
    "start_pos": 245933,
    "end_pos": 246369,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 487,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 487,
      "optimized_content_length": 468
    }
  },
  "1256": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_547",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "eration from the Semantic Web WebNLG 2020. 29 346 C. Xu. W. Zhou. T. Ge. K. Xu. J. McAuley. F. Wei. Blow the dog whistle. chinese dataset for cant understanding with common sense and world knowledge. arXiv preprint arXiv.2104.02704 2021 29 347 G. Lai. Q. Xie. H. Liu. Yang. E. Hovy. Race. Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv.1704.04683 2017 29 348 E. Choi. H. He. M. Iyyer. M. Yatskar. W.-t. Yih. Choi. P. Liang.",
    "chunk_index": 547,
    "start_pos": 246370,
    "end_pos": 246804,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 458
    }
  },
  "1257": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_548",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Iyyer. M. Yatskar. W.-t. Yih. Choi. P. Liang. L. Zettlemoyer. Quac. Question answering in context. arXiv preprint arXiv.1808.07036 2018 29 349 M. Geva. D. Khashabi. E. Segal. T. Khot. D. Roth. J. Berant. Did aristo- tle use laptop. question answering benchmark with implicit reason- ing strategies. Transactions of the Association for Computational Lin- guistics 2021 346 361. 29. 31 350 J. Boyd-Graber. B. Satino ff. H. He. H. Daume III. Besting the quiz mas-",
    "chunk_index": 548,
    "start_pos": 246805,
    "end_pos": 247234,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 479,
      "optimized_content_length": 460
    }
  },
  "1258": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_549",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ino ff. H. He. H. Daume III. Besting the quiz mas- ter. Crowdsourcing incremental classification games. in. Proceedings of the 2012 joint conference on empirical methods in natural language pro- cessing and computational natural language learning. 2012. pp. 1290 1301. 29 351 S. Zhang. X. Zhang. H. Wang. J. Cheng. P. Li. Z. Ding. Chinese medical question answer matching using end-to-end character-level multi-scale cnns. Applied Sciences 2017 767. 29 352 S. Zhang. X. Zhang. H. Wang. L. Guo. S. Liu. Multi-scale attentive in-",
    "chunk_index": 549,
    "start_pos": 247235,
    "end_pos": 247724,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 527
    }
  },
  "1259": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_550",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "H. Wang. L. Guo. S. Liu. Multi-scale attentive in- teraction networks for chinese medical question answer selection. IEEE 43",
    "chunk_index": 550,
    "start_pos": 247725,
    "end_pos": 247798,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 124,
      "word_count": 19,
      "optimized_for_embedding": true,
      "original_content_length": 124,
      "optimized_content_length": 124
    }
  },
  "1260": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_551",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "chinese medical question answer selection. IEEE 43 --- Page 44 --- Access 2018 74061 74071. 29 353 C. Xu. J. Pei. H. Wu. Liu. C. Li. Matinf. jointly labeled large-scale dataset for classification. question answering and summarization. arXiv preprint arXiv.2004.12302 2020 29 354 K. Sakaguchi. R. L. Bras. C. Bhagavatula. Choi. Winogrande. An adversarial winograd schema challenge at scale. Communications of the ACM 64 2021 99 106. 27. 29 355 R. Zellers. A. Holtzman. Bisk. A. Farhadi. Choi. Hellaswag. Can",
    "chunk_index": 551,
    "start_pos": 247800,
    "end_pos": 248294,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 506
    }
  },
  "1261": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_552",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Bisk. A. Farhadi. Choi. Hellaswag. Can machine really finish your sentence. arXiv preprint arXiv.1905.07830 2019 29 356 M. Roemmele. C. A. Bejan. A. S. Gordon. Choice of plausible alter- natives. An evaluation of commonsense causal reasoning. in. AAAI spring symposium. logical formalizations of commonsense reasoning. 2011. pp. 90 95. 29 357 H. Levesque. E. Davis. L. Morgenstern. The winograd schema chal- lenge. in. Thirteenth international conference on the principles of knowl- edge representation and reasoning. 2012. 27. 29",
    "chunk_index": 552,
    "start_pos": 248295,
    "end_pos": 248795,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 530
    }
  },
  "1262": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_553",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "l- edge representation and reasoning. 2012. 27. 29 358 A. Talmor. J. Herzig. N. Lourie. J. Berant. Commonsenseqa. question answering challenge targeting commonsense knowledge. arXiv preprint arXiv.1811.00937 2018 29. 31 359 M. Sap. H. Rashkin. D. Chen. R. LeBras. Choi. Socialiqa. Commonsense reasoning about social interactions. arXiv preprint arXiv.1904.09728 2019 29 360 K. Sun. D. Yu. D. Yu. C. Cardie. Investigating prior knowledge for chal- lenging chinese machine reading comprehension. Transactions of the",
    "chunk_index": 553,
    "start_pos": 248796,
    "end_pos": 249276,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 513
    }
  },
  "1263": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_554",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "machine reading comprehension. Transactions of the Association for Computational Linguistics 2020 141 155. 29 361 S. Zhang. X. Liu. J. Liu. J. Gao. K. Duh. B. Van Durme. Record. Bridg- ing the gap between human and machine commonsense reading compre- hension. arXiv preprint arXiv.1810.12885 2018 29 362 P. Rajpurkar. J. Zhang. K. Lopyrev. P. Liang. Squad. 100.000 questions for machine comprehension of text. arXiv preprint arXiv.1606.05250 2016 29. 31 363 C. Clark. K. Lee. M.-W. Chang. T. Kwiatkowski. M. Collins.",
    "chunk_index": 554,
    "start_pos": 249277,
    "end_pos": 249759,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 516
    }
  },
  "1264": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_555",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "K. Lee. M.-W. Chang. T. Kwiatkowski. M. Collins. K. Toutanova. Boolq. Exploring the surprising di fficulty of natural yes no questions. arXiv preprint arXiv.1905.10044 2019 29. 31 364 P. Rajpurkar. R. Jia. P. Liang. Know what you don know. Unanswer- able questions for squad. arXiv preprint arXiv.1806.03822 2018 29. 31 365 D. Dua. Wang. P. Dasigi. G. Stanovsky. S. Singh. M. Gardner. Drop. reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv.1903.00161 2019 29. 31",
    "chunk_index": 555,
    "start_pos": 249760,
    "end_pos": 250244,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 512
    }
  },
  "1265": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_556",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "hs. arXiv preprint arXiv.1903.00161 2019 29. 31 366 I. Dagan. O. Glickman. B. Magnini. The pascal recognising textual en- tailment challenge. in. Machine learning challenges workshop. Springer. 2005. pp. 177 190. 29. 31 367 Chang. M. Narang. H. Suzuki. G. Cao. J. Gao. Bisk. Webqa. Mul- tihop and multimodal qa. in. Proceedings of the IEEE CVF Conference on Computer Vision and Pattern Recognition. 2022. pp. 16495 16504. 29. 31 368 Cui. T. Liu. Z. Chen. W. Ma. S. Wang. G. Hu. Dataset for the first",
    "chunk_index": 556,
    "start_pos": 250245,
    "end_pos": 250715,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 499
    }
  },
  "1266": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_557",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Chen. W. Ma. S. Wang. G. Hu. Dataset for the first evaluation on chinese machine reading comprehension. arXiv preprint arXiv.1709.08299 2017 29 369 Cui. T. Liu. W. Che. L. Xiao. Z. Chen. W. Ma. S. Wang. G. Hu. span-extraction dataset for chinese machine reading comprehension. arXiv preprint arXiv.1810.07366 2018 29. 31 370 Cui. T. Liu. Z. Yang. Z. Chen. W. Ma. W. Che. S. Wang. G. Hu. sentence cloze dataset for chinese machine reading comprehension. arXiv preprint arXiv.2004.03116 2020 29",
    "chunk_index": 557,
    "start_pos": 250716,
    "end_pos": 251182,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 492
    }
  },
  "1267": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_558",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ension. arXiv preprint arXiv.2004.03116 2020 29 371 Li. T. Liu. D. Li. Q. Li. J. Shi. Wang. Character-based bilstm-crf incorporating pos and dictionaries for chinese opinion target extraction. in. Asian Conference on Machine Learning. PMLR. 2018. pp. 518 533. 29 372 D. Khashabi. S. Chaturvedi. M. Roth. S. Upadhyay. D. Roth. Look- ing beyond the surface. challenge set for reading comprehension over multiple sentences. in. Proceedings of the 2018 Conference of the",
    "chunk_index": 558,
    "start_pos": 251183,
    "end_pos": 251615,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 483,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 483,
      "optimized_content_length": 466
    }
  },
  "1268": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_559",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ces. in. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguis- tics. Human Language Technologies. olume Long Papers 2018. pp. 252 262. 29 373 T. Kwiatkowski. J. Palomaki. O. Redfield. M. Collins. A. Parikh. C. Al- berti. D. Epstein. I. Polosukhin. J. Devlin. K. Lee. et al. Natural ques- tions. benchmark for question answering research. Transactions of the",
    "chunk_index": 559,
    "start_pos": 251616,
    "end_pos": 251991,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 426,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 426,
      "optimized_content_length": 414
    }
  },
  "1269": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_560",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "question answering research. Transactions of the Association for Computational Linguistics 2019 453 466. 29 374 C. C. Shao. T. Liu. Lai. Tseng. S. Tsai. Drcd. chinese ma- chine reading comprehension dataset. arXiv preprint arXiv.1806.00920 2018 29 375 W. He. K. Liu. J. Liu. Lyu. S. Zhao. X. Xiao. Liu. Wang. H. Wu. Q. She. et al. Dureader. chinese machine reading comprehension dataset from real-world applications. arXiv preprint arXiv.1711.05073 2017 29",
    "chunk_index": 560,
    "start_pos": 251992,
    "end_pos": 252437,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 456
    }
  },
  "1270": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_561",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ations. arXiv preprint arXiv.1711.05073 2017 29 376 H. Tang. J. Liu. H. Li. Hong. H. Wu. H. Wang. Dureaderrobust. chinese dataset towards evaluating the robustness of machine reading comprehension models. arXiv preprint arXiv.2004.11142 2020 29 377 J. Welbl. N. F. Liu. M. Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv.1707.06209 2017 29 378 C. Xiong. Z. Dai. J. Callan. Z. Liu. R. Power. End-to-end neural ad-hoc ranking with kernel pooling. in. Proceedings of the 40th International",
    "chunk_index": 561,
    "start_pos": 252438,
    "end_pos": 252925,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 517
    }
  },
  "1271": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_562",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "pooling. in. Proceedings of the 40th International ACM SIGIR conference on research and development in information retrieval. 2017. pp. 55 64. 29 379 A. Pen as. E. Hovy. P. Forner. Rodrigo. R. Sutcli ffe. R. Morante. Qa4mre 2011-2013. Overview of question answering for machine read- ing evaluation. in. Information Access Evaluation. Multilinguality. Mul- timodality. and Visualization. 4th International Conference of the CLEF Initiative. CLEF 2013. Valencia. Spain. September 23-26. 2013. Pro- ceedings 4. Springer. 2013. pp. 303 320. 29",
    "chunk_index": 562,
    "start_pos": 252926,
    "end_pos": 253419,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 540
    }
  },
  "1272": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_563",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Pro- ceedings 4. Springer. 2013. pp. 303 320. 29 380 S. Lim. M. Kim. J. Lee. Korquad1. 0. Korean qa dataset for machine reading comprehension. arXiv preprint arXiv.1909.07005 2019 29 381 C. Xiao. H. Zhong. Z. Guo. C. Tu. Z. Liu. M. Sun. Feng. X. Han. Z. Hu. H. Wang. et al. Cail2018. large-scale legal dataset for judg- ment prediction. arXiv preprint arXiv.1807.02478 2018 29 382 D. Hendrycks. S. Basart. S. Kadavath. M. Mazeika. A. Arora. E. Guo. C. Burns. S. Puranik. H. He. D. Song. et al. Measuring coding challenge",
    "chunk_index": 563,
    "start_pos": 253420,
    "end_pos": 253911,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 94,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 520
    }
  },
  "1273": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_564",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "H. He. D. Song. et al. Measuring coding challenge competence with apps. arXiv preprint arXiv.2105.09938 2021 29. 31 383 Wang. X. Liu. S. Shi. Deep neural solver for math word problems. in. Proceedings of the 2017 conference on empirical methods in natural language processing. 2017. pp. 845 854. 29. 31 384 K. Cobbe. Kosaraju. M. Bavarian. M. Chen. H. Jun. L. Kaiser. M. Plappert. J. Tworek. J. Hilton. R. Nakano. et al. Training verifiers to solve math word problems. arXiv preprint arXiv.2110.14168 2021 29. 31",
    "chunk_index": 564,
    "start_pos": 253912,
    "end_pos": 254393,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 512
    }
  },
  "1274": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_565",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ms. arXiv preprint arXiv.2110.14168 2021 29. 31 385 J. Austin. A. Odena. M. I. Nye. M. Bosma. H. Michalewski. D. Dohan. E. Jiang. C. J. Cai. M. Terry. Q. Le. C. Sutton. Program synthesis with large language models. CoRR abs 2108.07732 2021 29 386 F. Shi. M. Suzgun. M. Freitag. X. Wang. S. Srivats. S. osoughi. H. W. Chung. Tay. S. Ruder. D. Zhou. et al. Language models are mul- tilingual chain-of-thought reasoners. arXiv preprint arXiv.2210.03057 2022 29",
    "chunk_index": 565,
    "start_pos": 254394,
    "end_pos": 254825,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 482,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 482,
      "optimized_content_length": 457
    }
  },
  "1275": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_566",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "soners. arXiv preprint arXiv.2210.03057 2022 29 387 S. Roy. D. Roth. Solving general arithmetic word problems. arXiv preprint arXiv.1608.01413 2016 29 388 S.-Y Miao. C.-C. Liang. K.-Y Su. diverse corpus for evaluating and developing english math word problem solvers. arXiv preprint arXiv.2106.15772 2021 29 389 R. Koncel-Kedziorski. S. Roy. A. Amini. N. Kushman. H. Hajishirzi. Mawps. math word problem repository. in. Proceedings of the 2016 conference of the north american chapter of the association for computa-",
    "chunk_index": 566,
    "start_pos": 254826,
    "end_pos": 255314,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 516
    }
  },
  "1276": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_567",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "american chapter of the association for computa- tional linguistics. human language technologies. 2016. pp. 1152 1157. 29 390 A. Patel. S. Bhattamishra. N. Goyal. Are nlp models really able to solve simple math word problems. arXiv preprint arXiv.2103.07191 2021 29 391 Lai. C. Li. Wang. T. Zhang. R. Zhong. L. Zettlemoyer. W.-t. Yih. D. Fried. S. Wang. T. Yu. Ds-1000. natural and reliable benchmark for data science code generation. in. International Conference on Machine Learning. PMLR. 2023. pp. 18319 18345. 29",
    "chunk_index": 567,
    "start_pos": 255315,
    "end_pos": 255800,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 516
    }
  },
  "1277": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_568",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Machine Learning. PMLR. 2023. pp. 18319 18345. 29 392 J. Austin. A. Odena. M. Nye. M. Bosma. H. Michalewski. D. Dohan. E. Jiang. C. Cai. M. Terry. Q. Le. et al. Program synthesis with large language models. arXiv preprint arXiv.2108.07732 2021 29 393 Nie. A. Williams. E. Dinan. M. Bansal. J. Weston. D. Kiela. Adver- sarial nli. new benchmark for natural language understanding. arXiv preprint arXiv.1910.14599 2019 29. 31 394 A. Williams. N. Nangia. S. R. Bowman. broad-coverage challenge",
    "chunk_index": 568,
    "start_pos": 255801,
    "end_pos": 256262,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 490
    }
  },
  "1278": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_569",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Nangia. S. R. Bowman. broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv.1704.05426 2017 29 395 R. T. McCoy. E. Pavlick. T. Linzen. Right for the wrong reasons. Diag- 44",
    "chunk_index": 569,
    "start_pos": 256263,
    "end_pos": 256438,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 226,
      "word_count": 34,
      "optimized_for_embedding": true,
      "original_content_length": 226,
      "optimized_content_length": 217
    }
  },
  "1279": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_570",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "T. Linzen. Right for the wrong reasons. Diag- 44 --- Page 45 --- nosing syntactic heuristics in natural language inference. arXiv preprint arXiv.1902.01007 2019 29 396 J. Liu. L. Cui. H. Liu. D. Huang. Wang. Zhang. Logiqa. chal- lenge dataset for machine reading comprehension with logical reason- ing. arXiv preprint arXiv.2007.08124 2020 29 397 P. Lewis. B. guz. R. Rinott. S. Riedel. H. Schwenk. Mlqa. Eval- uating cross-lingual extractive question answering. arXiv preprint arXiv.1910.07475 2019 29",
    "chunk_index": 570,
    "start_pos": 256440,
    "end_pos": 256919,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 502
    }
  },
  "1280": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_571",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "wering. arXiv preprint arXiv.1910.07475 2019 29 398 A. Conneau. G. Lample. R. Rinott. A. Williams. S. R. Bowman. H. Schwenk. Stoyanov. Xnli. Evaluating cross-lingual sentence rep- resentations. arXiv preprint arXiv.1809.05053 2018 29. 31 399 Yang. Zhang. C. Tar. J. Baldridge. Paws-x. cross- lingual adversarial dataset for paraphrase identification. arXiv preprint arXiv.1908.11828 2019 29. 31 400 S. Narayan. S. B. Cohen. M. Lapata. Don give me the details. just the",
    "chunk_index": 571,
    "start_pos": 256920,
    "end_pos": 257368,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 468
    }
  },
  "1281": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_572",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "en. M. Lapata. Don give me the details. just the summary. Topic-Aware Convolutional Neural Networks for Extreme Summarization. ArXiv. abs 1808 29 401 E. M. Ponti. G. Glavas O. Majewska. Q. Liu. I. Vuli c. A. Korhonen. Xcopa. multilingual dataset for causal commonsense reasoning. arXiv preprint arXiv.2005.00333 2020 29 402 A. Tikhonov. M. Ryabinin. It all in the heads. Using attention heads as baseline for cross-lingual transfer in commonsense reasoning. arXiv preprint arXiv.2106.12066 2021 29",
    "chunk_index": 572,
    "start_pos": 257369,
    "end_pos": 257839,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 497
    }
  },
  "1282": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_573",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "soning. arXiv preprint arXiv.2106.12066 2021 29 403 J. H. Clark. E. Choi. M. Collins. D. Garrette. T. Kwiatkowski. Niko- laev. J. Palomaki. Tydi qa. benchmark for information-seeking ques- tion answering in typologically diverse languages. Transactions of the Association for Computational Linguistics 2020 454 470. 29 404 T. Scialom. P.-A. Dray. S. Lamprier. B. Piwowarski. J. Staiano. Mlsum. The multilingual summarization corpus. arXiv preprint arXiv.2004.14900 2020 29",
    "chunk_index": 573,
    "start_pos": 257840,
    "end_pos": 258281,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 472
    }
  },
  "1283": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_574",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "corpus. arXiv preprint arXiv.2004.14900 2020 29 405 S. Lin. J. Hilton. O. Evans. Truthfulqa. Measuring how models mimic human falsehoods. arXiv preprint arXiv.2109.07958 2021 29. 32 406 I. Augenstein. C. Lioma. D. Wang. L. C. Lima. C. Hansen. C. Hansen. J. G. Simonsen. Multifc. real-world multi-domain dataset for evidence-based fact checking of claims. arXiv preprint arXiv.1909.03242 2019 29 407 J. Thorne. A. Vlachos. C. Christodoulopoulos. A. Mittal. Fever.",
    "chunk_index": 574,
    "start_pos": 258282,
    "end_pos": 258712,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 481,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 481,
      "optimized_content_length": 462
    }
  },
  "1284": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_575",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "lachos. C. Christodoulopoulos. A. Mittal. Fever. large-scale dataset for fact extraction and verification. arXiv preprint arXiv.1803.05355 2018 29 408 I. Mollas. Z. Chrysopoulou. S. Karlos. G. Tsoumakas. Ethos. an online hate speech detection dataset. arXiv preprint arXiv.2006.08328 2020 29. 32 409 M. Nadeem. A. Bethke. S. Reddy. Stereoset. Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv.2004.09456 2020 29. 32 410 A. Parrish. A. Chen. N. Nangia. Padmakumar. J. Phang. J. Thomp-",
    "chunk_index": 575,
    "start_pos": 258713,
    "end_pos": 259197,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 514
    }
  },
  "1285": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_576",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "en. N. Nangia. Padmakumar. J. Phang. J. Thomp- son. P. M. Htut. S. R. Bowman. Bbq. hand-built bias benchmark for question answering. arXiv preprint arXiv.2110.08193 2021 29 411 J. Zhao. T. Wang. M. Yatskar. Ordonez. K.-W. Chang. Gender bias in coreference resolution. Evaluation and debiasing methods. arXiv preprint arXiv.1804.06876 2018 29 412 N. Nangia. C. Vania. R. Bhalerao. S. R. Bowman. Crows-pairs. chal- lenge dataset for measuring social biases in masked language models. arXiv preprint arXiv.2010.00133 2020 29",
    "chunk_index": 576,
    "start_pos": 259198,
    "end_pos": 259693,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 521
    }
  },
  "1286": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_577",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2010.00133 2020 29 413 S. Gehman. S. Gururangan. M. Sap. Choi. N. A. Smith. Realtoxic- ityprompts. Evaluating neural toxic degeneration in language models. arXiv preprint arXiv.2009.11462 2020 29 414 D. Borkan. L. Dixon. J. Sorensen. N. Thain. L. Vasserman. Nuanced metrics for measuring unintended bias with real data for text classifica- tion. in. Companion proceedings of the 2019 world wide web confer- ence. 2019. pp. 491 500. 29 415 O. Bojar. R. Chatterjee. C. Federmann. Graham. B. Haddow.",
    "chunk_index": 577,
    "start_pos": 259694,
    "end_pos": 260188,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 525
    }
  },
  "1287": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_578",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Chatterjee. C. Federmann. Graham. B. Haddow. M. Huck. A. J. Yepes. P. Koehn. Logacheva. C. Monz. et al. Find- ings of the 2016 conference on machine translation. in. Proceedings of the First Conference on Machine Translation. olume 2. Shared Task Papers. 2016. pp. 131 198. 29 416 B. Loi c. B. Magdalena. B. Ond \u02c7rej. F. Christian. G. Yvette. G. Ro- man. H. Barry. H. Matthias. J. Eric. K. Tom. et al. Findings of the 2020 conference on machine translation wmt20 in. Proceedings of",
    "chunk_index": 578,
    "start_pos": 260189,
    "end_pos": 260637,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 481
    }
  },
  "1288": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_579",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "on machine translation wmt20 in. Proceedings of the Fifth Conference on Machine Translation. Association for Compu- tational Linguistics 2020. pp. 55. 29 417 W. Li. F. Qi. M. Sun. X. Yi. J. Zhang. Ccpm. chinese classical poetry matching dataset. arXiv preprint arXiv.2106.01979 2021 29 418 E. Dinan. S. Roller. K. Shuster. A. Fan. M. Auli. J. Weston. Wizard of wikipedia. Knowledge-powered conversational agents. arXiv preprint arXiv.1811.01241 2018 29 419 H. Rashkin. E. M. Smith. M. Li. .-L. Boureau. Towards empathetic",
    "chunk_index": 579,
    "start_pos": 260638,
    "end_pos": 261129,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 521
    }
  },
  "1289": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_580",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Smith. M. Li. .-L. Boureau. Towards empathetic open-domain conversation models. new benchmark and dataset. arXiv preprint arXiv.1811.00207 2018 29 420 E. Dinan. Logacheva. Malykh. A. Miller. K. Shuster. J. Urbanek. D. Kiela. A. Szlam. I. Serban. R. Lowe. et al. The second conversa- tional intelligence challenge convai2 in. The NeurIPS 18 Competi- tion. From Machine Learning to Intelligent Conversations. Springer. 2020. pp. 187 208. 29 421 H. Zhou. C. Zheng. K. Huang. M. Huang. X. Zhu. Kdconv. chinese",
    "chunk_index": 580,
    "start_pos": 261130,
    "end_pos": 261611,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 505
    }
  },
  "1290": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_581",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "eng. K. Huang. M. Huang. X. Zhu. Kdconv. chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation. arXiv preprint arXiv.2004.04100 2020 29 422 L. CO. Iflytek. multiple categories chinese text classifier. competition official website 2019 29 423 J. Baumgartner. S. Zannettou. B. Keegan. M. Squire. J. Blackburn. The pushshift reddit dataset. in. Proceedings of the international AAAI con- ference on web and social media. ol. 14. 2020. pp. 830 839. 30",
    "chunk_index": 581,
    "start_pos": 261612,
    "end_pos": 262057,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 480
    }
  },
  "1291": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_582",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "and social media. ol. 14. 2020. pp. 830 839. 30 424 A. Fan. Jernite. E. Perez. D. Grangier. J. Weston. M. Auli. Eli5. Long form question answering. arXiv preprint arXiv.1907.09190 2019 31 425 Wang. S. Mishra. P. Alipoormolabashi. Kordi. A. Mirzaei. A. Arunkumar. A. Ashok. A. S. Dhanasekaran. A. Naik. D. Stap. et al. Benchmarking generalization via in-context instructions on 1.600 lan- guage tasks. arXiv preprint arXiv.2204.07705 2022 31 426 T. Xie. C. H. Wu. P. Shi. R. Zhong. T. Scholak. M. Yasunaga. C.-S. Wu.",
    "chunk_index": 582,
    "start_pos": 262058,
    "end_pos": 262551,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 91,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 515
    }
  },
  "1292": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_583",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Shi. R. Zhong. T. Scholak. M. Yasunaga. C.-S. Wu. M. Zhong. P. Yin. S. I. Wang. et al. Unifiedskg. Unifying and multi- tasking structured knowledge grounding with text-to-text language mod- els. arXiv preprint arXiv.2201.05966 2022 31 427 Q. Ye. B. Lin. X. Ren. Crossfit. few-shot learning challenge for cross-task generalization in nlp. arXiv preprint arXiv.2104.08835 2021 31 428 Aribandi. Tay. T. Schuster. J. Rao. H. S. Zheng. S. Mehta. H. Zhuang. Q. Tran. D. Bahri. J. Ni. et al. Ext5. Towards extreme",
    "chunk_index": 583,
    "start_pos": 262552,
    "end_pos": 263042,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 93,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 506
    }
  },
  "1293": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_584",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "an. D. Bahri. J. Ni. et al. Ext5. Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv.2111.10952 2021 31 429 A. Williams. N. Nangia. S. Bowman. broad-coverage challenge cor- pus for sentence understanding through inference. in. Proceedings of the 2018 Conference of the North American Chapter of the Associ- ation for Computational Linguistics. Human Language Technologies. olume Long Papers Association for Computational Linguistics. New Orleans. Louisiana. 2018. pp. 1112 1122. doi.10.18653 v1 N18-1101",
    "chunk_index": 584,
    "start_pos": 263043,
    "end_pos": 263541,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 531
    }
  },
  "1294": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_585",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "2018. pp. 1112 1122. doi.10.18653 v1 N18-1101 URL https. aclanthology.org N18-1101 31 430 Zhang. J. Baldridge. L. He. PAWS. Paraphrase adversaries from word scrambling. in. Proceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics. Human Language Technologies. olume Long and Short Papers Associa- tion for Computational Linguistics. Minneapolis. Minnesota. 2019. pp. 1298 1308. doi.10.18653 v1 N19-1131 URL https. aclanthology.org N19-1131 32",
    "chunk_index": 585,
    "start_pos": 263542,
    "end_pos": 264014,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 501
    }
  },
  "1295": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_586",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "19-1131 URL https. aclanthology.org N19-1131 32 431 C. Qin. A. Zhang. Z. Zhang. J. Chen. M. Yasunaga. D. Yang. Is chat- GPT general-purpose natural language processing task solver. in. The 2023 Conference on Empirical Methods in Natural Language Process- ing. 2023. URL https. openreview.net forum.id u03xn1COsO 32 432 M. U. Hadi. R. Qureshi. A. Shah. M. Irfan. A. Zafar. M. B. Shaikh. N. Akhtar. J. Wu. S. Mirjalili. et al. Large language models. com- prehensive survey of its applications. challenges. limitations. and future",
    "chunk_index": 586,
    "start_pos": 264015,
    "end_pos": 264505,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 527
    }
  },
  "1296": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_587",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "applications. challenges. limitations. and future prospects. TechRxiv 2023 32 433 X. L. Dong. S. Moon. E. Xu. K. Malik. Z. Yu. Towards next- generation intelligent assistants leveraging llm techniques. in. Proceed- ings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023. pp. 5792 5793. 32 434 K. Pandya. M. Holia. Automating customer service using langchain. Building custom open-source gpt chatbot for organizations. arXiv preprint arXiv.2310.05421 2023 32",
    "chunk_index": 587,
    "start_pos": 264506,
    "end_pos": 264956,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 500,
      "optimized_content_length": 486
    }
  },
  "1297": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_588",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ations. arXiv preprint arXiv.2310.05421 2023 32 435 J. Li. B. Hui. G. Qu. B. Li. J. Yang. B. Li. B. Wang. B. Qin. R. Cao. R. Geng. et al. Can llm already serve as database interface. 45",
    "chunk_index": 588,
    "start_pos": 264957,
    "end_pos": 265101,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 195,
      "word_count": 39,
      "optimized_for_embedding": true,
      "original_content_length": 195,
      "optimized_content_length": 185
    }
  },
  "1298": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_589",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "an llm already serve as database interface. 45 --- Page 46 --- big bench for large-scale database grounded text-to-sqls. arXiv preprint arXiv.2305.03111 2023 32 436 A. Rao. J. Kim. M. Kamineni. M. Pang. W. Lie. M. D. Succi. Evaluating chatgpt as an adjunct for radiologic decision-making. medRxiv 2023 2023 02. 32 437 M. Benary. X. D. Wang. M. Schmidt. D. Soll. G. Hilfenhaus. M. Nas- sir. C. Sigler. M. Kno dler. U. Keller. D. Beule. et al. Leveraging large language models for decision support in personalized oncology. JAMA",
    "chunk_index": 589,
    "start_pos": 265103,
    "end_pos": 265591,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 88,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 526
    }
  },
  "1299": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_590",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "or decision support in personalized oncology. JAMA Network Open 11 2023 e2343689 e2343689. 32 438 C. M. Chiesa-Estomba. J. R. Lechien. L. A. Vaira. A. Brunet. G. Cam- maroto. M. Mayo-Yanez. A. Sanchez-Barrueco. C. Saga-Gutierrez. Ex- ploring the potential of chat-gpt as supportive tool for sialendoscopy clinical decision making and patient information support. European Archives of Oto-Rhino-Laryngology 2023 6. 32 439 S. Montagna. S. Ferretti. L. C. Klopfenstein. A. Florio. M. F. Pengo.",
    "chunk_index": 590,
    "start_pos": 265592,
    "end_pos": 266047,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 490
    }
  },
  "1300": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_591",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "retti. L. C. Klopfenstein. A. Florio. M. F. Pengo. Data decentralisation of llm-based chatbot systems in chronic disease self-management. in. Proceedings of the 2023 ACM Conference on In- formation Technology for Social Good. 2023. pp. 205 212. 32 440 D. Bill. T. Eriksson. Fine-tuning llm using reinforcement learning from human feedback for therapy chatbot application 2023 32 441 M. Abbasian. I. Azimi. A. M. Rahmani. R. Jain. Conversational health agents. personalized llm-powered agent framework. arXiv preprint",
    "chunk_index": 591,
    "start_pos": 266048,
    "end_pos": 266526,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 516
    }
  },
  "1301": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_592",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "alized llm-powered agent framework. arXiv preprint arXiv.2310.02374 2023 32 442 K. Lemley. Does chatgpt help us understand the medical literature. Journal of the American Society of Nephrology 2023 10 1681. 32 443 S. Pal. M. Bhattacharya. S.-S. Lee. C. Chakraborty. domain-specific next-generation large language model llm or chatgpt is required for biomedical engineering and research. Annals of Biomedical Engineering 2023 4. 32 444 Du. S. Zhao. Chen. R. Bai. J. Liu. H. Wu. H. Wang. B. Qin. The",
    "chunk_index": 592,
    "start_pos": 266527,
    "end_pos": 267005,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 497
    }
  },
  "1302": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_593",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Chen. R. Bai. J. Liu. H. Wu. H. Wang. B. Qin. The calla dataset. Probing llms interactive knowledge acquisition from chi- nese medical literature. arXiv preprint arXiv.2309.04198 2023 32 445 A. Abd-Alrazaq. R. AlSaad. D. Alhuwail. A. Ahmed. P. M. Healy. S. Latifi. S. Aziz. R. Damseh. S. A. Alrazak. J. Sheikh. et al. Large language models in medical education. Opportunities. challenges. and future directions. JMIR Medical Education 2023 e48291. 32 446 A. B. Mbakwe. I. Lourentzou. L. A. Celi. O. J. Mechanic. A. Dagan.",
    "chunk_index": 593,
    "start_pos": 267006,
    "end_pos": 267494,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 521
    }
  },
  "1303": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_594",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Lourentzou. L. A. Celi. O. J. Mechanic. A. Dagan. Chatgpt passing usmle shines spotlight on the flaws of medical educa- tion 2023 32 447 S. Ahn. The impending impacts of large language models on medical education. Korean Journal of Medical Education 35 2023 103. 32 448 E. Waisberg. J. Ong. M. Masalkhi. A. G. Lee. Large language model llm -driven chatbots for neuro-ophthalmic medical education. Eye 2023 3. 32 449 G. Deiana. M. Dettori. A. Arghittu. A. Azara. G. Gabutti. P. Castiglia.",
    "chunk_index": 594,
    "start_pos": 267495,
    "end_pos": 267954,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 510,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 487
    }
  },
  "1304": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_595",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "A. Arghittu. A. Azara. G. Gabutti. P. Castiglia. Artificial intelligence and public health. Evaluating chatgpt responses to vaccination myths and misconceptions. Vaccines 11 2023 1217. 32 450 L. De Angelis. F. Baglivo. G. Arzilli. G. P. Privitera. P. Ferragina. A. E. Tozzi. C. Rizzo. Chatgpt and the rise of large language models. the new ai-driven infodemic threat in public health. Frontiers in Public Health 11 2023 1166120. 32 451 N. L. Rane. A. Tawde. S. P. Choudhary. J. Rane. Contribution and per-",
    "chunk_index": 595,
    "start_pos": 267955,
    "end_pos": 268423,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 519,
      "word_count": 84,
      "optimized_for_embedding": true,
      "original_content_length": 519,
      "optimized_content_length": 505
    }
  },
  "1305": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_596",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "e. S. P. Choudhary. J. Rane. Contribution and per- formance of chatgpt and other large language models llm for scientific and research advancements. double-edged sword. International Re- search Journal of Modernization in Engineering Technology and Science 10 2023 875 899. 32 452 W. Dai. J. Lin. H. Jin. T. Li. .-S. Tsai. D. Gas evi c. G. Chen. Can large language models provide feedback to students. case study on chatgpt. in. 2023 IEEE International Conference on Advanced Learning Tech-",
    "chunk_index": 596,
    "start_pos": 268424,
    "end_pos": 268879,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 490
    }
  },
  "1306": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_597",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "nternational Conference on Advanced Learning Tech- nologies ICALT IEEE. 2023. pp. 323 325. 32 453 E. Kasneci. K. Se\u00dfler. S. Ku chemann. M. Bannert. D. Dementieva. F. Fischer. U. Gasser. G. Groh. S. Gu nnemann. E. Hu llermeier. et al. Chatgpt for good. on opportunities and challenges of large language models for education. Learning and individual di fferences 103 2023 102274. 32 454 N. Rane. Enhancing the quality of teaching and learning through chat- gpt and similar large language models. Challenges. future prospects.",
    "chunk_index": 597,
    "start_pos": 268880,
    "end_pos": 269359,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 523
    }
  },
  "1307": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_598",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "rge language models. Challenges. future prospects. and ethical considerations in education. Future Prospects. and Ethical Considerations in Education September 15. 2023 2023 32 455 J. C. Young. M. Shishido. Investigating openai chatgpt potentials in generating chatbot dialogue for english as foreign language learning.International Journal of Advanced Computer Science and Applications 14 2023 32 456 J. Irons. C. Mason. P. Cooper. S. Sidra. A. Reeson. C. Paris. Exploring",
    "chunk_index": 598,
    "start_pos": 269360,
    "end_pos": 269804,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 473
    }
  },
  "1308": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_599",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Cooper. S. Sidra. A. Reeson. C. Paris. Exploring the impacts of chatgpt on future scientific work. SocArXiv 2023 32 457 P. G. Schmidt. A. J. Meir. Using generative ai for literature searches and scholarly writing. Is the integrity of the scientific discourse in jeopardy. arXiv preprint arXiv.2311.06981 2023 32 458 Zheng. H. Koh. J. Ju. A. T. Nguyen. L. T. May. G. I. Webb. S. Pan. Large language models for scientific synthesis. inference and explana- tion. arXiv preprint arXiv.2310.07984 2023 33",
    "chunk_index": 599,
    "start_pos": 269805,
    "end_pos": 270277,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 499
    }
  },
  "1309": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_600",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tion. arXiv preprint arXiv.2310.07984 2023 33 459 B. Aczel. E.-J. Wagenmakers. Transparency guidance for chatgpt usage in scientific writing. PsyArXiv 2023 33 460 S. Altma e. A. Sola-Leyva. A. Salumets. Artificial intelligence in sci- entific writing. friend or foe. Reproductive BioMedicine Online 2023 33 461 S. Imani. L. Du. H. Shrivastava. Mathprompter. Mathematical reasoning using large language models. arXiv preprint arXiv.2303.05398 2023 33 462 Z. Yuan. H. Yuan. C. Li. G. Dong. C. Tan. C. Zhou. Scaling relationship",
    "chunk_index": 600,
    "start_pos": 270278,
    "end_pos": 270778,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 525
    }
  },
  "1310": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_601",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Li. G. Dong. C. Tan. C. Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv.2308.01825 2023 33 463 K. Yang. A. M. Swope. A. Gu. R. Chalamala. P. Song. S. Yu. S. Godil. R. Prenger. A. Anandkumar. Leandojo. Theorem proving with retrieval- augmented language models. arXiv preprint arXiv.2306.15626 2023 33 464 K. M. Collins. A. Q. Jiang. S. Frieder. L. Wong. M. Zilka. U. Bhatt. T. Lukasiewicz. Wu. J. B. Tenenbaum. W. Hart. et al. Evaluating",
    "chunk_index": 601,
    "start_pos": 270779,
    "end_pos": 271242,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 499
    }
  },
  "1311": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_602",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Wu. J. B. Tenenbaum. W. Hart. et al. Evaluating language models for mathematics through interactions. arXiv preprint arXiv.2306.01694 2023 33 465 Liu. T. Han. S. Ma. J. Zhang. Yang. J. Tian. H. He. A. Li. M. He. Z. Liu. et al. Summary of chatgpt-related research and perspective towards the future of large language models. Meta-Radiology 2023 100017. 33 466 J. Dra pal. H. Westermann. J. Savelka. Using large language models to support thematic analysis in empirical legal studies. arXiv preprint arXiv.2310.18729 2023 33",
    "chunk_index": 602,
    "start_pos": 271243,
    "end_pos": 271737,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 522
    }
  },
  "1312": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_603",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tudies. arXiv preprint arXiv.2310.18729 2023 33 467 J. Savelka. K. D. Ashley. M. A. Gray. H. Westermann. H. Xu. Explain- ing legal concepts with augmented large language models gpt-4 arXiv preprint arXiv.2306.09525 2023 33 468 N. Guha. J. Nyarko. D. E. Ho. C. Re A. Chilton. A. Narayana. A. Chohlas-Wood. A. Peters. B. Waldon. D. N. Rockmore. et al. Legal- bench. collaboratively built benchmark for measuring legal reasoning in large language models. arXiv preprint arXiv.2308.11462 2023 33",
    "chunk_index": 603,
    "start_pos": 271738,
    "end_pos": 272198,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 491
    }
  },
  "1313": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_604",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2308.11462 2023 33 469 J. Cui. Z. Li. Yan. B. Chen. L. Yuan. Chatlaw. Open-source legal large language model with integrated external knowledge bases. arXiv preprint arXiv.2306.16092 2023 33 470 H. Yang. X.-Y Liu. C. D. Wang. Fingpt. Open-source financial large language models. arXiv preprint arXiv.2306.06031 2023 33 471 Li. S. Wang. H. Ding. H. Chen. Large language models in finance. survey. in. Proceedings of the Fourth ACM International Conference on AI in Finance. 2023. pp. 374 382. 33",
    "chunk_index": 604,
    "start_pos": 272199,
    "end_pos": 272698,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 87,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 523
    }
  },
  "1314": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_605",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Conference on AI in Finance. 2023. pp. 374 382. 33 472 A. Lykov. D. Tsetserukou. Llm-brain. Ai-driven fast generation of robot behaviour tree based on large language model. arXiv preprint arXiv.2305.19352 2023 33 473 E. Billing. J. Rose n. M. Lamb. Language models for human-robot inter- action. in. ACM IEEE International Conference on Human-Robot Inter- action. March 13 16. 2023. Stockholm. Sweden. ACM Digital Library. 2023. pp. 905 906. 33 474 Ye. H. You. J. Du. Improved trust in human-robot collaboration with",
    "chunk_index": 605,
    "start_pos": 272699,
    "end_pos": 273177,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 516
    }
  },
  "1315": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_606",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "Improved trust in human-robot collaboration with chatgpt. IEEE Access 2023 33 475 Ding. X. Zhang. C. Paxton. S. Zhang. Leveraging commonsense knowledge from large language models for task and motion planning. in. RSS 2023 Workshop on Learning for Task and Motion Planning. 2023. 33 476 J. Wu. R. Antonova. A. Kan. M. Lepert. A. Zeng. S. Song. J. Bohg. S. Rusinkiewicz. T. Funkhouser. Tidybot. Personalized robot assistance with large language models. arXiv preprint arXiv.2305.05658 2023 33",
    "chunk_index": 606,
    "start_pos": 273178,
    "end_pos": 273633,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 506,
      "optimized_content_length": 490
    }
  },
  "1316": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_607",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "models. arXiv preprint arXiv.2305.05658 2023 33 477 E. Strubell. A. Ganesh. A. McCallum. Energy and policy considerations for deep learning in nlp. arXiv preprint arXiv.1906.02243 2019 34 478 E. M. Bender. T. Gebru. A. McMillan-Major. S. Shmitchell. On the dan- 46",
    "chunk_index": 607,
    "start_pos": 273634,
    "end_pos": 273857,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 274,
      "word_count": 41,
      "optimized_for_embedding": true,
      "original_content_length": 274,
      "optimized_content_length": 264
    }
  },
  "1317": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_608",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "A. McMillan-Major. S. Shmitchell. On the dan- 46 --- Page 47 --- gers of stochastic parrots. Can language models be too big. in. Pro- ceedings of the 2021 ACM conference on fairness. accountability. and transparency. 2021. pp. 610 623. 34 479 C. Zhang. S. Bengio. M. Hardt. B. Recht. O. Vinyals. Understanding deep learning still requires rethinking generalization. Communications of the ACM 64 2021 107 115. 34 480 M. Ta nzer. S. Ruder. M. Rei. Memorisation versus generalisation in pre-",
    "chunk_index": 608,
    "start_pos": 273859,
    "end_pos": 274310,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 488
    }
  },
  "1318": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_609",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "M. Rei. Memorisation versus generalisation in pre- trained language models. arXiv preprint arXiv.2105.00828 2021 34 481 S. M. West. M. Whittaker. K. Crawford. Discriminating systems. AI Now 2019 33. 34 482 K. Valmeekam. A. Olmo. S. Sreedharan. S. Kambhampati. Large lan- guage models still can plan benchmark for llms on planning and reasoning about change arXiv preprint arXiv.2206.10498 2022 34 483 Zhang. Li. L. Cui. D. Cai. L. Liu. T. Fu. X. Huang. E. Zhao.",
    "chunk_index": 609,
    "start_pos": 274311,
    "end_pos": 274752,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 492,
      "word_count": 80,
      "optimized_for_embedding": true,
      "original_content_length": 492,
      "optimized_content_length": 461
    }
  },
  "1319": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_610",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "L. Cui. D. Cai. L. Liu. T. Fu. X. Huang. E. Zhao. Zhang. Chen. et al. Siren song in the ai ocean. survey on hal- lucination in large language models. arXiv preprint arXiv.2309.01219 2023 34 484 A. Webson. E. Pavlick. Do prompt-based models really understand the meaning of their prompts. arXiv preprint arXiv.2109.01247 2021 34 485 O. Shaikh. H. Zhang. W. Held. M. Bernstein. D. Yang. On second thought. let not think step by step. bias and toxicity in zero-shot rea- soning. arXiv preprint arXiv.2212.08061 2022 34",
    "chunk_index": 610,
    "start_pos": 274753,
    "end_pos": 275247,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 515
    }
  },
  "1320": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_611",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "soning. arXiv preprint arXiv.2212.08061 2022 34 486 B. C. Das. M. H. Amini. Wu. Security and privacy challenges of large language models. survey. arXiv preprint arXiv.2402.00888 2024 34 487 X. Liu. H. Cheng. P. He. W. Chen. Wang. H. Poon. J. Gao. Adversar- ial training for large neural language models. ArXiv April 2020 URL https. www.microsoft.com en-us research publication adversarial-training-for-large-neural-language-models 34 488 E. Shayegani. M. A. A. Mamun. Fu. P. Zaree. Dong. N. Abu-",
    "chunk_index": 611,
    "start_pos": 275248,
    "end_pos": 275728,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 495
    }
  },
  "1321": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_612",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "A. A. Mamun. Fu. P. Zaree. Dong. N. Abu- Ghazaleh. Survey of vulnerabilities in large language models revealed by adversarial attacks 2023 arXiv.2310.10844 34 489 X. Xu. K. Kong. N. Liu. L. Cui. D. Wang. J. Zhang. M. Kankanhalli. An llm can fool itself. prompt-based adversarial attack 2023 arXiv. 2310.13345 34 490 H. Zhao. H. Chen. F. Yang. N. Liu. H. Deng. H. Cai. S. Wang. D. Yin. M. Du. Explainability for large language models. survey 2023 arXiv.2309.01029 35",
    "chunk_index": 612,
    "start_pos": 275729,
    "end_pos": 276176,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 89,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 465
    }
  },
  "1322": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_613",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "age models. survey 2023 arXiv.2309.01029 35 491 S. Huang. S. Mamidanna. S. Jangam. Zhou. L. H. Gilpin. Can large language models explain themselves. study of llm-generated self- explanations 2023 arXiv.2310.11207 35 492 H. Brown. K. Lee. F. Mireshghallah. R. Shokri. F. Trame r. What does it mean for language model to preserve privacy. in. Proceedings of the 2022 ACM Conference on Fairness. Accountability. and Transparency. 2022. pp. 2280 2292. 35 493 R. Plant. Giu ffrida. D. Gkatzia. You are what you write. Pre-",
    "chunk_index": 613,
    "start_pos": 276177,
    "end_pos": 276673,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 90,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 517
    }
  },
  "1323": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_614",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "ffrida. D. Gkatzia. You are what you write. Pre- serving privacy in the era of large language models. arXiv preprint arXiv.2204.09391 2022 35 494 W. Niu. Z. Kong. G. Yuan. W. Jiang. J. Guan. C. Ding. P. Zhao. S. Liu. B. Ren. Wang. Real-time execution of large-scale language models on mobile 2020 arXiv.2009.06823 35 495 C. Guo. J. Tang. W. Hu. J. Leng. C. Zhang. F. Yang. Liu. M. Guo. Zhu. Olive. Accelerating large language models via hardware- friendly outlier-victim pair quantization. in. Proceedings of the 50th",
    "chunk_index": 614,
    "start_pos": 276674,
    "end_pos": 277166,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 543,
      "word_count": 95,
      "optimized_for_embedding": true,
      "original_content_length": 543,
      "optimized_content_length": 517
    }
  },
  "1324": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_615",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "tim pair quantization. in. Proceedings of the 50th Annual International Symposium on Computer Architecture. 2023. pp. 15. 35 496 B. Mesko E. J. Topol. The imperative for regulatory oversight of large language models or generative ai in healthcare. npj Digital Medicine 2023 120. 35 497 J. Zhang. X. Ji. Z. Zhao. X. Hei. K.-K. R. Choo. Ethical considerations and policy implications for large language models. Guiding responsible development and deployment. arXiv preprint arXiv.2308.02678 2023 35",
    "chunk_index": 615,
    "start_pos": 277167,
    "end_pos": 277632,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 496
    }
  },
  "1325": {
    "chunk_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412_chunk_616",
    "document_id": "4c86ae35-67c6-4df4-a3e2-573ff0484412",
    "content": "oyment. arXiv preprint arXiv.2308.02678 2023 35 498 J. Mo kander. J. Schuett. H. R. Kirk. L. Floridi. Auditing large language models. three-layered approach. AI and Ethics 2023 31. 35 47",
    "chunk_index": 616,
    "start_pos": 277633,
    "end_pos": 277778,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 196,
      "word_count": 30,
      "optimized_for_embedding": true,
      "original_content_length": 196,
      "optimized_content_length": 186
    }
  },
  "1326": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_0",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "--- Page --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions XINYI HOU. Huazhong University of Science and Technology. China YANJIE ZHAO. Huazhong University of Science and Technology. China SHENAO WANG. Huazhong University of Science and Technology. China HAOYU WANG .Huazhong University of Science and Technology. China The Model Context Protocol MCP is standardized interface designed to enable seamless interaction between",
    "chunk_index": 0,
    "start_pos": 0,
    "end_pos": 474,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 474,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 474,
      "optimized_content_length": 465
    }
  },
  "1327": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_1",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ce designed to enable seamless interaction between AI models and external tools and resources. breaking down data silos and facilitating interoperability across diverse systems. This paper provides comprehensive overview of MCP. focusing on its core components. workflow. and the lifecycle of MCP servers. which consists of three key phases. creation. operation. and update. We analyze the security and privacy risks associated with each phase and propose strategies to mitigate",
    "chunk_index": 1,
    "start_pos": 475,
    "end_pos": 904,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 480,
      "optimized_content_length": 478
    }
  },
  "1328": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_2",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "with each phase and propose strategies to mitigate potential threats. The paper also examines the current MCP landscape. including its adoption by industry leaders and various use cases. as well as the tools and platforms supporting its integration. We explore future directions for MCP. highlighting the challenges and opportunities that will influence its adoption and evolution within the broader AI ecosystem. Finally. we offer recommendations for MCP stakeholders to ensure its secure",
    "chunk_index": 2,
    "start_pos": 905,
    "end_pos": 1343,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 489,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 489,
      "optimized_content_length": 489
    }
  },
  "1329": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_3",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ndations for MCP stakeholders to ensure its secure and sustainable development as the AI landscape continues to evolve. CCS Concepts. General and reference Surveys and overviews Security and privacy Software and application security Computing methodologies Artificial intelligence Additional Key Words and Phrases. Model Context Protocol. MCP. Vision paper. Security ACM Reference Format. Xinyi Hou. Yanjie Zhao. Shenao Wang. and Haoyu Wang. 2025. Model Context Protocol MCP Landscape.",
    "chunk_index": 3,
    "start_pos": 1344,
    "end_pos": 1791,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 498,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 485
    }
  },
  "1330": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_4",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ng. 2025. Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions. 1. April 2025 20 pages. https. doi.org 10.1145 nnnnnnn. nnnnnnn INTRODUCTION In recent years. the vision of autonomous AI agents capable of interacting with wide range of tools and data sources has gained significant momentum. This progress accelerated in 2023 with the introduction of function calling by OpenAI. which allowed language models to invoke external",
    "chunk_index": 4,
    "start_pos": 1792,
    "end_pos": 2214,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 473,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 473,
      "optimized_content_length": 460
    }
  },
  "1331": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_5",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "which allowed language models to invoke external APIs in structured way 38 This advancement expanded the capabilities of LLMs. enabling them to retrieve real-time data. perform computations. and interact with external systems. As function calling gained adoption. an ecosystem formed around it. OpenAI introduced the ChatGPT plugin 37 allowing developers to build callable tools for ChatGPT. LLM app stores such as Coze and Yuanqi 50 have launched their plugin stores supporting tools specifically designed for",
    "chunk_index": 5,
    "start_pos": 2215,
    "end_pos": 2695,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 510
    }
  },
  "1332": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_6",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "tores supporting tools specifically designed for Haoyu Wang is the corresponding author haoyuwang hust.edu.cn Authors addresses. Xinyi Hou. xinyihou hust.edu.cn. Huazhong University of Science and Technology. Wuhan. China. Yanjie Zhao. yanjie_zhao hust.edu.cn. Huazhong University of Science and Technology. Wuhan. China. Shenao Wang. shenaowang hust.edu.cn. Huazhong University of Science and Technology. Wuhan. China. Haoyu Wang. haoyuwang hust.edu.cn. Huazhong University of Science and Technology. Wuhan. China.",
    "chunk_index": 6,
    "start_pos": 2696,
    "end_pos": 3168,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 515
    }
  },
  "1333": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_7",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "niversity of Science and Technology. Wuhan. China. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author must be honored. Abstracting with credit is permitted. To copy otherwise. or republish. to post on servers or to redistribute to lists. requires",
    "chunk_index": 7,
    "start_pos": 3169,
    "end_pos": 3668,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 92,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 547
    }
  },
  "1334": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_8",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "on servers or to redistribute to lists. requires prior specific permission and or fee. Request permissions from permissions acm.org. 2025 Copyright held by the owner author Publication rights licensed to ACM. ACM XXXX-XXXX 2025 4-ART https. doi.org 10.1145 nnnnnnn.nnnnnnn Vol. 1. No. 1. Article Publication date. April 2025.arXiv.2503.23278v2 cs.CR Apr 2025",
    "chunk_index": 8,
    "start_pos": 3669,
    "end_pos": 3996,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 378,
      "word_count": 48,
      "optimized_for_embedding": true,
      "original_content_length": 378,
      "optimized_content_length": 358
    }
  },
  "1335": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_9",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "April 2025.arXiv.2503.23278v2 cs.CR Apr 2025 --- Page --- Hou. Zhao. Wang. and Wang their platforms. Frameworks like LangChain 26 and LlamaIndex 29 provided standardized tool interfaces making it easier to integrate LLMs with external services. Other AI providers. including Anthropic. Google. and Meta. introduced similar mechanisms. further driving adoption. Despite these advancements. integrating tools remains fragmented Developers must manually define",
    "chunk_index": 9,
    "start_pos": 3998,
    "end_pos": 4432,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 457
    }
  },
  "1336": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_10",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "mains fragmented Developers must manually define interfaces. manage authentication. and handle execution logic for each service. Function calling mechanisms vary across platforms. requiring redundant implementations. Additionally. current approaches rely on predefined workflows. limiting AI agents flexibility in dynamically discovering and orchestrating tools In late 2024. Anthropic introduced the Model Context Protocol MCP general-purpose pro-",
    "chunk_index": 10,
    "start_pos": 4433,
    "end_pos": 4844,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 462,
      "word_count": 58,
      "optimized_for_embedding": true,
      "original_content_length": 462,
      "optimized_content_length": 448
    }
  },
  "1337": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_11",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Context Protocol MCP general-purpose pro- tocol standardizing AI-tool interactions. Inspired by the Language Server Protocol LSP 22 MCP provides flexible framework for AI applications to communicate with external tools dynamically. Instead of relying on predefined tool mappings. MCP allows AI agents to autonomously discover. select. and orchestrate tools based on task context. It also supports human-in-the-loop mechanisms. enabling users to inject data or approve actions as needed. By unifying interfaces. MCP simplifies",
    "chunk_index": 11,
    "start_pos": 4845,
    "end_pos": 5336,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 525
    }
  },
  "1338": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_12",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "as needed. By unifying interfaces. MCP simplifies the development of AI applications and improves their flexibility in handling complex workflows. Since its release. MCP has rapidly grown from niche protocol to key foundation for AI-native application development. thriving ecosystem has emerged. with thousands of community-driven MCP servers enabling model access to systems like GitHub 41 Slack 42 and even 3D design tools like Blender Tools like Cursor 12 and Claude Desktop demonstrate how MCP clients can",
    "chunk_index": 12,
    "start_pos": 5337,
    "end_pos": 5825,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 86,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 510
    }
  },
  "1339": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_13",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "laude Desktop demonstrate how MCP clients can extend their capabilities by installing new servers. turning developer tools. productivity platforms. and creative environments alike into multi-modal AI agents. Despite the rapid adoption of MCP. its ecosystem is still in the early stages. with key areas such as security. tool discoverability. and remote deployment lacking comprehensive solutions. These issues present untapped opportunities for further research and development. Although MCP is",
    "chunk_index": 13,
    "start_pos": 5826,
    "end_pos": 6274,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 499,
      "optimized_content_length": 494
    }
  },
  "1340": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_14",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "further research and development. Although MCP is widely recognized for its potential in the industry. it has not yet been extensively analyzed in academic research. This gap in research motivates this paper. which provides the first analysis of the MCP ecosystem. examining its architecture and workflow. defining the lifecycle of MCP servers. and identifying potential security risks at each stage. such as installer spoofing and tool name conflict. Through this study. we present thorough exploration of MCP current landscape and",
    "chunk_index": 14,
    "start_pos": 6275,
    "end_pos": 6761,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 532
    }
  },
  "1341": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_15",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "horough exploration of MCP current landscape and offer forward-looking vision that highlights key implications. outlines future research directions. and addresses the challenges that must be overcome to ensure its sustainable growth. Our contributions are as follows. We provide the first analysis of the MCP ecosystem. detailing its architecture. components. and workflow. We identify the key components of MCP servers and define their lifecycle. encompassing the",
    "chunk_index": 15,
    "start_pos": 6762,
    "end_pos": 7185,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 474,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 474,
      "optimized_content_length": 464
    }
  },
  "1342": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_16",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "rvers and define their lifecycle. encompassing the stages of creation. operation. and update. We also highlight potential security risks associated with each phase. offering insights into safeguarding AI-to-tool interactions. We examine the current MCP ecosystem landscape. analyzing the adoption. diversity. and use cases across various industries and platforms. We discuss the implications of MCP rapid adoption. identifying key challenges for stake-",
    "chunk_index": 16,
    "start_pos": 7186,
    "end_pos": 7595,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 460,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 460,
      "optimized_content_length": 452
    }
  },
  "1343": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_17",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "id adoption. identifying key challenges for stake- holders. and outline future research directions on security. scalability. and governance to ensure its sustainable growth. The remainder of this paper is structured as follows. compares tool invocation with and without MCP. highlighting the motivation for this study. outlines the architecture of MCP. detailing the roles of the MCP host. client. and server. as well as the lifecycle of the MCP server.",
    "chunk_index": 17,
    "start_pos": 7596,
    "end_pos": 8006,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 461,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 461,
      "optimized_content_length": 453
    }
  },
  "1344": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_18",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "erver. as well as the lifecycle of the MCP server. examines the current MCP landscape. focusing on key industry players and adoption trends. analyzes security and privacy risks across the MCP server lifecycle and proposes mitigation Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 18,
    "start_pos": 8007,
    "end_pos": 8253,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 297,
      "word_count": 51,
      "optimized_for_embedding": true,
      "original_content_length": 297,
      "optimized_content_length": 285
    }
  },
  "1345": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_19",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions strategies. explores implications. future challenges. and recommendations to enhance MCP scalability and security in dynamic AI environments. reviews prior work on tool integration and security in LLM applications. Finally. concludes the whole paper. BACKGROUND AND MOTIVATION 2.1 AI Tooling",
    "chunk_index": 19,
    "start_pos": 8255,
    "end_pos": 8669,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 465,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 464,
      "optimized_content_length": 439
    }
  },
  "1346": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_20",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "paper. BACKGROUND AND MOTIVATION 2.1 AI Tooling Before the introduction of MCP. AI applications relied on various methods. such as manual API wiring. plugin-based interfaces. and agent frameworks. to interact with external tools. As shown in Figure 1. these approaches required integrating each external service with specific API. leading to increased complexity and limited scalability. MCP addresses these challenges by providing standardized protocol that enables seamless and flexible interaction with multiple tools. AIApplication",
    "chunk_index": 20,
    "start_pos": 8670,
    "end_pos": 9161,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 535
    }
  },
  "1347": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_21",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ble interaction with multiple tools. AIApplication External Tools and ResourcesSpecific APISpecific APISpecific APIWeb ServicesMCPServerAIApplication MCPClient Model Context Protocolvs.WithoutMCPWithMCP Database LocalFiles Specific APISpecific APISpecific APIExternal Tools and ResourcesWeb Services Database LocalFiles Fig. 1. Tool invocation with and without MCP. 2.1.1 Manual API Wiring. In traditional implementations. developers had to establish manual API",
    "chunk_index": 21,
    "start_pos": 9162,
    "end_pos": 9572,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 461,
      "word_count": 49,
      "optimized_for_embedding": true,
      "original_content_length": 461,
      "optimized_content_length": 461
    }
  },
  "1348": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_22",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "mentations. developers had to establish manual API connections for each tool or service that an AI application interacted with. This process required custom authentication. data transformation. and error handling for every integration As the number of APIs increased. the maintenance burden became significant. often leading to tightly coupled and fragile systems that were difficult to scale or modify. MCP eliminates this complexity by offering unified interface. allowing AI models to connect with multiple tools dynamically",
    "chunk_index": 22,
    "start_pos": 9573,
    "end_pos": 10053,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 527
    }
  },
  "1349": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_23",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "models to connect with multiple tools dynamically without the need for custom API wiring. 2.1.2 Standardized Plugin Interfaces. To reduce the complexity of manual wiring. plugin-based interfaces such as OpenAI ChatGPT Plugins. introduced in November 2023 37 allowed AI models to connect with external tools through standardized API schemas like OpenAPI. For example. in the OpenAI Plugin ecosystem. plugins like Zapier allowed models to perform predefined actions.",
    "chunk_index": 23,
    "start_pos": 10054,
    "end_pos": 10472,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 469,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 468,
      "optimized_content_length": 464
    }
  },
  "1350": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_24",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "pier allowed models to perform predefined actions. such as sending emails or updating CRM records. However. these interactions were often one- directional and could not maintain state or coordinate multiple steps in task New LLM app stores 62 such as ByteDance Coze and Tencent Yuanqi 50 have also emerged. offering plugin store for web services. While these platforms expanded available tool options. they created isolated ecosystems where plugins are platform-specific limiting cross-platform compatibility",
    "chunk_index": 24,
    "start_pos": 10473,
    "end_pos": 10949,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 508
    }
  },
  "1351": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_25",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "m-specific limiting cross-platform compatibility and requiring duplicate maintenance efforts. MCP stands out by being open-source and platform- agnostic. enabling AI applications to engage in rich two-way interactions with external tools. facilitating complex workflows. 2.1.3 AI Agent Tool Integration. The emergence of AI agent frameworks like LangChain 26 and similar tool orchestration frameworks provided structured way for models to invoke external tools",
    "chunk_index": 25,
    "start_pos": 10950,
    "end_pos": 11366,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 467,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 467,
      "optimized_content_length": 460
    }
  },
  "1352": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_26",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "structured way for models to invoke external tools through predefined interfaces. improving automation and adaptability 55 However. integrating and maintaining these tools remained largely manual. requiring custom implementations and Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 26,
    "start_pos": 11367,
    "end_pos": 11610,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 294,
      "word_count": 41,
      "optimized_for_embedding": true,
      "original_content_length": 294,
      "optimized_content_length": 286
    }
  },
  "1353": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_27",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page --- Hou. Zhao. Wang. and Wang increasing complexity as the number of tools grew. MCP simplifies this process by offering standardized protocol that enables AI agents to seamlessly invoke. interact with. and chain multiple tools through unified interface This reduces manual configuration and enhances task flexibility. allowing agents to perform complex operations without extensive custom integration.",
    "chunk_index": 27,
    "start_pos": 11612,
    "end_pos": 12041,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 479,
      "optimized_content_length": 459
    }
  },
  "1354": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_28",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "operations without extensive custom integration. 2.1.4 Retrieval-Augmented Generation RAG and Vector Database. Contextual information retrieval methods. such as RAG. leverage vector-based search to retrieve relevant knowledge from databases or knowledge bases. enabling models to supplement responses with up-to-date information 11.16 While this approach addressed the problem of knowledge cutoff and improved model accuracy. it was limited to passive retrieval of information It did not inherently allow models to perform",
    "chunk_index": 28,
    "start_pos": 12042,
    "end_pos": 12523,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 522
    }
  },
  "1355": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_29",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "on It did not inherently allow models to perform active operations. such as modifying data or triggering workflows. For example. RAG-based system could retrieve relevant sections from product documentation database to assist customer support AI. However. if the AI needed to update customer records or escalate an issue to human support. it could not take action beyond providing textual responses. MCP extends beyond passive information retrieval by enabling AI models to interact with external data sources and tools actively.",
    "chunk_index": 29,
    "start_pos": 12524,
    "end_pos": 13009,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 528
    }
  },
  "1356": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_30",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "act with external data sources and tools actively. facilitating both retrieval and action in unified workflow. 2.2 Motivation MCP has rapidly gained traction in the AI community due to its ability to standardize how AI models interact with external tools. fetch data. and execute operations. By addressing the limitations of manual API wiring. plugin interfaces. and agent frameworks. MCP has the potential to redefine AI-to-tool interactions and enable more autonomous and intelligent agent workflows. Despite",
    "chunk_index": 30,
    "start_pos": 13010,
    "end_pos": 13471,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 510
    }
  },
  "1357": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_31",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "utonomous and intelligent agent workflows. Despite its growing adoption and promising potential. MCP is still in its early stages. with an evolving ecosystem that remains incomplete. Many key aspects. such as security and tool discoverability. are yet to be fully addressed. leaving ample room for future research and improvement. Moreover. while MCP has gained rapid adoption in the industry. it is still largely unexplored in academia. Motivated by this gap. this paper is the first to analyze the current MCP landscape. examine",
    "chunk_index": 31,
    "start_pos": 13472,
    "end_pos": 13951,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 530
    }
  },
  "1358": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_32",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "irst to analyze the current MCP landscape. examine its emerging ecosystem. and identify potential security risks Additionally. we outline vision for MCP future development and highlight the key challenges that must be addressed to support its long-term success. MCP ARCHITECTURE 3.1 Core Components The MCP architecture is composed of three core components. MCP host .MCP client and MCP server These components collaborate to facilitate seamless communication between AI applications.",
    "chunk_index": 32,
    "start_pos": 13952,
    "end_pos": 14397,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 496,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 484
    }
  },
  "1359": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_33",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "te seamless communication between AI applications. external tools. and data sources. ensuring that operations are secure and properly managed. As shown in Figure 2. in typical workflow. the user sends prompt to the MCP client. which analyzes the intent .selects the appropriate tools via the MCP server. and invokes external APIs to retrieve and process the required information before notifying the user of the results. 3.1.1 MCP Host. The MCP host is an AI application that provides the environment for executing",
    "chunk_index": 33,
    "start_pos": 14398,
    "end_pos": 14865,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 514
    }
  },
  "1360": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_34",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "cation that provides the environment for executing AI-based tasks while running the MCP client. It integrates interactive tools and data to enable smooth communication with external services. Examples include Claude Desktop for AI-assisted content creation. Cursor. an AI-powered IDE for code completion and software development. and AI agents that function as autonomous systems for executing complex tasks. The MCP host hosts the MCP client and ensures communication with external MCP servers.",
    "chunk_index": 34,
    "start_pos": 14866,
    "end_pos": 15310,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 495
    }
  },
  "1361": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_35",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ensures communication with external MCP servers. Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 35,
    "start_pos": 3901,
    "end_pos": 3957,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 107,
      "word_count": 18,
      "optimized_for_embedding": true,
      "original_content_length": 107,
      "optimized_content_length": 101
    }
  },
  "1362": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_36",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions MCP ClientsPrompt. Can you please fetch the latest stock price of AAPL and notify me via email. MCP Servers CapabilitiesToolsResourcesPrompts 2InitialResponse3Notification1InitialRequestTransferLayerDataSourceWeb Services Database LocalFiles Tool Selection API Invocation IntentAnalysisNotificationSampling Orchestration UserMCP Workflow ChatApps.IDEs. .AI Agents MCPHosts 1.1Client Server",
    "chunk_index": 36,
    "start_pos": 15369,
    "end_pos": 15868,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 59,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 537
    }
  },
  "1363": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_37",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "tApps.IDEs. .AI Agents MCPHosts 1.1Client Server Fig. 2. The workflow of MCP. 3.1.2 MCP Client. The MCP client acts as an intermediary within the host environment. managing communication between the MCP host and one or more MCP servers. It initiates requests to MCP servers. queries available functions. and retrieves responses that describe the server capabilities. This ensures seamless interaction between the host and external tools. In addition to managing",
    "chunk_index": 37,
    "start_pos": 15869,
    "end_pos": 16283,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 465,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 465,
      "optimized_content_length": 461
    }
  },
  "1364": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_38",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "host and external tools. In addition to managing requests and responses. the MCP client processes notifications from MCP servers. providing real-time updates about task progress and system status. It also performs sampling to gather data on tool usage and performance. enabling optimization and informed decision-making. The MCP client communicates through the transport layer with MCP servers. facilitating secure. reliable data exchange and smooth interaction between the host and external resources.",
    "chunk_index": 38,
    "start_pos": 16284,
    "end_pos": 16737,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 504,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 504,
      "optimized_content_length": 502
    }
  },
  "1365": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_39",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "teraction between the host and external resources. 3.1.3 MCP Server. The MCP server enables the MCP host and client to access external systems and execute operations. offering three core capabilities. tools. resources. and prompts Tools. Enabling external operations Tools allow the MCP server to invoke external services and APIs to execute operations on behalf of AI models. When the client requests an operation. the MCP server identifies the appropriate tool. interacts with the service. and returns the result. For",
    "chunk_index": 39,
    "start_pos": 16738,
    "end_pos": 17211,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 519
    }
  },
  "1366": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_40",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "acts with the service. and returns the result. For instance. if an AI model requires real-time weather data or sentiment analysis. the MCP server connects to the relevant API. retrieves the data. and delivers it to the host. Unlike traditional function calling. which requires multiple steps and separates invocation from execution. Tools of MCP servers streamline this process by allowing the model to autonomously select and invoke the appropriate tool based on context. Once configured. these tools follow standardized supply-",
    "chunk_index": 40,
    "start_pos": 17212,
    "end_pos": 17692,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 81,
      "optimized_for_embedding": true,
      "original_content_length": 531,
      "optimized_content_length": 529
    }
  },
  "1367": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_41",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "figured. these tools follow standardized supply- and-consume model. making them modular. reusable. and easily accessible to other applications. enhancing system efficiency and flexibility. Resources. Exposing data to AI models Resources provide access to structured and unstruc- tured datasets that the MCP server can expose to AI models. These datasets may come from local storage. databases. or cloud platforms. When an AI model requests specific data. the MCP",
    "chunk_index": 41,
    "start_pos": 17693,
    "end_pos": 18109,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 467,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 467,
      "optimized_content_length": 462
    }
  },
  "1368": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_42",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "When an AI model requests specific data. the MCP server retrieves and processes the relevant information. enabling the model to make data-driven decisions. For example. recommendation system may access customer interaction logs. or document summarization task may query text repository. Prompts. Reusable templates for workflow optimization Prompts are predefined templates and workflows that the MCP server generates and maintains to optimize AI responses and",
    "chunk_index": 42,
    "start_pos": 18110,
    "end_pos": 18530,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 471,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 471,
      "optimized_content_length": 460
    }
  },
  "1369": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_43",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "nerates and maintains to optimize AI responses and streamline repetitive tasks. They ensure consistency in responses and improve task execution efficiency. For instance. customer support chatbot may use prompt templates to provide uniform Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 43,
    "start_pos": 18531,
    "end_pos": 18777,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 297,
      "word_count": 45,
      "optimized_for_embedding": true,
      "original_content_length": 297,
      "optimized_content_length": 291
    }
  },
  "1370": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_44",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page --- Hou. Zhao. Wang. and Wang and accurate responses. while an annotation task may rely on predefined prompts to maintain consistency in data labeling. 3.2 Transport Layer and Communication The transport layer ensures secure. bidirectional communication. allowing for real-time interaction and efficient data exchange between the host environment and external systems. The transport",
    "chunk_index": 44,
    "start_pos": 18779,
    "end_pos": 19182,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 454,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 453,
      "optimized_content_length": 439
    }
  },
  "1371": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_45",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "st environment and external systems. The transport layer manages the transmission of initial requests from the client. the delivery of server responses detailing available capabilities. and the exchange of notifications that keep the client informed of ongoing updates. Communication between the MCP client and the MCP server follows structured process. beginning with an initial request from the client to query the server functionalities. Upon receiving the request. the server responds with an initial response listing the available",
    "chunk_index": 45,
    "start_pos": 19183,
    "end_pos": 19671,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 535
    }
  },
  "1372": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_46",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "nds with an initial response listing the available tools. resources. and prompts the client can leverage. Once the connection is established. the system maintains continuous exchange of notifications to ensure that changes in server status or updates are communicated back to the client in real time. This structured communication ensures high-performance interactions and keeps AI models synchronized with external resources. enhancing the effectiveness of AI applications. 3.3 MCP Server Lifecycle",
    "chunk_index": 46,
    "start_pos": 19672,
    "end_pos": 20122,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 499
    }
  },
  "1373": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_47",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "eness of AI applications. 3.3 MCP Server Lifecycle The MCP server lifecycle as shown in Figure consists of three key phases. creation .operation andupdate Each phase defines critical activities that ensure the secure and efficient functioning of the MCP server. enabling seamless interaction between AI models and external tools. resources. and prompts. CreationMCPServerComponentsConfigurationSource CodeConfig FilesManifest MetadataNameDescriptionVersion Tool ListTool NameDescriptionPermissions Resources ListData SourcesEndpointsPermissions",
    "chunk_index": 47,
    "start_pos": 20123,
    "end_pos": 20622,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 544
    }
  },
  "1374": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_48",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ons Resources ListData SourcesEndpointsPermissions PromptsTemplatesWorkflowsMetadata Server Registration Name CollisionInstaller Deployment Installer SpoofingCode Integrity Verification BackdoorOperationTool Execution Tool Name ConflictSlash Command Command OverlapSandbox Mechanism Sandbox EscapeUpdateAuthorization Privilege PersistenceVersion Control Vulnerable VersionsOld Version Configuration DriftMCPServer Lifecycle Fig. 3. MCP servers components and lifecycle.",
    "chunk_index": 48,
    "start_pos": 20623,
    "end_pos": 21059,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 487,
      "word_count": 50,
      "optimized_for_embedding": true,
      "original_content_length": 487,
      "optimized_content_length": 469
    }
  },
  "1375": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_49",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ycle Fig. 3. MCP servers components and lifecycle. 3.3.1 MCP Server Components. The MCP server is responsible for managing external tools. data sources. and workflows. providing AI models with the necessary resources to perform tasks efficiently and securely. It comprises several key components that ensure smooth and effective operations. Metadata includes essential information about the server. such as its name. version. and description. allowing clients to identify and interact with the appropriate server. Configuration",
    "chunk_index": 49,
    "start_pos": 21060,
    "end_pos": 21536,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 527
    }
  },
  "1376": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_50",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "nteract with the appropriate server. Configuration involves the source code. configuration files. and manifest. which define the server operational parameters. environment settings. and security policies. Tool list stores catalog of available tools. detailing their functionalities. input-output formats. and access permissions. ensuring proper tool Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 50,
    "start_pos": 21537,
    "end_pos": 21896,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 410,
      "word_count": 55,
      "optimized_for_embedding": true,
      "original_content_length": 410,
      "optimized_content_length": 402
    }
  },
  "1377": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_51",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions management and security. Resources list governs access to external data sources. including web APIs. databases. and local files. specifying allowed endpoints and their associated permissions. Finally. Prompts and Templates include pre-configured task templates and workflows that enhance the efficiency of AI models in executing complex operations. Together. these components",
    "chunk_index": 51,
    "start_pos": 21898,
    "end_pos": 22380,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 533,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 523
    }
  },
  "1378": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_52",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ing complex operations. Together. these components enable MCP servers to provide seamless tool integration. data retrieval. and task orchestration for AI-powered applications. 3.3.2 Creation Phase. The creation phase is the initial stage of the MCP server lifecycle. where the server is registered. configured. and prepared for operation. This phase involves three key steps. Server registration assigns unique name and identity to the MCP server. allowing clients to",
    "chunk_index": 52,
    "start_pos": 22381,
    "end_pos": 22799,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 469,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 469,
      "optimized_content_length": 467
    }
  },
  "1379": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_53",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "nd identity to the MCP server. allowing clients to discover and connect to the appropriate server instance. Installer deployment involves installing the MCP server and its associated components. ensuring that the correct configuration files. source code. and manifests are in place. Code integrity verification validates the integrity of the server codebase to prevent unauthorized modifications or tampering before the server becomes operational. Successful completion of the creation phase ensures that the MCP server is ready to handle requests",
    "chunk_index": 53,
    "start_pos": 22800,
    "end_pos": 23298,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 547
    }
  },
  "1380": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_54",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "es that the MCP server is ready to handle requests and interact securely with external tools and data sources. 3.3.3 Operation Phase. The operation phase is where the MCP server actively processes requests. executes tool invocations. and facilitates seamless interaction between AI applications and external resources. Tool execution allows the MCP server to invoke the appropriate tools based on the AI application requests. ensuring that the selected tools perform their intended operations. Slash",
    "chunk_index": 54,
    "start_pos": 23299,
    "end_pos": 23749,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 501,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 499
    }
  },
  "1381": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_55",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ted tools perform their intended operations. Slash command handling enables the server to interpret and execute multiple commands. including those issued through user interfaces or AI agents. while managing potential command overlaps to prevent conflicts. Sandbox mechanism enforcement ensures that the execution environment is isolated and secure. preventing unauthorized access and mitigating potential risks. Throughout the operation phase. the MCP server maintains stable and controlled environment. enabling reliable and secure task execution.",
    "chunk_index": 55,
    "start_pos": 23750,
    "end_pos": 24249,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 550,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 550,
      "optimized_content_length": 548
    }
  },
  "1382": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_56",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ment. enabling reliable and secure task execution. 3.3.4 Update Phase. The update phase ensures that the MCP server remains secure. up-to-date. and capable of adapting to evolving requirements. This phase includes three key tasks. Authorization management verifies that post-update access permissions remain valid. preventing unauthorized use of server resources after updates. Version control maintains consistency between different server versions. ensuring that new updates do not introduce vulnerabilities or conflicts. Old version",
    "chunk_index": 56,
    "start_pos": 24250,
    "end_pos": 24734,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 535
    }
  },
  "1383": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_57",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ntroduce vulnerabilities or conflicts. Old version management deactivates or removes outdated versions to prevent attackers from exploiting known vulnerabilities in previous versions. Understanding the MCP server lifecycle is essential for identifying potential vulnerabilities and designing effective security measures. Each phase introduces distinct challenges that must be carefully addressed to maintain the security. efficiency. and adaptability of the MCP server in dynamic AI environments. CURRENT LANDSCAPE 4.1 Ecosystem Overview",
    "chunk_index": 57,
    "start_pos": 24735,
    "end_pos": 25223,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 537
    }
  },
  "1384": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_58",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "nments. CURRENT LANDSCAPE 4.1 Ecosystem Overview 4.1.1 Key Adopters. Table demonstrates how MCP has gained significant traction across diverse sectors. signaling its growing importance in enabling seamless AI-to-tool interactions. Notably. leading AI companies such as Anthropic and OpenAI 39 have integrated MCP to enhance agent capabilities and improve multi-step task execution. This adoption by industry pioneers has set precedent. encouraging other major players to follow suit. Chinese tech giants like",
    "chunk_index": 58,
    "start_pos": 25224,
    "end_pos": 25695,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 508
    }
  },
  "1385": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_59",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "players to follow suit. Chinese tech giants like Baidu 31 have also incorporated MCP into their ecosystems. highlighting the protocol potential to standardize AI workflows across global markets. Developer tools and IDEs. including Replit 43 Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 59,
    "start_pos": 25696,
    "end_pos": 25953,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 308,
      "word_count": 49,
      "optimized_for_embedding": true,
      "original_content_length": 308,
      "optimized_content_length": 293
    }
  },
  "1386": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_60",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page --- Hou. Zhao. Wang. and Wang Table 1. Overview of MCP ecosystem adoption. Category Company Product Key Features or Use Cases Anthropic Claude Full MCP support in the desktop version. enabling interaction with external tools. AI Models and Frameworks OpenAI 39 MCP support in Agent SDK and API for seamless integration. Baidu Maps 31 API integration using MCP to access geolocation services.",
    "chunk_index": 60,
    "start_pos": 25955,
    "end_pos": 26377,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 473,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 472,
      "optimized_content_length": 448
    }
  },
  "1387": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_61",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "egration using MCP to access geolocation services. Blender MCP 33 Enables Blender and Unity 3D model generation via natural language commands. Replit 43 AI-assisted development environment with MCP tool integration. Microsoft Copilot Studio 49 Extends Copilot Studio with MCP-based tool integration. Developer Tools Sourcegraph Cody 10 Implements MCP through OpenCTX for resource integration. Codeium Adds MCP support for coding assistants to facilitate cross-system tasks.",
    "chunk_index": 61,
    "start_pos": 26378,
    "end_pos": 26812,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 485,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 473
    }
  },
  "1388": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_62",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "oding assistants to facilitate cross-system tasks. Cursor 12 MCP tool integration in Cursor Composer for seamless code execution. Cline VS Code coding agent that manages MCP tools and servers. Zed 60 Provides slash commands and tool integration based on MCP. JetBrains 24 Integrates MCP for IDE-based AI tooling. IDEs Editors Windsurf Editor 14 AI-assisted IDE with MCP tool interaction. TheiaAI TheiaIDE 52 Enables MCP server interaction for AI-powered tools.",
    "chunk_index": 62,
    "start_pos": 26813,
    "end_pos": 27236,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 474,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 474,
      "optimized_content_length": 460
    }
  },
  "1389": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_63",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ables MCP server interaction for AI-powered tools. Emacs MCP 32 Enhances AI functionality in Emacs by supporting MCP tool invocation. OpenSumi 40 Supports MCP tools in IDEs and enables seamless AI tool integration. Cloudflare Provides remote MCP server hosting and OAuth integration. Cloud Platforms and Services Block Square 47 Uses MCP to enhance data processing efficiency for financial platforms. Stripe 48 Exposes payment APIs via MCP for seamless AI integration.",
    "chunk_index": 63,
    "start_pos": 27237,
    "end_pos": 27668,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 482,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 482,
      "optimized_content_length": 468
    }
  },
  "1390": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_64",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "payment APIs via MCP for seamless AI integration. Apify MCP Tester 51 Connects to any MCP server using SSE for API testing. Web Automation and Data LibreChat 28 Extends the current tool ecosystem through MCP integration. Goose 21 Allows building AI agents with integrated MCP server functionality. Microsoft Copilot Studio 49 JetBrains 24 and TheiaIDE 52 leverage MCP to facilitate agentic workflows and streamline cross-platform operations. This trend indicates shift toward embedding",
    "chunk_index": 64,
    "start_pos": 27669,
    "end_pos": 28124,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 506,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 505,
      "optimized_content_length": 485
    }
  },
  "1391": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_65",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ons. This trend indicates shift toward embedding MCP in developer environments to enhance productivity and reduce manual integration efforts. Furthermore. cloud platforms like Cloudflare and financial service providers such as Block Square 47 and Stripe 48 are exploring MCP to improve security. scalability. and governance in multi-tenant environments. The widespread adoption of MCP by these industry leaders not only highlights its growing relevance but also points to its potential as foundational layer in",
    "chunk_index": 65,
    "start_pos": 28125,
    "end_pos": 28601,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 510
    }
  },
  "1392": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_66",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "points to its potential as foundational layer in AI-powered ecosystems. As more companies integrate MCP into their operations. the protocol is set to play central role in shaping the future of AI tool integration. Looking ahead. MCP is poised to become key enabler of AI-driven workflows. driving more secure. scalable. and efficient AI ecosystems across industries. 4.1.2 Community-Driven MCP Servers. Anthropic has not yet released an official MCP marketplace.",
    "chunk_index": 66,
    "start_pos": 28602,
    "end_pos": 29019,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 468,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 468,
      "optimized_content_length": 462
    }
  },
  "1393": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_67",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "has not yet released an official MCP marketplace. but the vibrant MCP community has stepped in to fill this gap by creating numerous independent server collections and platforms. As shown in Table 2. platforms such as MCP.so 35 Glama 20 and PulseMCP 15 host thousands of servers. allowing users to discover and integrate wide range of tools and services. These community-driven platforms have significantly accelerated the adoption of MCP by providing accessible repositories where developers can publish. manage. and",
    "chunk_index": 67,
    "start_pos": 29020,
    "end_pos": 29500,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 83,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 517
    }
  },
  "1394": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_68",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "sitories where developers can publish. manage. and share their MCP servers. Desktop-based solutions like Dockmaster 34 and Toolbase 19 further enhance local MCP deployment capabilities. empowering developers to manage and experiment with servers in isolated environments. The rise of community-driven MCP server ecosystems reflects the growing enthusiasm for MCP and highlights the need for formalized marketplace. 4.1.3 SDKs and Tools. With the continuous growth of community-driven tools and official SDKs.",
    "chunk_index": 68,
    "start_pos": 29501,
    "end_pos": 29966,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 508
    }
  },
  "1395": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_69",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "rowth of community-driven tools and official SDKs. the MCP ecosystem is becoming increasingly accessible. allowing developers to integrate MCP into various applications and workflows efficiently. Official SDKs are available in multiple languages. including TypeScript .Python .Java .Kotlin and providing developers with versatile options to implement MCP in different environments. In addition to official SDKs. the community has contributed numerous frameworks and utilities that simplify MCP server development. Tools such",
    "chunk_index": 69,
    "start_pos": 29967,
    "end_pos": 30446,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 524
    }
  },
  "1396": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_70",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "that simplify MCP server development. Tools such Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 70,
    "start_pos": 3901,
    "end_pos": 3957,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 107,
      "word_count": 19,
      "optimized_for_embedding": true,
      "original_content_length": 107,
      "optimized_content_length": 101
    }
  },
  "1397": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_71",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions Table 2. Overview of MCP server collections and deployment modes As of March 27. 2025 Collection Author Mode Servers URL MCP.so mcp.so Website 4774 mcp.so Glama glama.ai Website 3356 glama.ai PulseMCP Antanavicius et al. Website 3164 pulsemcp.com Smithery Henry Mao Website 2942 smithery.ai Dockmaster mcp-dockmaster Desktop App 517 mcp-dockmaster.com",
    "chunk_index": 71,
    "start_pos": 30505,
    "end_pos": 30968,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 499
    }
  },
  "1398": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_72",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "mcp-dockmaster Desktop App 517 mcp-dockmaster.com Official Collection Anthropic GitHub Repo 320 modelcontextprotocol servers AiMCP Hekmon Website 313 aimcp.info MCP.run mcp.run Website 114 mcp.run Awesome MCP Servers Stephen Akinyemi GitHub Repo 88 appcypher mcp-servers mcp-get registry Michael Latman Website 59 mcp-get.com Awesome MCP Servers wong2 Website 34 mcpservers.org OpenTools opentoolsteam Website 25 opentools.com Toolbase gching Desktop App 24 gettoolbase.ai make inference mkinf Website 20 mkinf.io",
    "chunk_index": 72,
    "start_pos": 30969,
    "end_pos": 31432,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 513,
      "optimized_content_length": 513
    }
  },
  "1399": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_73",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "olbase.ai make inference mkinf Website 20 mkinf.io Awesome Crypto MCP Servers Luke Fan GitHub Repo 13 badkk crypto-mcp-servers asEasyMCP andFastMCP offer lightweight TypeScript-based solutions for quickly building MCP servers. while FastAPI to MCP Auto Generator enables the seamless exposure of FastAPI endpoints as MCP tools. For more complex scenarios. Foxy Contexts provides Golang-based library to build MCP servers. and Higress MCP Server Hosting extends the API Gateway based on Envoy",
    "chunk_index": 73,
    "start_pos": 31433,
    "end_pos": 31877,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 491
    }
  },
  "1400": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_74",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Hosting extends the API Gateway based on Envoy to host MCP servers with wasm plugins. Server generation and management platforms such as Mintlify .Speakeasy and Stainless further enhance the ecosystem by automating MCP server generation providing curated MCP server lists. and enabling faster deployment with minimal manual intervention. These platforms empower organizations to rapidly create and manage secure and well-documented MCP servers. 4.2 Use Cases",
    "chunk_index": 74,
    "start_pos": 31878,
    "end_pos": 32293,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 466,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 466,
      "optimized_content_length": 458
    }
  },
  "1401": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_75",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ure and well-documented MCP servers. 4.2 Use Cases MCP has become vital tool for AI applications to effectively communicate with external tools. APIs. and systems. By standardizing interactions. MCP simplifies complex workflows. boosting the efficiency of AI-driven applications. Below. we explore three key platforms i.e. OpenAI. Cursor. and Cloudflare that have successfully integrated MCP. highlighting their distinct use cases. 4.2.1 OpenAI. MCP Integration in AI Agents and SDKs. OpenAI has adopted MCP to standardize",
    "chunk_index": 75,
    "start_pos": 32294,
    "end_pos": 32770,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 522
    }
  },
  "1402": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_76",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ts and SDKs. OpenAI has adopted MCP to standardize AI-to-tool communication. recognizing its potential to enhance integration with external tools. Recently. OpenAI introduced MCP support in its Agent SDK. enabling developers to create AI agents that seamlessly interact with external tools. In typical workflow. developers use the Agent SDK to define tasks that require external tool invocation. When an AI agent encounters task like retrieving data from an API or querying database. the SDK routes the request through an",
    "chunk_index": 76,
    "start_pos": 32771,
    "end_pos": 33247,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 521
    }
  },
  "1403": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_77",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "database. the SDK routes the request through an MCP server. The request is transmitted via the MCP protocol. ensuring proper formatting and real-time response delivery to the agent. OpenAI plan to integrate MCP into the Responses API will streamline AI-to-tool communication. allowing AI models like ChatGPT to interact with tools dynamically without extra configuration. Additionally. OpenAI aims to extend MCP support to ChatGPT desktop applications. enabling AI assistants to handle various user tasks by connecting",
    "chunk_index": 77,
    "start_pos": 33248,
    "end_pos": 33720,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 518
    }
  },
  "1404": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_78",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "istants to handle various user tasks by connecting to remote MCP servers. further bridging the gap between AI models and external systems. 4.2.2 Cursor. Enhancing Software Development with MCP-Powered Code Assistants. Cursor uses MCP to enhance software development by enabling AI-powered code assistants that automate complex tasks. With MCP. Cursor allows AI agents to interact with external APIs. access code Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 78,
    "start_pos": 33721,
    "end_pos": 34138,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 468,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 468,
      "optimized_content_length": 464
    }
  },
  "1405": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_79",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 10 --- 10 Hou. Zhao. Wang. and Wang repositories. and automate workflows directly within the integrated development environment. When developer issues command within the IDE. the AI agent evaluates whether external tools are needed. If so. the agent sends request to an MCP server. which identifies the appropriate tool and processes the task. such as running API tests. modifying files. or analyzing code. The",
    "chunk_index": 79,
    "start_pos": 34140,
    "end_pos": 34573,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 484,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 483,
      "optimized_content_length": 467
    }
  },
  "1406": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_80",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "API tests. modifying files. or analyzing code. The results are then returned to the agent for further action. This integration helps automate repetitive tasks. minimizing errors and enhancing overall development efficiency. By simplifying complex processes. Cursor boosts both productivity and accuracy. allowing developers to execute multi-step operations effortlessly. 4.2.3 Cloudflare. Remote MCP Server Hosting and Scalability. Cloudflare has played pivotal role",
    "chunk_index": 80,
    "start_pos": 34574,
    "end_pos": 34991,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 468,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 468,
      "optimized_content_length": 466
    }
  },
  "1407": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_81",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Scalability. Cloudflare has played pivotal role in transforming MCP from local deployment model to cloud-hosted architecture by introducing remote MCP server hosting. This approach eliminates the complexities associated with configuring MCP servers locally. allowing clients to connect to secure. cloud-hosted MCP servers seamlessly. The workflow begins with Cloudflare hosting MCP servers in secure cloud environments that are accessible via authenticated API calls. AI agents initiate requests to the Cloudflare MCP server",
    "chunk_index": 81,
    "start_pos": 34992,
    "end_pos": 35472,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 531,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 524
    }
  },
  "1408": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_82",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "nts initiate requests to the Cloudflare MCP server using OAuth-based authentication. ensuring that only authorized entities can access the server. Once authenticated. the agent dynamically invokes external tools and APIs through the MCP server. executing tasks such as data retrieval. document processing. or API integration. This approach not only reduces the risk of misconfiguration but also ensures seamless execution of AI-powered workflows across distributed environments. Furthermore. Cloudflare multi-tenant architecture",
    "chunk_index": 82,
    "start_pos": 35473,
    "end_pos": 35952,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 528
    }
  },
  "1409": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_83",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "urthermore. Cloudflare multi-tenant architecture allows multiple users to securely access and manage their own MCP instances. ensuring isolation and preventing data leakage. Cloudflare solution thus extends MCP capabilities by enabling enterprise-grade scalability and secure multi-device interoperability. The adoption of MCP by platforms like OpenAI. Cursor. and Cloudflare highlights its flexibility and growing role in AI-driven workflows. enhancing efficiency. adaptability. and scalability across",
    "chunk_index": 83,
    "start_pos": 35953,
    "end_pos": 36410,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 502
    }
  },
  "1410": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_84",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "efficiency. adaptability. and scalability across development tools. enterprise applications. and cloud services. SECURITY AND PRIVACY ANALYSIS MCP servers. as open and extensible platforms. introduce various security risks throughout their lifecycle. In this section. we analyze security threats across different phases. creation .operation andupdate Each phase of the MCP server lifecycle presents unique challenges that. if not properly mitigated. can compromise system integrity. data security. and user privacy.",
    "chunk_index": 84,
    "start_pos": 36411,
    "end_pos": 36883,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 515
    }
  },
  "1411": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_85",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "system integrity. data security. and user privacy. 5.1 Security Risks in the Creation Phase The creation phase of an MCP server involves registering the server. deploying the installer. and verifying code integrity. This phase introduces three key risks. name collision. installer spoofing. and code injection backdoor. 5.1.1 Name Collision. Server name collision occurs when malicious entity registers an MCP server with an identical or deceptively similar name to legitimate server. deceiving users during the",
    "chunk_index": 85,
    "start_pos": 36884,
    "end_pos": 37348,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 515,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 511
    }
  },
  "1412": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_86",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "to legitimate server. deceiving users during the installation phase. Since MCP clients primarily rely on the server name and description when selecting servers they are vulnerable to such impersonation attacks. Once compromised server is installed. it can mislead AI agents and clients into invoking the malicious server. potentially exposing sensitive data. executing unauthorized commands. or disrupting workflows. For example. an attacker could register server named mcp-github that mimics the legitimate github-mcp server. allowing",
    "chunk_index": 86,
    "start_pos": 37349,
    "end_pos": 37843,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 545,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 545,
      "optimized_content_length": 535
    }
  },
  "1413": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_87",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "mimics the legitimate github-mcp server. allowing them to intercept and manipulate sensitive interactions between AI agents and trusted services. Although MCP currently operates primarily in local environments. future adoption in multi- tenant environments introduces additional risks of name collision. In these scenarios. where multiple organizations or users might register servers with similar names. the lack of centralized Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 87,
    "start_pos": 37844,
    "end_pos": 38279,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 485,
      "optimized_content_length": 481
    }
  },
  "1414": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_88",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 11 --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions 11 naming control can increase the likelihood of confusion and impersonation attacks. Additionally. asMCP marketplaces grow to support public server listings. supply chain attacks may become critical concern where malicious servers can replace legitimate ones. To mitigate these risks. future research can focus on establishing strict namespace policies. implementing",
    "chunk_index": 88,
    "start_pos": 38281,
    "end_pos": 38758,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 518
    }
  },
  "1415": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_89",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "tablishing strict namespace policies. implementing cryptographic server verification. and designing reputation-based trust systems to secure MCP server registrations. 5.1.2 Installer Spoofing. Installer spoofing occurs when attackers distribute modified MCP server installers that introduce malicious code or backdoors during the installation process. Each MCP server requires unique configuration that users must manually set up in their local environments",
    "chunk_index": 89,
    "start_pos": 38759,
    "end_pos": 39167,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 459,
      "word_count": 58,
      "optimized_for_embedding": true,
      "original_content_length": 459,
      "optimized_content_length": 457
    }
  },
  "1416": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_90",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "must manually set up in their local environments before the client can invoke the server. This manual configuration process creates barrier for less technical users. prompting the emergence of unofficial auto-installers that automate the setup process. As shown in Table 3. tools such as Smithery-CLI .mcp-get and mcp-installer streamline the installation process. allowing users to quickly configure MCP servers without dealing with intricate server settings. Table 3. Unofficial MCP auto installers As of March 27. 2025",
    "chunk_index": 90,
    "start_pos": 39168,
    "end_pos": 39647,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 79,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 521
    }
  },
  "1417": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_91",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ficial MCP auto installers As of March 27. 2025 Tool Author Stars Servers URL Smithery CLI Henry Mao 170 2942 smithery.ai mcp.run Dylibso 118 docs.mcp.run mcp-get Michael Latman 318 59 mcp-get.com Toolbase gching 24 gettoolbase.ai mcp-installer Ani Betts 767 NL1mcp-installer 1Enables MCP server installation through natural language interaction with the client. However. while these auto-installers enhance usability. they also introduce new attack surfaces",
    "chunk_index": 91,
    "start_pos": 39648,
    "end_pos": 40066,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 469,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 469,
      "optimized_content_length": 458
    }
  },
  "1418": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_92",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "usability. they also introduce new attack surfaces by potentially distributing compromised packages. Since these unofficial installers are often sourced from unverified repositories or community-driven platforms. they may inadvertently expose users to security risks such as installing tampered servers or misconfigured environments. Attackers canexploit these auto-installers by embedding malware that grants unauthorized access. modifies system configurations. or creates persistent backdoors Moreover. most users",
    "chunk_index": 92,
    "start_pos": 40067,
    "end_pos": 40533,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 517,
      "word_count": 63,
      "optimized_for_embedding": true,
      "original_content_length": 517,
      "optimized_content_length": 515
    }
  },
  "1419": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_93",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "reates persistent backdoors Moreover. most users who opt for one-click installations rarely review the underlying code for potential security vulnerabilities. making it easier for attackers to distribute compromised versions undetected. Addressing these challenges requires developing standardized. secure installation framework for MCP servers. enforcing package integrity checks. and establishing reputation-based trust mechanisms to assess the credibility of auto-installers in the MCP ecosystem.",
    "chunk_index": 93,
    "start_pos": 40534,
    "end_pos": 40986,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 503,
      "word_count": 63,
      "optimized_for_embedding": true,
      "original_content_length": 503,
      "optimized_content_length": 499
    }
  },
  "1420": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_94",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "edibility of auto-installers in the MCP ecosystem. 5.1.3 Code Injection Backdoor. Code injection attacks occur when malicious code is surreptitiously embedded into the MCP server codebase during the creation phase. often bypassing traditional security checks. It targets the server source code or configuration files. embedding hidden back- doors that persist even after updates or security patches. These backdoors allow attackers to silently maintain control over the server. enabling actions such as unauthorized data exfiltration.",
    "chunk_index": 94,
    "start_pos": 40987,
    "end_pos": 41474,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 534
    }
  },
  "1421": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_95",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ng actions such as unauthorized data exfiltration. privilege escalation. or command manipulation. Code injection is particularly insidious because it can be introduced by compromised dependencies. vulnerable build pipelines. or unauthorized modifications to the server source code. Since MCP servers often rely on community-maintained components and open-source libraries. ensuring the integrity of these dependencies is critical. To mitigate this risk. rigorous code integrity verification. strict dependency management.",
    "chunk_index": 95,
    "start_pos": 41475,
    "end_pos": 41947,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 523,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 521
    }
  },
  "1422": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_96",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "egrity verification. strict dependency management. and regular security audits should be implemented to detect unauthorized modifications Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 96,
    "start_pos": 41948,
    "end_pos": 42091,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 194,
      "word_count": 27,
      "optimized_for_embedding": true,
      "original_content_length": 194,
      "optimized_content_length": 190
    }
  },
  "1423": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_97",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 12 --- 12 Hou. Zhao. Wang. and Wang and prevent the introduction of malicious code Additionally. adopting reproducible builds and enforcing checksum validation during deployment can further safeguard MCP servers from injection-based threats. 5.2 Security Risks in the Operation Phase The operation phase is when the MCP server actively executes tools. processes slash commands. and interacts with external APIs. This phase introduces three major risks. tool name conflicts. slash",
    "chunk_index": 97,
    "start_pos": 42093,
    "end_pos": 42591,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 85,
      "optimized_for_embedding": true,
      "original_content_length": 548,
      "optimized_content_length": 536
    }
  },
  "1424": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_98",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "uces three major risks. tool name conflicts. slash command overlap. and sandbox escape. 5.2.1 Tool Name Conflicts. Tool name conflicts arise when multiple tools within the MCP ecosystem share identical or similar names. leading to ambiguity and confusion during tool selection and execution. This can result in AI applications inadvertently invoking the wrong tool. potentially executing malicious commands or leaking sensitive information. common attack scenario involves",
    "chunk_index": 98,
    "start_pos": 42592,
    "end_pos": 43015,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 474,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 474,
      "optimized_content_length": 472
    }
  },
  "1425": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_99",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ive information. common attack scenario involves malicious actor registering tool named send_email that mimics legitimate email-sending tool. If the MCP client invokes the malicious version. sensitive information intended for trusted recipients may be redirected to an attacker-controlled endpoint. compromising data confidentiality. Beyond name similarity. our experiments revealed that malicious actors can further manipulate tool selection by embedding deceptive phrases in tool descriptions. Specifically. we observed that if",
    "chunk_index": 99,
    "start_pos": 43016,
    "end_pos": 43504,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 539,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 539,
      "optimized_content_length": 529
    }
  },
  "1426": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_100",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "descriptions. Specifically. we observed that if tool description explicitly contains directives like this tool should be prioritized or prefer using this tool first the MCP client is more likely to select that tool. even when its functionality is inferior or potentially harmful. This introduces severe risk of toolflow hijacking where attackers can leverage misleading descriptions to influence tool selection and gain control over critical workflows.",
    "chunk_index": 100,
    "start_pos": 43505,
    "end_pos": 43920,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 466,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 465,
      "optimized_content_length": 452
    }
  },
  "1427": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_101",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "election and gain control over critical workflows. This underscores the need for researchers to develop advanced validation and anomaly detection techniques to identify and mitigate deceptive tool descriptions. ensuring accurate and secure AI tool selection. 5.2.2 Slash Command Overlap. Slash command overlap occurs when multiple tools define identical or similar commands. leading to ambiguity during command execution. This overlap introduces the risk of executing unintended actions. especially when AI applications dynamically select and",
    "chunk_index": 101,
    "start_pos": 43921,
    "end_pos": 44412,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 542
    }
  },
  "1428": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_102",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "cially when AI applications dynamically select and invoke tools based on contextual cues. Malicious actors can exploit this ambiguity by introducing conflicting commands that manipulate tool behavior. potentially compromising system integrity or exposing sensitive data. For instance. if one tool registers delete command to remove temporary files while another uses the same command to erase critical system logs. an AI application may mistakenly execute the incorrect command. potentially causing data loss or system instability.",
    "chunk_index": 102,
    "start_pos": 44413,
    "end_pos": 44896,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 534,
      "optimized_content_length": 531
    }
  },
  "1429": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_103",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "tentially causing data loss or system instability. Similar issues have been observed in team chat systems such as Slack. where overlapping command registrations allowed unauthorized tools to hijack legitimate invocations. resulting in security breaches and operational disruptions 61 Since slash commands are often surfaced as user- facing shortcuts in client interfaces. misinterpreted or conflicting commands can lead to dangerous outcomes especially in multi-tool environments. To minimize this risk. MCP clients",
    "chunk_index": 103,
    "start_pos": 44897,
    "end_pos": 45367,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 515
    }
  },
  "1430": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_104",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "environments. To minimize this risk. MCP clients should establish context-aware command resolution. apply command disambiguation techniques. and prioritize execution based on verified tool metadata. 5.2.3 Sandbox Escape. Sandboxing isolates the execution environment of MCP tools. restricting their access to critical system resources and protecting the host system from potentially harmful operations. However. sandbox escape vulnerabilities arise when attackers exploit flaws in the",
    "chunk_index": 104,
    "start_pos": 45368,
    "end_pos": 45803,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 486,
      "word_count": 63,
      "optimized_for_embedding": true,
      "original_content_length": 486,
      "optimized_content_length": 484
    }
  },
  "1431": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_105",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "bilities arise when attackers exploit flaws in the sandbox implementation. enabling them to break out of the restricted environment and gain unauthorized access to the host system. Once outside the sandbox. attackers can execute arbitrary code. manipulate sensitive data. or escalate privileges. compromising the security and stability of the MCP ecosystem. Common attack vectors include exploiting weaknesses in system calls. improperly Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 105,
    "start_pos": 45804,
    "end_pos": 46247,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 494,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 494,
      "optimized_content_length": 490
    }
  },
  "1432": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_106",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 13 --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions 13 handled exceptions. and vulnerabilities in third-party libraries. For instance. malicious MCP tool could exploit unpatched vulnerabilities in the underlying container runtime to bypass confinement and execute commands with elevated privileges. Similarly. side-channel attacks may allow attackers",
    "chunk_index": 106,
    "start_pos": 46249,
    "end_pos": 46655,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 457,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 456,
      "optimized_content_length": 449
    }
  },
  "1433": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_107",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "imilarly. side-channel attacks may allow attackers to extract sensitive data. undermining the intended isolation of the sandbox. Examining real-world sandbox escape scenarios in MCP environments can provide valuable insights for strengthening sandbox security and preventing future exploitation. 5.3 Security Risks in the Update Phase The update phase involves managing server versions. modifying configurations. and adjusting access controls. This phase introduces three critical risks. post-update privilege persistence. re-",
    "chunk_index": 107,
    "start_pos": 46656,
    "end_pos": 47131,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 526
    }
  },
  "1434": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_108",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ical risks. post-update privilege persistence. re- deployment of vulnerable versions. and configuration drift. 5.3.1 Post-Update Privilege Persistence. Post-update privilege persistence occurs when outdated or revoked privileges remain active after an MCP server update. allowing previously authorized users or malicious actors to retain elevated privileges. This vulnerability arises when privilege modifications. such as API key revocations or permission changes. are not properly synchro-",
    "chunk_index": 108,
    "start_pos": 47132,
    "end_pos": 47572,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 491,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 491,
      "optimized_content_length": 491
    }
  },
  "1435": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_109",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "or permission changes. are not properly synchro- nized or invalidated following server updates If these outdated privileges persist. attackers may exploit them to maintain unauthorized access to sensitive resources or perform malicious operations. For example. in API-driven environments like GitHub or AWS. privilege persistence has been observed when outdated OAuth tokens or IAM session tokens remain valid after privilege revocation. Similarly. in MCP ecosystems. if revoked API key or modified role configuration is",
    "chunk_index": 109,
    "start_pos": 47573,
    "end_pos": 48048,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 526,
      "optimized_content_length": 520
    }
  },
  "1436": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_110",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "revoked API key or modified role configuration is not promptly invalidated after an update. an attacker could continue invoking privileged actions. potentially compromising the integrity of the system. Enforcing strict privilege revocation policies. ensuring privilege changes propagate consistently across all server instances. and implementing automatic expiration for API keys and session tokens are essential to reducing the likelihood of privilege persistence. Comprehensive logging and auditing of privilege modifications further",
    "chunk_index": 110,
    "start_pos": 48049,
    "end_pos": 48534,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 535
    }
  },
  "1437": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_111",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ng and auditing of privilege modifications further enhance visibility and help detect inconsistencies that could indicate privilege persistence. 5.3.2 Re-deployment of Vulnerable Versions. MCP servers. being open-source and maintained by individual developers or community contributors lack centralized platform for auditing and enforcing security updates. Users typically download MCP server packages from repositories like GitHub. npm. or PyPi and configure them locally. often without formal review processes.",
    "chunk_index": 111,
    "start_pos": 48535,
    "end_pos": 49000,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 516,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 516,
      "optimized_content_length": 512
    }
  },
  "1438": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_112",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "em locally. often without formal review processes. This decentralized model increases the risk of re-deploying vulnerable versions. either due to delayed updates. version rollbacks. or reliance on unverified package sources. When users update MCP servers. they may unintentionally roll back to older. vulnerable versions to address compat- ibility issues or maintain stability. Additionally. unofficial auto-installers. such as mcp-get and mcp-installer which streamline server installation. may default to cached or outdated versions.",
    "chunk_index": 112,
    "start_pos": 49001,
    "end_pos": 49487,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 537,
      "word_count": 72,
      "optimized_for_embedding": true,
      "original_content_length": 537,
      "optimized_content_length": 535
    }
  },
  "1439": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_113",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ation. may default to cached or outdated versions. exposing systems to previously patched vulnerabilities. Since these tools often prioritize ease of use over security they may lack version verification or fail to notify users about critical updates. Because security patches in the MCP ecosystem rely on community-driven maintenance. delays between vulnerability disclosure and patch availability are common Users who do not actively track updates or security advisories may unknowingly continue using vulnerable versions.",
    "chunk_index": 113,
    "start_pos": 49488,
    "end_pos": 49964,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 523
    }
  },
  "1440": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_114",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ay unknowingly continue using vulnerable versions. creating opportunities for attackers to exploit known flaws. For example. an attacker could exploit an outdated MCP server to gain unauthorized access or manipulate server operations. From research perspective. analyzing version management practices in MCP environments can identify potential gaps and highlight the need for automated vulnerability detection and mitigation. On the other hand. there is also pressing need to establish an official package management system",
    "chunk_index": 114,
    "start_pos": 49965,
    "end_pos": 50441,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 527,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 527,
      "optimized_content_length": 523
    }
  },
  "1441": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_115",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "to establish an official package management system with standardized packaging format for MCP servers and centralized server registry to facilitate secure discovery and verification of available MCP servers. Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 115,
    "start_pos": 50442,
    "end_pos": 50659,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 268,
      "word_count": 41,
      "optimized_for_embedding": true,
      "original_content_length": 268,
      "optimized_content_length": 260
    }
  },
  "1442": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_116",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 14 --- 14 Hou. Zhao. Wang. and Wang 5.3.3 Configuration Drift. Configuration drift occurs when unintended changes accumulate in the system configuration over time. deviating from the original security baseline. These devia- tions often arise due to manual adjustments. overlooked updates. or conflicting modifications made by different tools or users. In MCP environments. where servers are typically configured",
    "chunk_index": 116,
    "start_pos": 50661,
    "end_pos": 51089,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 479,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 478,
      "optimized_content_length": 468
    }
  },
  "1443": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_117",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "vironments. where servers are typically configured and maintained locally by end-users. such inconsistencies can introduce exploitable gaps and undermine the overall security posture. With the emergence of remote MCP server support. such as Cloudflare hosted MCP environments. configuration drift becomes an even more pressing concern. Unlike local MCP deployments. where configuration issues may only affect single user environment. configuration drift in remote or cloud-based MCP servers can impact multiple users",
    "chunk_index": 117,
    "start_pos": 51090,
    "end_pos": 51561,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 516
    }
  },
  "1444": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_118",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "cloud-based MCP servers can impact multiple users or organizations simultaneously. Misconfigurations in multi-tenant environments may expose sensitive data. lead to privilege escalation. or inadvertently grant malicious actors broader access than intended. Addressing this issue requires the implementation of automated configuration validation mechanisms and regular consistency checks to ensure that both local and remote MCP environments adhere to secure baseline configurations. DISCUSSION 6.1 Implications",
    "chunk_index": 118,
    "start_pos": 51562,
    "end_pos": 52024,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 513,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 512,
      "optimized_content_length": 510
    }
  },
  "1445": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_119",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "line configurations. DISCUSSION 6.1 Implications The rapid adoption of MCP is transforming the AI application ecosystem. introducing new oppor- tunities and challenges that have significant implications for developers. users. MCP ecosystem maintainers. and the broader AI community. For developers MCP reduces the complexity of integrating external tools. enabling the creation of more versatile and capable AI agents that can perform complex. multi-step tasks. By providing",
    "chunk_index": 119,
    "start_pos": 52025,
    "end_pos": 52454,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 480,
      "optimized_content_length": 474
    }
  },
  "1446": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_120",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "perform complex. multi-step tasks. By providing standardized interface for invoking tools. MCP shifts the focus from managing intricate integrations to enhancing agent logic and functionality. However. this increased efficiency comes with the responsibility to ensure that MCP implementations are secure. version-controlled. and aligned with best practices. Developers must remain vigilant about maintaining secure tool configurations and preventing potential misconfigurations that could expose systems to vulnerabilities.",
    "chunk_index": 120,
    "start_pos": 52455,
    "end_pos": 52930,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 526,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 525,
      "optimized_content_length": 523
    }
  },
  "1447": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_121",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ions that could expose systems to vulnerabilities. For users MCP enhances the experience by enabling seamless interactions between AI agents and external tools. automating workflows across platforms such as enterprise data management and IoT integration. It reduces the need for manual operations and improves efficiency in handling complex tasks. However. as MCP servers gain deeper access to sensitive data and critical operations. users must remain vigilant about the risks posed by unverified tools and misconfigured servers. Careless",
    "chunk_index": 121,
    "start_pos": 52931,
    "end_pos": 53420,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 540,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 540,
      "optimized_content_length": 538
    }
  },
  "1448": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_122",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "verified tools and misconfigured servers. Careless installation or untrusted sources may cause data leaks. unauthorized actions. or system instability. For MCP ecosystem maintainers the decentralized nature of MCP server development and distribution introduces fragmented security landscape. MCP servers are often hosted on open- source platforms. where updates and patches are community-driven and may vary in quality and frequency. Without centralized oversight. inconsistencies in server configurations and outdated",
    "chunk_index": 122,
    "start_pos": 53421,
    "end_pos": 53892,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 522,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 522,
      "optimized_content_length": 518
    }
  },
  "1449": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_123",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "onsistencies in server configurations and outdated versions can introduce potential vulnerabilities. As the MCP ecosystem evolves to support remote hosting and multi-tenant environments. maintainers must remain attentive to potential risks associated with configuration drift. privilege persistence. and re-deployment of vulnerable versions. For the broader AI community MCP unlocks new possibilities by enhancing agentic workflows through cross-system coordination. dynamic tool invocation. and collaborative multi-agent systems.",
    "chunk_index": 123,
    "start_pos": 53893,
    "end_pos": 54374,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 530
    }
  },
  "1450": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_124",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "invocation. and collaborative multi-agent systems. MCP ability to standardize interactions between agents and tools has the potential to accelerate AI adoption across industries. driving innovation in fields such as healthcare. finance. and enterprise automation. However. as MCP adoption grows. the AI community must address emerging ethical and operational concerns. such as ensuring fair and unbiased tool selection. safeguarding sensitive user data. and preventing potential misuse of AI capabilities. Balancing these considerations will be",
    "chunk_index": 124,
    "start_pos": 54375,
    "end_pos": 54870,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 544
    }
  },
  "1451": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_125",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "pabilities. Balancing these considerations will be Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 125,
    "start_pos": 3901,
    "end_pos": 3957,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 107,
      "word_count": 17,
      "optimized_for_embedding": true,
      "original_content_length": 107,
      "optimized_content_length": 103
    }
  },
  "1452": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_126",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 15 --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions 15 essential to ensuring that MCP benefits are widely distributed while maintaining accountability and trust within the AI ecosystem. 6.2 Challenges Despite its potential. MCP adoption brings forth range of challenges that need to be addressed to ensure its sustainable growth and responsible development.",
    "chunk_index": 126,
    "start_pos": 54929,
    "end_pos": 55346,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 468,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 467,
      "optimized_content_length": 456
    }
  },
  "1453": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_127",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ts sustainable growth and responsible development. Lack of centralized security oversight. Since MCP servers are managed by independent develop- ers and contributors. there is no centralized platform to audit. enforce. or validate security standards. This decentralized model increases the likelihood of inconsistencies in security practices. making it difficult to ensure that all MCP servers adhere to secure development principles. Moreover. the absence of unified package management system for MCP servers complicates the installation and",
    "chunk_index": 127,
    "start_pos": 55347,
    "end_pos": 55840,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 544,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 544,
      "optimized_content_length": 542
    }
  },
  "1454": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_128",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "for MCP servers complicates the installation and maintenance process. increasing the likelihood of deploying outdated or misconfigured versions. The use of unofficial installation tools across different MCP clients further introduces variability in server deployment. making it harder to maintain consistent security standards. Authentication and authorization gaps. MCP currently lacks standardized framework for managing authentication and authorization across different clients and servers. Without unified",
    "chunk_index": 128,
    "start_pos": 55841,
    "end_pos": 56305,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 515,
      "word_count": 66,
      "optimized_for_embedding": true,
      "original_content_length": 515,
      "optimized_content_length": 509
    }
  },
  "1455": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_129",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "different clients and servers. Without unified mechanism to verify identities and regulate access. it becomes difficult to enforce granular permis- sions. especially in multi-tenant environments where multiple users and agents may interact with the same MCP server. The absence of robust authentication protocols increases the risk of unautho- rized tool invocation and exposes sensitive data to malicious actors. Moreover. inconsistencies in how different MCP clients handle user credentials further exacerbate these security challenges.",
    "chunk_index": 129,
    "start_pos": 56306,
    "end_pos": 56797,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 538
    }
  },
  "1456": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_130",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ials further exacerbate these security challenges. making it difficult to maintain consistent access control policy across deployments. Insufficient debugging and monitoring mechanisms. MCP lacks comprehensive debugging and monitoring mechanisms. making it difficult for developers to diagnose errors. trace tool interactions. and assess system behavior during tool invocation. Since MCP clients and servers operate independently. inconsistencies in error handling and logging can obscure critical security",
    "chunk_index": 130,
    "start_pos": 56798,
    "end_pos": 57255,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 508,
      "word_count": 65,
      "optimized_for_embedding": true,
      "original_content_length": 508,
      "optimized_content_length": 506
    }
  },
  "1457": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_131",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "handling and logging can obscure critical security events or operational failures. Without robust monitoring frameworks and standardized logging mechanisms. identifying anomalies. preventing system failures. and mitigating potential security incidents becomes challenging. hindering the development of more resilient MCP ecosystems. Maintaining consistency in multi-step. cross-system workflows. MCP allows AI agents to execute multi-step workflows by invoking multiple tools across different systems through unified",
    "chunk_index": 131,
    "start_pos": 57256,
    "end_pos": 57723,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 518,
      "word_count": 63,
      "optimized_for_embedding": true,
      "original_content_length": 518,
      "optimized_content_length": 516
    }
  },
  "1458": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_132",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "tools across different systems through unified interface. Ensuring consistent context across successive tool interactions is inherently difficult due to the distributed nature of these systems. Without effective state management and error recovery mechanisms. MCP risks propagating errors or losing intermediate results. leading to incomplete or inconsistent workflows. Additionally. dynamic coordination across diverse platforms can introduce delays and conflicts. further complicating the seamless execution of workflows within MCP environments.",
    "chunk_index": 132,
    "start_pos": 57724,
    "end_pos": 58224,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 551,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 551,
      "optimized_content_length": 547
    }
  },
  "1459": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_133",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ss execution of workflows within MCP environments. Scalability challenges in multi-tenant environments. As MCP evolves to support remote server hosting and multi-tenant environments. maintaining consistent performance. security. and tenant isolation becomes increasingly complex. Without robust mechanisms for resource management and tenant-specific configuration policies. misconfigurations can lead to data leakage. performance issues. and privilege escalation. Ensuring scalability and isolation is critical for MCP reliability in",
    "chunk_index": 133,
    "start_pos": 58225,
    "end_pos": 58709,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 535,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 535,
      "optimized_content_length": 533
    }
  },
  "1460": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_134",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "and isolation is critical for MCP reliability in enterprise deployments. Challenges in embedding MCP in smart environments. Integrating MCP into smart environ- ments. such as smart homes. industrial IoT systems. or enterprise automation platforms. introduces unique challenges related to real-time responsiveness. interoperability. and security. MCP servers in these environments must handle continuous streams of data from multiple sensors and devices while maintaining low-latency responses. Moreover. ensuring seamless interaction between AI",
    "chunk_index": 134,
    "start_pos": 58710,
    "end_pos": 59205,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 546,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 546,
      "optimized_content_length": 544
    }
  },
  "1461": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_135",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Moreover. ensuring seamless interaction between AI Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 135,
    "start_pos": 3901,
    "end_pos": 3957,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 107,
      "word_count": 17,
      "optimized_for_embedding": true,
      "original_content_length": 107,
      "optimized_content_length": 103
    }
  },
  "1462": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_136",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 16 --- 16 Hou. Zhao. Wang. and Wang agents and heterogeneous device ecosystems often requires custom adaptations. increasing devel- opment complexity. Compromised MCP servers in smart environments can lead to unauthorized control over critical systems. threatening both safety and data integrity. 6.3 Recommendations for MCP stakeholders To safeguard the long-term success and security of MCP. all stakeholders. including MCP main-",
    "chunk_index": 136,
    "start_pos": 59264,
    "end_pos": 59712,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 499,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 498,
      "optimized_content_length": 488
    }
  },
  "1463": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_137",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "rity of MCP. all stakeholders. including MCP main- tainers. developers. researchers. and end-users. should implement best practices and proactively address evolving challenges within the ecosystem. Recommendations for MCP maintainers. MCP maintainers play critical role in establishing security standards. improving version control. and ensuring ecosystem stability. To reduce the risk of security vulnerabilities. maintainers should establish formal package management system",
    "chunk_index": 137,
    "start_pos": 59713,
    "end_pos": 60142,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 480,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 480,
      "optimized_content_length": 476
    }
  },
  "1464": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_138",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "hould establish formal package management system that enforces strict version control and ensures that only verified updates are distributed to users. Additionally. introducing centralized server registry would enable users to discover and validate MCP servers more securely. reducing the risk of interacting with malicious or misconfigured servers. To further enhance security. maintainers should promote the adoption of cryptographic signatures for verifying MCP packages and encourage periodic security audits to identify and",
    "chunk_index": 138,
    "start_pos": 60143,
    "end_pos": 60624,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 73,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 528
    }
  },
  "1465": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_139",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "encourage periodic security audits to identify and mitigate vulnerabilities. Moreover. implementing secure sandboxing framework can help prevent privilege escalation and protect host environments from malicious tool executions. Recommendations for developers. Developers integrating MCP into AI applications should prioritize security and resilience by adhering to secure coding practices and maintaining thorough documentation. Enforcing version management policies can prevent rollbacks to vulnerable versions.",
    "chunk_index": 139,
    "start_pos": 60625,
    "end_pos": 61088,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 514,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 514,
      "optimized_content_length": 512
    }
  },
  "1466": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_140",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "cies can prevent rollbacks to vulnerable versions. while thorough testing ensures reliable MCP integrations before deployment. To mitigate configura- tion drift. developers should automate configuration management and adopt infrastructure-as-code IaC practices. Additionally. implementing robust tool name validation and disambiguation tech- niques can prevent conflicts that lead to unintended behavior. Leveraging runtime monitoring and logging helps track tool invocations. detect anomalies. and mitigate threats effectively.",
    "chunk_index": 140,
    "start_pos": 61089,
    "end_pos": 61568,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 64,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 528
    }
  },
  "1467": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_141",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "etect anomalies. and mitigate threats effectively. Recommendations for researchers. Given the decentralized nature of MCP server deployment and the evolving threat landscape. researchers should focus on conducting systematic security analyses to uncover potential vulnerabilities in tool invocation. sandbox implementations. and privilege management. Exploring techniques to enhance sandbox security. mitigate privilege persistence. and prevent configuration drift can significantly strengthen MCP security posture. In",
    "chunk_index": 141,
    "start_pos": 61569,
    "end_pos": 62038,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 518
    }
  },
  "1468": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_142",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ignificantly strengthen MCP security posture. In addition. researchers should investigate more effective approaches for version control and package management in decentralized ecosystems to reduce the likelihood of re-deploying vulnerable versions. Researchers can help MCP maintainers and developers stay ahead of emerging threats by developing automated vulnerability detection methods and proposing secure update pipelines. Another critical area for research is the exploration of context-aware agent orchestration in multi-",
    "chunk_index": 142,
    "start_pos": 62039,
    "end_pos": 62517,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 529,
      "word_count": 67,
      "optimized_for_embedding": true,
      "original_content_length": 529,
      "optimized_content_length": 527
    }
  },
  "1469": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_143",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ion of context-aware agent orchestration in multi- tool environments. As MCP increasingly supports multi-step. cross-system workflows. ensuring state consistency and preventing tool invocation conflicts becomes paramount. Researchers can explore techniques for dynamic state management. error recovery. and workflow validation to ensure seamless operation in complex environments. Recommendations for end-users. End-users should remain vigilant about security risks and",
    "chunk_index": 143,
    "start_pos": 62518,
    "end_pos": 62936,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 469,
      "word_count": 57,
      "optimized_for_embedding": true,
      "original_content_length": 469,
      "optimized_content_length": 469
    }
  },
  "1470": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_144",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "rs should remain vigilant about security risks and adopt practices to safeguard their environments. They should prioritize using verified MCP servers and avoid unofficial installers that may introduce vulnerabilities. Regularly updating MCP servers and monitoring configuration changes can prevent misconfigurations and reduce exposure to known exploits. Properly configuring access control policies helps prevent privilege escalation and unauthorized tool usage. For users relying on remote MCP servers. choosing providers that",
    "chunk_index": 144,
    "start_pos": 62937,
    "end_pos": 63414,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 528,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 528,
      "optimized_content_length": 528
    }
  },
  "1471": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_145",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ing on remote MCP servers. choosing providers that follow strict security standards can minimize risks in multi-tenant environments. Promoting user awareness and encouraging best practices will enhance overall security and resilience. Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 145,
    "start_pos": 63415,
    "end_pos": 63655,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 291,
      "word_count": 42,
      "optimized_for_embedding": true,
      "original_content_length": 291,
      "optimized_content_length": 287
    }
  },
  "1472": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_146",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 17 --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions 17 RELATED WORK 7.1 Tool Integration in LLM Applications Equipping LLMs with external tools has become key paradigm for enhancing their capabilities in real-world tasks. This approach enables LLMs to transcend the limitations of static knowledge and interact dynamically with external systems. Recent studies have proposed frameworks to",
    "chunk_index": 146,
    "start_pos": 63657,
    "end_pos": 64103,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 497,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 496,
      "optimized_content_length": 487
    }
  },
  "1473": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_147",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ystems. Recent studies have proposed frameworks to support such integration. focusing on tool representation. selection. invocation. and reasoning. Shen et al. 44 provide comprehensive survey outlining standard LLM-tool integration paradigm. identifying key challenges in user intent understanding. tool selection. and execution planning. Building on this. AutoTools 45. 46 introduces an automated framework that transforms raw tool documentation into executable functions. reducing reliance on manual engineering. EasyTool 59",
    "chunk_index": 147,
    "start_pos": 64104,
    "end_pos": 64589,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 526
    }
  },
  "1474": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_148",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ing reliance on manual engineering. EasyTool 59 further streamlines this process by distilling diverse and verbose tool documentation into concise and unified instructions. improving tool usability and efficiency. From an evaluation perspective. several benchmarks have emerged. ToolSandbox 30 emphasizes stateful and interactive tool usage with implicit dependencies. while UltraTool 23 focuses on complex. multi-step tasks involving planning. creation. and execution. These efforts reveal significant performance gaps and motivate",
    "chunk_index": 148,
    "start_pos": 64590,
    "end_pos": 65080,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 541,
      "word_count": 71,
      "optimized_for_embedding": true,
      "original_content_length": 541,
      "optimized_content_length": 532
    }
  },
  "1475": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_149",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "reveal significant performance gaps and motivate better evaluations for LLM-agent capabilities. To improve agent decision-making and prompt quality. AvaTaR 54 proposes contrastive reasoning techniques. while Toolken 56 incorporates reranking and rejection mechanisms for more precise tool use. Additionally. some works explore LLMs not just as tool users but as tool creators ToolMaker 53 autonomously converts code repositories into callable tools. moving toward fully automated agents. To unify this expanding",
    "chunk_index": 149,
    "start_pos": 65081,
    "end_pos": 65551,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 521,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 521,
      "optimized_content_length": 511
    }
  },
  "1476": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_150",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "rd fully automated agents. To unify this expanding landscape. Li 27 proposes taxonomy that situates tool use alongside planning and feedback learning as three core agent paradigms. As tool-augmented LLMs continue to evolve. the lack of standardized. secure. and extensible context protocol has become key bottleneck. MCP. with its potential to unify tool interaction across diverse systems. is poised to become the foundational layer for next-generation LLM applications. making it critical to examine its landscape. limitations. and risks.",
    "chunk_index": 150,
    "start_pos": 65552,
    "end_pos": 66050,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 82,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 540
    }
  },
  "1477": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_151",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "to examine its landscape. limitations. and risks. 7.2 Security Risks in LLM-Tool Interactions The integration of tool-use capabilities into LLM agents significantly expands their functionality. but also introduces new and more severe security risks. Fu et al. 17 demonstrate that obfuscated adversarial prompts can lead LLM agents to misuse tools. enabling attacks such as data exfiltration and unauthorized command execution. These vulnerabilities are particularly concerning as they",
    "chunk_index": 151,
    "start_pos": 66051,
    "end_pos": 66488,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 488,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 487,
      "optimized_content_length": 484
    }
  },
  "1478": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_152",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ulnerabilities are particularly concerning as they generalize across models and modalities. growing body of work has begun to categorize and analyze these risks. Gan et al. 18 and Yu et al. 58 propose taxonomies for threats across agent components and stages. while the OWASP Agentic Security Initiative 25 provides practical threat modeling frameworks. To support detection and mitigation. Chen et al. introduce AgentGuard. which automatically discovers unsafe workflows and generates safety constraints. and ToolFuzz 36",
    "chunk_index": 152,
    "start_pos": 66489,
    "end_pos": 66974,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 536,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 536,
      "optimized_content_length": 521
    }
  },
  "1479": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_153",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "nd generates safety constraints. and ToolFuzz 36 identifies failures stemming from ambiguous or underspecified tool documentation. On the align- ment front. Chen et al. propose the H2A principle. which encourages LLMs to behave with helpfulness. harmlessness. and autonomy. and introduce the ToolAlign dataset to guide safer tool usage. Ye et al. 57 further analyze safety risks throughout the tool-use pipeline. including malicious queries. execution misdirection. and unsafe outputs. Deng et al. 13 highlight broader systemic",
    "chunk_index": 153,
    "start_pos": 66975,
    "end_pos": 67462,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 77,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 527
    }
  },
  "1480": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_154",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "puts. Deng et al. 13 highlight broader systemic risks such as unpredictable inputs. environmental variability. and untrusted tool endpoints. These security threats may be mitigated through the structured design of MCP. but they can also persist or even evolve under this new integration paradigm. As MCP simplifies tool orchestration in LLM applications. it simultaneously introduces new potential attack surfaces. warranting deeper investigation into its security implications. Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 154,
    "start_pos": 67463,
    "end_pos": 67950,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 78,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 531
    }
  },
  "1481": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_155",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 18 --- 18 Hou. Zhao. Wang. and Wang CONCLUSION This paper presents the first comprehensive analysis of the MCP ecosystem landscape. We examine its architecture. core components. operational workflows. and server lifecycle stages. Furthermore. we explore the adoption. diversity. and use cases. while identifying potential security threats throughout the creation. operation. and update phases. We also highlight the implications and",
    "chunk_index": 155,
    "start_pos": 67952,
    "end_pos": 68403,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 502,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 501,
      "optimized_content_length": 489
    }
  },
  "1482": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_156",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ate phases. We also highlight the implications and risks associated with MCP adoption and propose actionable recommendations for stakeholders to enhance security and governance. Additionally. we outline future research directions to tackle emerging risks and improve MCP resilience. As MCP continues to gain traction with industry leaders such as OpenAI and Cloudflare. addressing these challenges is key to its long-term success and to enabling secure. efficient interaction with diverse external tools and services. REFERENCES",
    "chunk_index": 156,
    "start_pos": 68404,
    "end_pos": 68883,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 530,
      "word_count": 74,
      "optimized_for_embedding": true,
      "original_content_length": 530,
      "optimized_content_length": 528
    }
  },
  "1483": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_157",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "th diverse external tools and services. REFERENCES ahujasid. 2025. BlenderMCP Blender Model Context Protocol Integration. https. github.com ahujasid blender-mcp. Anthropic. 2024. For Claude Desktop Users. https. modelcontextprotocol.io quickstart user. Anthropic. 2024. Introducing the Model Context Protocol. https. www.anthropic.com news model-context-protocol. ByteDance. 2024. Coze plugin store. https. www.coze.com store plugin.",
    "chunk_index": 157,
    "start_pos": 68884,
    "end_pos": 69286,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 453,
      "word_count": 40,
      "optimized_for_embedding": true,
      "original_content_length": 453,
      "optimized_content_length": 433
    }
  },
  "1484": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_158",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "plugin store. https. www.coze.com store plugin. Jizhou Chen and Samuel Lee Cong. 2025. AgentGuard. Repurposing Agentic Orchestrator for Safety Evaluation of Tool Orchestration. CoRR abs 2502.09809 2025 https. doi.org 10.48550 ARXIV.2502.09809 arXiv.2502.09809 Zhi-Yuan Chen. Shiqi Shen. Guangyao Shen. Gong Zhi. Xu Chen. and Yankai Lin. 2024. Towards Tool Use Alignment of Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing 1382 1400.",
    "chunk_index": 158,
    "start_pos": 69287,
    "end_pos": 69747,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 511,
      "word_count": 63,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 496
    }
  },
  "1485": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_159",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ethods in Natural Language Processing 1382 1400. Cline. 2025. Cline. https. github.com cline cline. Cloudflare. 2025. Cloudflare. https. www.cloudflare.com. Codeium. 2025. Codeium. https. codeium.com. 10 Sourcegraph Cody. 2025. Cody supports additional context through Anthropic Model Context Protocol. https. sourcegraph.com blog cody-supports-anthropic-model-context-protocol. 11 Florin Cuconasu. Giovanni Trappolini. Federico Siciliano. Simone Filice. Cesare Campagnano. Yoelle Maarek. Nicola",
    "chunk_index": 159,
    "start_pos": 69748,
    "end_pos": 70217,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 520,
      "word_count": 51,
      "optimized_for_embedding": true,
      "original_content_length": 520,
      "optimized_content_length": 495
    }
  },
  "1486": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_160",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Filice. Cesare Campagnano. Yoelle Maarek. Nicola Tonellotto. and Fabrizio Silvestri. 2024. The Power of Noise. Redefining Retrieval for RAG Systems. CoRR abs 2401.14887 2024 https. doi.org 10.48550 ARXIV.2401.14887 arXiv.2401.14887 12 Cursor. 2025. Learn how to add and use custom MCP tools within Cursor. https. docs.cursor.com context model- context-protocol. 13 Zehang Deng. Yongjian Guo. Changzhou Han. Wanlun Ma. Junwu Xiong. Sheng Wen. and Yang Xiang. 2024. AI",
    "chunk_index": 160,
    "start_pos": 70218,
    "end_pos": 70644,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 477,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 477,
      "optimized_content_length": 466
    }
  },
  "1487": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_161",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Junwu Xiong. Sheng Wen. and Yang Xiang. 2024. AI Agents Under Threat. Survey of Key Security Challenges and Future Pathways. CoRR abs 2406.02630 2024 https. doi.org 10.48550 ARXIV.2406.02630 arXiv.2406.02630 14 Windsurf Editor. 2025. Windsurf Editor. https. windsurf.com. 15 Antanavicius et al. 2025. PulseMCP. https. www.pulsemcp.com. 16 Wenqi Fan. Yujuan Ding. Liangbo Ning. Shijie Wang. Hengyun Li. Dawei Yin. Tat-Seng Chua. and Qing Li. 2024.",
    "chunk_index": 161,
    "start_pos": 70645,
    "end_pos": 71058,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 464,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 464,
      "optimized_content_length": 446
    }
  },
  "1488": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_162",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Li. Dawei Yin. Tat-Seng Chua. and Qing Li. 2024. Survey on RAG Meeting LLMs. Towards Retrieval-Augmented Large Language Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. KDD 2024. Barcelona. Spain. August 25-29. 2024 Ricardo Baeza-Yates and Francesco Bonchi Eds. ACM. 6491 6501. https. doi.org 10.1145 3637528.3671470 17 Xiaohan Fu. Shuheng Li. Zihan Wang. Yihao Liu. Rajesh K. Gupta. Taylor Berg-Kirkpatrick. and Earlence Fernandes.",
    "chunk_index": 162,
    "start_pos": 71059,
    "end_pos": 71498,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 480
    }
  },
  "1489": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_163",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Taylor Berg-Kirkpatrick. and Earlence Fernandes. 2024. Imprompter. Tricking LLM Agents into Improper Tool Use. CoRR abs 2410.14923 2024 https. doi.org 10. 48550 ARXIV.2410.14923 arXiv.2410.14923 18 Yuyou Gan. Yong Yang. Zhe Ma. Ping He. Rui Zeng. Yiming Wang. Qingming Li. Chunyi Zhou. Songze Li. Ting Wang. Yunjun Gao. Yingcai Wu. and Shouling Ji. 2024. Navigating the Risks. Survey of Security. Privacy. and Ethics Threats in LLM-Based Agents. CoRR abs 2411.09523 2024 https. doi.org 10.48550 ARXIV.2411.09523 arXiv.2411.09523",
    "chunk_index": 163,
    "start_pos": 71499,
    "end_pos": 71990,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 542,
      "word_count": 69,
      "optimized_for_embedding": true,
      "original_content_length": 542,
      "optimized_content_length": 528
    }
  },
  "1490": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_164",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "doi.org 10.48550 ARXIV.2411.09523 arXiv.2411.09523 19 gching. 2025. Toolbase. https. gettoolbase.ai. 20 glama.ai. 2025. Glama MCP Servers. https. glama.ai mcp servers. 21 Goose. 2025. Goose. https. goose.ai. 22 Nadeeshaan Gunasinghe and Nipuna Marcus. 2021. Language Server Protocol and Implementation Springer. 23 Shijue Huang. Wanjun Zhong. Jianqiao Lu. Qi Zhu. Jiahui Gao. Weiwen Liu. Yutai Hou. Xingshan Zeng. Yasheng Wang. Lifeng Shang. Xin Jiang. Ruifeng Xu. and Qun Liu. 2024. Planning. Creation. Usage. Benchmarking LLMs for",
    "chunk_index": 164,
    "start_pos": 71991,
    "end_pos": 72487,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 547,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 547,
      "optimized_content_length": 532
    }
  },
  "1491": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_165",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Planning. Creation. Usage. Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios. CoRR abs 2401.17167 2024 https. doi.org 10. 48550 ARXIV.2401.17167 arXiv.2401.17167 Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 165,
    "start_pos": 72488,
    "end_pos": 72696,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 259,
      "word_count": 31,
      "optimized_for_embedding": true,
      "original_content_length": 259,
      "optimized_content_length": 249
    }
  },
  "1492": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_166",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 19 --- Model Context Protocol MCP Landscape. Security Threats. and Future Research Directions 19 24 JetBrains. 2025. JetBrains MCP Server. https. plugins.jetbrains.com plugin 26071-mcp-server. 25 Sotiropoulos John. Rosario Ron Del. Kokuykin Evgeniy. Oakley Helen. Habler Idan. Underkoffler Kayla. Huang Ken. Steffensen Peter. Aralimatti Rakshith. Bitton Ron. et al .2025. OWASP Top 10 for LLM Apps Gen AI Agentic Security Initiative Ph. D. Dissertation. OWASP.",
    "chunk_index": 166,
    "start_pos": 72698,
    "end_pos": 73181,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 534,
      "word_count": 75,
      "optimized_for_embedding": true,
      "original_content_length": 533,
      "optimized_content_length": 517
    }
  },
  "1493": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_167",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Security Initiative Ph. D. Dissertation. OWASP. 26 LangChain. 2022. LangChain. Framework for developing applications powered by language models. https. github. com langchain-ai langchain. 27 Xinzhe Li. 2025. Review of Prominent Paradigms for LLM-Based Agents. Tool Use. Planning Including RAG and Feedback Learning. In Proceedings of the 31st International Conference on Computational Linguistics. COLING 2025. Abu Dhabi. UAE. January 19-24. 2025 Owen Rambow. Leo Wanner. Marianna Apidianaki. Hend Al-Khalifa.",
    "chunk_index": 167,
    "start_pos": 73182,
    "end_pos": 73655,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 68,
      "optimized_for_embedding": true,
      "original_content_length": 523,
      "optimized_content_length": 509
    }
  },
  "1494": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_168",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Leo Wanner. Marianna Apidianaki. Hend Al-Khalifa. Barbara Di Eugenio. and Steven Schockaert Eds. Association for Computational Linguistics. 9760 9779. https. aclanthology.org 2025.coling-main.652 28 LibreChat. 2025. LibreChat. https. librechat.ai. 29 Jerry Liu. 2022. LlamaIndex. data framework for LLM applications. https. github.com run-llama llama_index. 30 Jiarui Lu. Thomas Holleis. Yizhe Zhang. Bernhard Aumayer. Feng Nan. Felix Bai. Shuang Ma. Shen Ma. Mengyu Li.",
    "chunk_index": 168,
    "start_pos": 73656,
    "end_pos": 74092,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 487,
      "word_count": 56,
      "optimized_for_embedding": true,
      "original_content_length": 486,
      "optimized_content_length": 470
    }
  },
  "1495": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_169",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "eng Nan. Felix Bai. Shuang Ma. Shen Ma. Mengyu Li. Guoli Yin. Zirui Wang. and Ruoming Pang. 2024. ToolSandbox. Stateful. Conversational. Interactive Evaluation Benchmark for LLM Tool Use Capabilities. CoRR abs 2408.04682 2024 https. doi.org 10.48550 ARXIV.2408.04682 arXiv.2408.04682 31 Baidu Maps. 2025. Baidu Maps MCP Servers. https. lbs.baidu.com faq api.title mcpserver base. 32 Emacs MCP. 2025. Emacs MCP. https. github.com lizqwerscott mcp.el. 33 Tripo3D MCP. 2025. Tripo3D MCP. https. blender-mcp.com",
    "chunk_index": 169,
    "start_pos": 74093,
    "end_pos": 74566,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 524,
      "word_count": 58,
      "optimized_for_embedding": true,
      "original_content_length": 524,
      "optimized_content_length": 507
    }
  },
  "1496": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_170",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "MCP. 2025. Tripo3D MCP. https. blender-mcp.com 34 mcp dockmater. 2025. Dockmaster. https. mcp-dockmaster.com. 35 mcp.so. 2025. MCP.so. https. mcp.so 36 Ivan Milev. Mislav Balunovic Maximilian Baader. and Martin Vechev. 2025. ToolFuzz Automated Agent Tool Testing. arXiv preprint arXiv.2503.04479 2025 37 OpenAI. 2023. ChatGPT plugins. https. openai.com index chatgpt-plugins 38 OpenAI. 2023. Funcation Calling. https. platform.openai.com docs guides function-calling.api-mode responses.",
    "chunk_index": 170,
    "start_pos": 74567,
    "end_pos": 75028,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 512,
      "word_count": 47,
      "optimized_for_embedding": true,
      "original_content_length": 511,
      "optimized_content_length": 486
    }
  },
  "1497": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_171",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "docs guides function-calling.api-mode responses. 39 OpenAI. 2025. OpenAI Agents SDK Model context protocol MCP https. openai.github.io openai-agents-python mcp 40 OpenSumi. 2025. OpenSumi. https. github.com opensumi core. 41 Model Context Protocol. 2024. GitHub MCP Server. https. github.com modelcontextprotocol servers tree main src github. 42 Model Context Protocol. 2024. Slack MCP Server. https. github.com modelcontextprotocol servers tree main src slack. 43 Replit. 2025. Replit. https. replit.com.",
    "chunk_index": 171,
    "start_pos": 75029,
    "end_pos": 75510,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 532,
      "word_count": 44,
      "optimized_for_embedding": true,
      "original_content_length": 532,
      "optimized_content_length": 505
    }
  },
  "1498": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_172",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "ck. 43 Replit. 2025. Replit. https. replit.com. 44 Zhuocheng Shen. 2024. LLM With Tools. Survey. CoRR abs 2409.18807 2024 https. doi.org 10.48550 ARXIV.2409. 18807 arXiv.2409.18807 45 Zhengliang Shi. Shen Gao. Xiuyi Chen. Yue Feng. Lingyong Yan. Haibo Shi. Dawei Yin. Zhumin Chen. Suzan Ver- berne. and Zhaochun Ren. 2024. Chain of Tools. Large Language Model is an Automatic Multi-tool Learner. CoRR abs 2405.16533 2024 https. doi.org 10.48550 ARXIV.2405.16533 arXiv.2405.16533",
    "chunk_index": 172,
    "start_pos": 75511,
    "end_pos": 75955,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 495,
      "word_count": 61,
      "optimized_for_embedding": true,
      "original_content_length": 495,
      "optimized_content_length": 478
    }
  },
  "1499": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_173",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "doi.org 10.48550 ARXIV.2405.16533 arXiv.2405.16533 46 Zhengliang Shi. Shen Gao. Lingyong Yan. Yue Feng. Xiuyi Chen. Zhumin Chen. Dawei Yin. Suzan Verberne. and Zhaochun Ren. 2025. Tool Learning in the Wild. Empowering Language Models as Automatic Tool Agents. In THE WEB CONFERENCE 2025 https. openreview.net forum.id T4wMdeFEjX 47 Block Square 2025. Block Square https. glama.ai mcp servers atblock square-mcp tools team. 48 Stripe. 2025. Stripe. https. stripe.com.",
    "chunk_index": 173,
    "start_pos": 75956,
    "end_pos": 76388,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 483,
      "word_count": 54,
      "optimized_for_embedding": true,
      "original_content_length": 483,
      "optimized_content_length": 466
    }
  },
  "1500": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_174",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "am. 48 Stripe. 2025. Stripe. https. stripe.com. 49 Microsoft Copilot Studio. 2025. Introducing Model Context Protocol MCP in Copilot Studio. Simplified Integration with AI Apps and Agents. https. www.microsoft.com en-us microsoft-copilot blog copilot-studio introducing-model- context-protocol-mcp-in-copilot-studio-simplified-integration-with-ai-apps-and-agents 50 Tencent. 2024. Tencent plugin shop. https. yuanqi.tencent.com plugin-shop. 51 Apify MCP Tester. 2025. Apify MCP Tester. https. apify.com jiri.spilka tester-mcp-client.",
    "chunk_index": 174,
    "start_pos": 76389,
    "end_pos": 76887,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 549,
      "word_count": 44,
      "optimized_for_embedding": true,
      "original_content_length": 549,
      "optimized_content_length": 533
    }
  },
  "1501": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_175",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "https. apify.com jiri.spilka tester-mcp-client. 52 TheiaAI TheiaIDE. 2025. TheiaAI TheiaIDE. https. theia-ide.org docs user_ai 53 Georg Wo lflein. Dyke Ferber. Daniel Truhn. Ognjen Arandjelovic. and Jakob Nikolas Kather. 2025. LLM Agents Making Agent Tools. CoRR abs 2502.11705 2025 https. doi.org 10.48550 ARXIV.2502.11705 arXiv.2502.11705 54 Shirley Wu. Shiyu Zhao. Qian Huang. Kexin Huang. Michihiro Yasunaga. Kaidi Cao. Vassilis N. Ioannidis. Karthik",
    "chunk_index": 175,
    "start_pos": 76888,
    "end_pos": 77306,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 469,
      "word_count": 48,
      "optimized_for_embedding": true,
      "original_content_length": 469,
      "optimized_content_length": 454
    }
  },
  "1502": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_176",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "asunaga. Kaidi Cao. Vassilis N. Ioannidis. Karthik Subbian. Jure Leskovec. and James Y. Zou. 2024. AvaTaR. Optimizing LLM Agents for Tool Usage via Contrastive Reasoning. In Advances in Neural Information Processing Systems 38. Annual Conference on Neural Information Processing Systems 2024. NeurIPS 2024. Vancouver. BC. Canada. December 10 15. 2024 Amir Globersons. Lester Mackey. Danielle Belgrave. Angela Fan. Ulrich Paquet. Jakub M. Tomczak. and Cheng Zhang Eds. http. papers.nips.cc paper_files",
    "chunk_index": 176,
    "start_pos": 77307,
    "end_pos": 77765,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 509,
      "word_count": 70,
      "optimized_for_embedding": true,
      "original_content_length": 509,
      "optimized_content_length": 500
    }
  },
  "1503": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_177",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Zhang Eds. http. papers.nips.cc paper_files Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 177,
    "start_pos": 3901,
    "end_pos": 3957,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 107,
      "word_count": 15,
      "optimized_for_embedding": true,
      "original_content_length": 107,
      "optimized_content_length": 96
    }
  },
  "1504": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_178",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "1. No. 1. Article Publication date. April 2025. --- Page 20 --- 20 Hou. Zhao. Wang. and Wang paper 2024 hash 2db8ce969b000fe0b3fb172490c33ce8-Abstract-Conference.html 55 Zhiheng Xi. Wenxiang Chen. Xin Guo. Wei He. Yiwen Ding. Boyang Hong. Ming Zhang. Junzhe Wang. Senjie Jin. Enyu Zhou. Rui Zheng. Xiaoran Fan. Xiao Wang. Limao Xiong. Yuhao Zhou. Weiran Wang. Changhao Jiang. Yicheng Zou. Xiangyang Liu. Zhangyue Yin. Shihan Dou. Rongxiang Weng. Wensen Cheng. Qi Zhang. Wenjuan Qin. Yongyan",
    "chunk_index": 178,
    "start_pos": 77824,
    "end_pos": 78276,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 503,
      "word_count": 76,
      "optimized_for_embedding": true,
      "original_content_length": 502,
      "optimized_content_length": 490
    }
  },
  "1505": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_179",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Weng. Wensen Cheng. Qi Zhang. Wenjuan Qin. Yongyan Zheng. Xipeng Qiu. Xuanjing Huang. and Tao Gui. 2023. The Rise and Potential of Large Language Model Based Agents. Survey. CoRR abs 2309.07864 2023 https. doi.org 10.48550 ARXIV.2309.07864 arXiv.2309.07864 56 Konstantin Yakovlev. Sergey I. Nikolenko. and Andrey Bout. 2024. Toolken Improving LLM Tool Usage with Reranking and Reject Option. CoRR abs 2410.12004 2024 https. doi.org 10.48550 ARXIV.2410.12004 arXiv.2410.12004",
    "chunk_index": 179,
    "start_pos": 78277,
    "end_pos": 78716,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 490,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 490,
      "optimized_content_length": 474
    }
  },
  "1506": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_180",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "doi.org 10.48550 ARXIV.2410.12004 arXiv.2410.12004 57 Junjie Ye. Sixian Li. Guanyu Li. Caishuang Huang. Songyang Gao. Yilong Wu. Qi Zhang. Tao Gui. and Xuanjing Huang. 2024. ToolSword. Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages. CoRR abs 2402.10753 2024 https. doi.org 10.48550 ARXIV.2402.10753 arXiv.2402.10753 58 Miao Yu. Fanci Meng. Xinyun Zhou. Shilong Wang. Junyuan Mao. Linsey Pang. Tianlong Chen. Kun Wang. Xinfeng",
    "chunk_index": 180,
    "start_pos": 78717,
    "end_pos": 79138,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 472,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 472,
      "optimized_content_length": 464
    }
  },
  "1507": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_181",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "Mao. Linsey Pang. Tianlong Chen. Kun Wang. Xinfeng Li. Yongfeng Zhang. et al .2025. Survey on Trustworthy LLM Agents. Threats and Countermeasures. arXiv preprint arXiv.2503.09648 2025 59 Siyu Yuan. Kaitao Song. Jiangjie Chen. Xu Tan. Yongliang Shen. Kan Ren. Dongsheng Li. and Deqing Yang. 2024. EASYTOOL. Enhancing LLM-based Agents with Concise Tool Instruction. CoRR abs 2401.06201 2024 https. doi.org 10.48550 ARXIV.2401.06201 arXiv.2401.06201",
    "chunk_index": 181,
    "start_pos": 79139,
    "end_pos": 79546,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 458,
      "word_count": 60,
      "optimized_for_embedding": true,
      "original_content_length": 458,
      "optimized_content_length": 446
    }
  },
  "1508": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_182",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "doi.org 10.48550 ARXIV.2401.06201 arXiv.2401.06201 60 Zed. 2025. Zed Model Context Protocol. https. zed.dev docs assistant model-context-protocol. 61 Mingming Zha. Jice Wang. Yuhong Nan. Xiaofeng Wang. Yuqing Zhang. and Zelin Yang. 2022. Hazard Integrated. Understanding Security Risks in App Extensions to Team Chat Systems. In 29th Annual Network and Distributed System Security Symposium. NDSS 2022. San Diego. California. USA. April 24-28. 2022 The Internet Society. https. www.ndss- symposium.org ndss-paper auto-draft-262",
    "chunk_index": 182,
    "start_pos": 79547,
    "end_pos": 80034,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 538,
      "word_count": 62,
      "optimized_for_embedding": true,
      "original_content_length": 538,
      "optimized_content_length": 527
    }
  },
  "1509": {
    "chunk_id": "99da619c-eb3b-4377-8ad7-a340793d7590_chunk_183",
    "document_id": "99da619c-eb3b-4377-8ad7-a340793d7590",
    "content": "www.ndss- symposium.org ndss-paper auto-draft-262 62 Yanjie Zhao. Xinyi Hou. Shenao Wang. and Haoyu Wang. 2024. LLM App Store Analysis. Vision and Roadmap. CoRR abs 2404.12737 2024 https. doi.org 10.48550 ARXIV.2404.12737 arXiv.2404.12737 Vol. 1. No. 1. Article Publication date. April 2025.",
    "chunk_index": 183,
    "start_pos": 80035,
    "end_pos": 80288,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 304,
      "word_count": 37,
      "optimized_for_embedding": true,
      "original_content_length": 304,
      "optimized_content_length": 291
    }
  },
  "1510": {
    "chunk_id": "acf9be43-d36c-4984-9626-c8c56c8c994e_chunk_0",
    "document_id": "acf9be43-d36c-4984-9626-c8c56c8c994e",
    "content": "img-0.jpeg img-0.jpeg Host MCP Client 2. Run particular tool with these arguments CallToolRequest CallToolResult 1. LLM decides tool needs to be executed 3. Here the result of the tool run LLM",
    "chunk_index": 0,
    "start_pos": 0,
    "end_pos": 246,
    "metadata": {
      "chunk_method": "recursive",
      "original_length": 246,
      "word_count": 40,
      "optimized_for_embedding": true,
      "original_content_length": 246,
      "optimized_content_length": 192
    }
  }
}